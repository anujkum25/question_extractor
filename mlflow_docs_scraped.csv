source,text
#,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
introduction/index.html,"   Documentation  What is MLflow?       What is MLflow?  Stepping into the world of Machine Learning (ML) is an exciting journey, but it often comes with complexities that can hinder innovation and experimentation. MLflow is a solution to many of these issues in this dynamic landscape, offering tools and simplifying processes to streamline the ML lifecycle and foster collaboration among ML practitioners. Whether you’re an individual researcher, a member of a large team, or somewhere in between, MLflow provides a unified platform to navigate the intricate maze of model development, deployment, and management. MLflow aims to enable innovation in ML solution development by streamlining otherwise cumbersome logging, organization, and lineage concerns that are unique to model development. This focus allows you to ensure that your ML projects are robust, transparent, and ready for real-world challenges. Read on to discover the core components of MLflow and understand the unique advantages it brings to the complex workflows associated with model development and management.  Core Components of MLflow  MLflow, at its core, provides a suite of tools aimed at simplifying the ML workflow. It is tailored to assist ML practitioners throughout the various stages of ML development and deployment. Despite its expansive offerings, MLflow’s functionalities are rooted in several foundational components:  Tracking: MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time. Whether working in standalone scripts, notebooks, or other environments, Tracking facilitates the logging of results either to local files or a server, making it easier to compare multiple runs across different users. Model Registry: A systematic approach to model management, the Model Registry assists in handling different versions of models, discerning their current state, and ensuring smooth productionization. It offers a centralized model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, including model lineage, versioning, aliasing, tagging, and annotations. MLflow Deployments for LLMs: This server, equipped with a set of standardized APIs, streamlines access to both SaaS and OSS LLM models. It serves as a unified interface, bolstering security through authenticated access, and offers a common set of APIs for prominent LLMs. Evaluate: Designed for in-depth model analysis, this set of tools facilitates objective model comparison, be it traditional ML algorithms or cutting-edge LLMs. Prompt Engineering UI: A dedicated environment for prompt engineering, this UI-centric component provides a space for prompt experimentation, refinement, evaluation, testing, and deployment. Recipes: Serving as a guide for structuring ML projects, Recipes, while offering recommendations, are focused on ensuring functional end results optimized for real-world deployment scenarios. Projects: MLflow Projects standardize the packaging of ML code, workflows, and artifacts, akin to an executable. Each project, be it a directory with code or a Git repository, employs a descriptor or convention to define its dependencies and execution method.  By integrating these core components, MLflow offers an end-to-end platform, ensuring efficiency, consistency, and traceability throughout the ML lifecycle.   Why Use MLflow?  The machine learning (ML) process is intricate, comprising various stages, from data preprocessing to model deployment and monitoring. Ensuring productivity and efficiency throughout this lifecycle poses several challenges:  Experiment Management: It’s tough to keep track of the myriad experiments, especially when working with files or interactive notebooks. Determining which combination of data, code, and parameters led to a particular result can become a daunting task. Reproducibility: Ensuring consistent results across runs is not trivial. Beyond just tracking code versions and parameters, capturing the entire environment, including library dependencies, is critical. This becomes even more challenging when collaborating with other data scientists or when scaling the code to different platforms. Deployment Consistency: With the plethora of ML libraries available, there’s often no standardized way to package and deploy models. Custom solutions can lead to inconsistencies, and the crucial link between a model and the code and parameters that produced it might be lost. Model Management: As data science teams produce numerous models, managing, testing, and continuously deploying these models becomes a significant hurdle. Without a centralized platform, managing model lifecycles becomes unwieldy. Library Agnosticism: While individual ML libraries might offer solutions to some of the challenges, achieving the best results often involves experimenting across multiple libraries. A platform that offers compatibility with various libraries while ensuring models are usable as reproducible “black boxes” is essential.  MLflow addresses these challenges by offering a unified platform tailored for the entire ML lifecycle. Its benefits include:  Traceability: With tools like the Tracking Server, every experiment is logged, ensuring that teams can trace back and understand the evolution of models. Consistency: Be it accessing models through the MLflow Deployments for LLMs or structuring projects with MLflow Recipes, MLflow promotes a consistent approach, reducing both the learning curve and potential errors. Flexibility: MLflow’s library-agnostic design ensures compatibility with a wide range of machine learning libraries. It offers comprehensive support across different programming languages, backed by a robust REST API, CLI, and APIs for Python API, R API, and Java API.  By simplifying the complex landscape of ML workflows, MLflow empowers data scientists and developers to focus on building and refining models, ensuring a streamlined path from experimentation to production.   Who Uses MLflow?  Throughout the lifecycle of a particular project, there are components within MLflow that are designed to cater to different needs.    MLflow’s versatility enhances workflows across various roles, from data scientists to prompt engineers, extending its impact beyond just the confines of a Data Science team.  Data Scientists leverage MLflow for:  Experiment tracking and hypothesis testing persistence. Code structuring for better reproducibility. Model packaging and dependency management. Evaluating hyperparameter tuning selection boundaries. Comparing the results of model retraining over time. Reviewing and selecting optimal models for deployment.  MLOps Professionals utilize MLflow to:  Manage the lifecycles of trained models, both pre and post deployment. Deploy models securely to production environments. Audit and review candidate models prior to deployment. Manage deployment dependencies.  Data Science Managers interact with MLflow by:  Reviewing the outcomes of experimentation and modeling activities. Collaborating with teams to ensure that modeling objectives align with business goals.  Prompt Engineering Users use MLflow for:  Evaluating and experimenting with large language models. Crafting custom prompts and persisting their candidate creations. Deciding on the best base model suitable for their specific project requirements.     Use Cases of MLflow  MLflow is versatile, catering to diverse machine learning scenarios. Here are some typical use cases:  Experiment Tracking: A data science team leverages MLflow Tracking to log parameters and metrics for experiments within a particular domain. Using the MLflow UI, they can compare results and fine-tune their solution approach. The outcomes of these experiments are preserved as MLflow models. Model Selection and Deployment: MLOps engineers employ the MLflow UI to assess and pick the top-performing models. The chosen model is registered in the MLflow Registry, allowing for monitoring its real-world performance. Model Performance Monitoring: Post deployment, MLOps engineers utilize the MLflow Registry to gauge the model’s efficacy, juxtaposing it against other models in a live environment. Collaborative Projects: Data scientists embarking on new ventures organize their work as an MLflow Project. This structure facilitates easy sharing and parameter modifications, promoting collaboration.    Scalability in MLflow  MLflow is architected to seamlessly integrate with diverse data environments, from small datasets to Big Data applications. It’s built with the understanding that quality machine learning outcomes often hinge on robust data sources, and as such, scales adeptly to accommodate varying data needs. Here’s how MLflow addresses scalability across different dimensions:  Distributed Execution: MLflow runs can operate on distributed clusters. For instance, integration with Apache Spark allows for distributed processing. Furthermore, runs can be initiated on the distributed infrastructure of your preference, with results relayed to a centralized Tracking Server for analysis. Notably, MLflow offers an integrated API to initiate runs on Databricks. Parallel Runs: For use cases like hyperparameter tuning, MLflow can orchestrate multiple runs simultaneously, each with distinct parameters. Interoperability with Distributed Storage: MLflow Projects can interface with distributed storage solutions, including Azure ADLS, Azure Blob Storage, AWS S3, Cloudflare R2 and DBFS. Whether it’s automatically fetching files to a local environment or interfacing with a distributed storage URI directly, MLflow ensures that projects can handle extensive datasets – even scenarios like processing a 100 TB file. Centralized Model Management with Model Registry: Large-scale organizations can benefit from the MLflow Model Registry, a unified platform tailored for collaborative model lifecycle management. In environments where multiple data science teams might be concurrently developing numerous models, the Model Registry proves invaluable. It streamlines model discovery, tracks experiments, manages versions, and facilitates understanding a model’s intent across different teams.  By addressing these scalability dimensions, MLflow ensures that users can capitalize on its capabilities regardless of their data environment’s size or complexity.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
getting-started/index.html,"   Documentation  Getting Started with MLflow       Getting Started with MLflow  For those new to MLflow or seeking a refresher on its core functionalities, the quickstart tutorials here are the perfect starting point. They will guide you step-by-step through fundamental concepts, focusing purely on a task that will maximize your understanding of how to use MLflow to solve a particular task.  Guidance on Running Tutorials  If you are new to MLflow and have never interfaced with the MLflow Tracking Server, we highly encourage you to head on over to quickly read the guide below. It will help you get started as quickly as possible with tutorial content throughout the documentation.                          Tracking Server Options                                   Learn about your options for running an MLflow Tracking Server for executing any of the guides and tutorials in the MLflow documentation                         Getting Started Guides   MLflow Tracking  MLflow Tracking is one of the primary service components of MLflow. In these guides, you will gain an understanding of what MLflow Tracking can do to enhance your MLOps related activities while building ML models.    In these introductory guides to MLflow Tracking, you will learn how to leverage MLflow to:  Log training statistics (loss, accuracy, etc.) and hyperparameters for a model Log (save) a model for later retrieval Register a model using the MLflow Model Registry to enable deployment Load the model and use it for inference  In the process of learning these key concepts, you will be exposed to the MLflow Tracking APIs, the MLflow Tracking UI, and learn how to add metadata associated with a model training event to an MLflow run.                          MLflow Tracking Quickstart Guide                                   Learn the basics of MLflow Tracking in a fast-paced guide with a focus on seeing your first model in the MLflow UI                                           In-depth Tutorial for MLflow Tracking                                       Learn the nuances of interfacing with the MLflow Tracking Server in an in-depth tutorial                     If you would like to get started immediately by downloading and running a notebook yourself:  Download the Tracking Quickstart Notebook    Autologging Basics  A great way to get started with MLflow is to use the autologging feature. Autologging automatically logs your model, metrics, examples, signature, and parameters with only a single line of code for many of the most popular ML libraries in the Python ecosystem.    In this brief tutorial, you’ll learn how to leverage MLflow’s autologging feature to simplify your model logging activities.                          MLflow Autologging Quickstart                                   Get started with logging to MLflow with the high-level autologging API in a fast-paced guide                       Run Comparison Basics  This quickstart tutorial focuses on the MLflow UI’s run comparison feature and provides a step-by-step walkthrough of registering the best model found from a hyperparameter tuning execution. After locally serving the registered model, a brief example of preparing a model for remote deployment by containerizing the model using Docker is covered.                             MLflow Run Comparison Quickstart                                   Get started with using the MLflow UI to compare runs and register a model for deployment                         Tracking Server Quickstart  This quickstart tutorial walks through different types of MLflow Tracking Servers and how to use them to log your MLflow experiments.                          5 Minute Tracking Server Overview                                    Learn how to log MLflow experiments with different tracking servers                    Model Registry Quickstart  This quickstart tutorial walks through registering a model in the MLflow model registry and how to retrieve registered models.                          5 Minute Model Registry Overview                                    Learn how to log MLflow models to the model registry                     Further Learning - What’s Next?  Now that you have the essentials under your belt, below are some recommended collections of tutorial and guide content that will help to broaden your understanding of MLflow and its APIs.  Tracking - Learn more abou the MLflow tracking APIs by reading the tracking guide. LLMs - Discover how you can leverage cutting-edge advanced LLMs to power your ML applications by reading the LLMs guide. MLflow Deployment - Follow the comprehensive guide on model deployment to learn how to deploy your MLflow models to a variety of deployment targets. Model Registry - Learn about the MLflow Model Registry and how it can help you manage the lifecycle of your ML models. Deep Learning Library Integrations - From PyTorch to TensorFlow and more, learn about the integrated deep learning capabilities in MLflow by reading the deep learning guide. Traditional ML - Learn about the traditional ML capabilities in MLflow and how they can help you manage your traditional ML workflows.           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
new-features/index.html,"   Documentation  New Features       New Features  Looking to learn about new significant releases in MLflow? Find out about the details of major features, changes, and deprecations below.                          MLflow Transformers Feature Enhancements                    The transformers flavor                     in MLflow has gotten a significant feature overhaul.  All supported pipeline types can now be logged without restriction Pipelines using foundation models can now be logged without copying the large model weights    released in 2.11.0                           PEFT (Parameter-Efficient Fine-Tuning) support                                        MLflow now natively supports PEFT (Parameter-Efficient Fine-Tuning)                     models in the Transformers flavor. PEFT unlocks significantly more efficient model fine-tuning processes such as LoRA, QLoRA, and Prompt Tuning. Check out                     the new QLoRA fine-tuning tutorial to learn how to                     build your own cutting-edge models with MLflow and PEFT!                  Learn more  released in 2.11.0                           ChatModel Pyfunc Subclass Added                                         OpenAI-compatible chat models are now easier than ever to build in MLflow!                     ChatModel is a new                     Pyfunc subclass that makes it easy to deploy and serve chat models with MLflow.                      Check out the                     new tutorial                     on building an OpenAI-compatible chat model using TinyLlama-1.1B-Chat!   released in 2.11.0                           Overhaul of MLflow Tracking UI for Deep Learning workflows                                        We've listened to your feedback and have put in a huge amount of new UI features designed to empower and                     simplify the process of evaluating DL model training runs. Be sure to upgrade your tracking server and                     benefit from all of the new UI enhancements today!                   released in 2.11.0                           Automated model checkpointing for Deep Learning model training                                        When performing training of Deep Learning models with PyTorch Lightning                     or Tensorflow with Keras, model checkpoint saving                     is enabled, allowing for state storage during long-running training events and the ability to resume if                     an issue is encountered during training.                   released in 2.11.0                           Mistral AI added as an MLflow Deployments Provider                                        The MLflow Deployments Server can now                     accept Mistral AI endpoints. Give their models a try today!                   released in 2.11.0                           Keras 3 is now supported in MLflow                                        You can now log and deploy models in the new Keras 3 format, allowing you                     to work with TensorFlow, Torch, or JAX models with a new high-level, easy-to-use suite of APIs.                   released in 2.11.0                           MLflow now has support for OpenAI SDK 1.x                                        We've updated flavors that interact with the OpenAI SDK, bringing full support for the API changes with the 1.x release.                   released in 2.11.0                           MLflow Site Overhaul                                        MLflow has a new homepage that has been completely modernized. Check it out today!                   released in 2.10.0                           LangChain Autologging Support                                        Autologging support for LangChain is now available. Try it out the next time                     that you're building a Generative AI application with Langchain!                   released in 2.10.0                           Object and Array Support for complex Model Signatures                                        Complex input types for model signatures are now supported with native                     support of Array and Object types.                   released in 2.10.0                           Direct Access to OpenAI through the MLflow Deployments API                                        MLflow Deployments now supports direct access to OpenAI services.                   released in 2.9.0                           MLflow Gateway renamed to MLflow Deployments Server                                        The previously known feature, MLflow Gateway has been refactored to the MLflow Deployments Server.                   released in 2.9.0                           MLflow Docs Overhaul                                        The MLflow docs are getting a facelift with added content, tutorials, and guides. Stay tuned for further improvements to the site!                   released in 2.8.0                           New Features for LLM Evaluation                                        The functionality provided for LLM evaluation in MLflow is getting greatly expanded. Check out all of the new features in the                     guide and the tutorials.                  Learn more  released in 2.8.0                           Updated Model Registry UI                                        A new opt-in Model Registry UI has been built that uses Aliases and Tags for managing model development. See                    more about the new UI workflow in the docs.                  Learn more  released in 2.8.0                           Spark Connect support                                        You can now log, save, and load models trained using Spark Connect. Try out Spark 3.5 and the MLflow integration today!                   released in 2.8.0                           AI21 Labs added as an MLflow Gateway provider                                        You can now use the MLflow AI Gateway to connect to LLMs hosted by AI21 Labs.                  Learn more  released in 2.8.0                           Amazon Bedrock added as an MLflow Gateway provider                                        You can now use the MLflow AI Gateway to connect to LLMs hosted by AWS's Bedrock service.                  Learn more  released in 2.8.0                           PaLM 2 added as an MLflow Gateway provider                                        You can now use the MLflow AI Gateway to connect to LLMs hosted by Google's PaLM 2 service.                  Learn more  released in 2.8.0                           Hugging Face TGI added as an MLflow Gateway provider                                        You can self-host your own transformers-based models from the Hugging Face Hub and directly connect to the models with the AI Gateway                     with TGI.                  Learn more  released in 2.8.0                           LLM evaluation viewer added to MLflow UI                                        You can view your LLM evaluation results directly from the MLflow UI.                  Learn more  released in 2.7.0                           Introducting the Prompt Engineering UI                                        Link your MLflow Tracking Server with your MLflow AI Gateway Server to experiment, evaluate, and construct                     prompts that can be compared amongst different providers without writing a single line of code.                  Learn more  released in 2.7.0                           MosaicML Support in AI Gateway                                        MosaicML has now been added to the supported providers in MLflow AI Gateway.                     You can now seamlessly interface with managed popular models like                     MPT-30B and other models in the MPT family.                                       Try it out today with our example.                  Learn more  released in 2.7.0                           Cloudflare R2 now supported as an artifact store                                        Cloudflare's R2 storage backend is now supported for use as an artifact store. To learn more about                     R2, read the Cloudflare docs to get more information and to explore what is possible.                   released in 2.7.0                           Params support for PyFunc Models                                        PyFunc models now support passing parameters at inference time. With this new feature,                     you can define the allowable keys, with default values, for any parameters that you would like                     consumers of your model to be able to override. This is particularly useful for LLMs, where you                     might want to let users adjust commonly modified parameters for a model, such as token counts and temperature.                  Learn more  released in 2.6.0                           MLflow Serving support added to MLflow AI Gateway                                        The MLflow AI Gateway now supports defining an MLflow serving endpoint as provider. With this                     new feature, you can serve any OSS transformers model that conforms to the                     completions or embeddings route type                     definitions.                                       Try it out today with our end-to-end example.                  Learn more  released in 2.6.0                           Introducing the MLflow AI Gateway                                        We're excited to announce the newest top-level component in the MLflow ecosystem: The AI Gateway.                                       With this new feature, you can create a single access point to many of the most popular LLM SaaS services available now,                     simplifying interfaces, managing credentials, and providing a unified standard set of APIs to reduce the complexity of                     building products and services around LLMs.                  Learn more  released in 2.5.0                           MLflow Evaluate now supports LLMs                                        You can now use MLflow evaluate to compare results from your favorite LLMs on a fixed prompt.                                       With support for many of the standard evaluation metrics for LLMs built in directly to the API, the featured                     LLM modeling tasks of text summarization, text classification, question answering, and text generation allows you                     to view the results of submitted text to multiple models in a single UI element.                  Learn more  released in 2.4.0                           Chart View added to the MLflow UI                                        You can now visualize parameters and metrics across multiple runs as a chart on the runs table.                  Learn more  released in 2.2.0           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/index.html,"   Documentation  LLMs       LLMs  LLMs, or Large Language Models, have rapidly become a cornerstone in the machine learning domain, offering immense capabilities ranging from natural language understanding to code generation and more. However, harnessing the full potential of LLMs often involves intricate processes, from interfacing with multiple providers to fine-tuning specific models to achieve desired outcomes. Such complexities can easily become a bottleneck for developers and data scientists aiming to integrate LLM capabilities into their applications. MLflow’s Support for LLMs aims to alleviate these challenges by introducing a suite of features and tools designed with the end-user in mind:  MLflow Deployments Server for LLMs    Serving as a unified interface, the MLflow Deployments Server (previously known as “MLflow AI Gateway”) simplifies interactions with multiple LLM providers. In addition to supporting the most popular SaaS LLM providers, the MLflow Deployments Server provides an integration to MLflow model serving, allowing you to serve your own LLM or a fine-tuned foundation model within your own serving infrastructure.  Note The MLflow Deployments Server is in active development and has been marked as Experimental. APIs may change as this new feature is refined and its functionality is expanded based on feedback.   Benefits of the MLflow Deployments Server   Unified Endpoint: No more juggling between multiple provider APIs. Simplified Integrations: One-time setup, no repeated complex integrations. Secure Credential Management:  Centralized storage prevents scattered API keys. No hardcoding or user-handled keys.   Consistent API Experience:  Uniform API across all providers. Easy-to-use REST endpoints and Client API.   Seamless Provider Swapping:  Swap providers without touching your code. Zero downtime provider, model, or route swapping.      Explore the Native Providers of the MLflow Deployments Server  The MLflow Deployments Server supports a large range of foundational models from popular SaaS model vendors, as well as providing a means of self-hosting your own open source model via an integration with MLflow model serving. Please refer to Supported Provider Models for the full list of supported providers and models. If you’re interested in learning about how to set up the MLflow Deployments Server for a specific provider, follow the links below for our up-to-date documentation on GitHub. Each link will take you to a README file that will explain how to set up a route for the provider. In the same directory as the README, you will find a runnable example of how to query the routes that the example creates, providing you with a quick reference for getting started with your favorite provider!                                                       Note The MLflow and Hugging Face TGI providers are for self-hosted LLM serving of either foundation open-source LLM models, fine-tuned open-source LLM models, or your own custom LLM. The example documentation for these providers will show you how to get started with these, using free-to-use open-source models from the Hugging Face Hub.     LLM Evaluation    Navigating the vast landscape of Large Language Models (LLMs) can be daunting. Determining the right model, prompt, or service that aligns with a project’s needs is no small feat. Traditional machine learning evaluation metrics often fall short when it comes to assessing the nuanced performance of generative models. Enter MLflow LLM Evaluation. This feature is designed to simplify the evaluation process, offering a streamlined approach to compare foundational models, providers, and prompts.  Benefits of MLflow’s LLM Evaluation   Simplified Evaluation: Navigate the LLM space with ease, ensuring the best fit for your project with standard metrics that can be used to compare generated text. Use-Case Specific Metrics: Leverage MLflow’s mlflow.evaluate() API for a high-level, frictionless evaluation experience. Customizable Metrics: Beyond the provided metrics, MLflow supports a plugin-style for custom scoring, enhancing the evaluation’s flexibility. Comparative Analysis: Effortlessly compare foundational models, providers, and prompts to make informed decisions. Deep Insights: Dive into the intricacies of generative models with a comprehensive suite of LLM-relevant metrics.  MLflow’s LLM Evaluation is designed to bridge the gap between traditional machine learning evaluation and the unique challenges posed by LLMs.    Prompt Engineering UI    Effective utilization of LLMs often hinges on crafting the right prompts. The development of a high-quality prompt is an iterative process of trial and error, where subsequent experimentation is not guaranteed to result in cumulative quality improvements. With the volume and speed of iteration through prompt experimentation, it can quickly become very overwhelming to remember or keep a history of the state of different prompts that were tried. Serving as a powerful tool for prompt engineering, the MLflow Prompt Engineering UI revolutionizes the way developers interact with and refine LLM prompts.  Benefits of the MLflow Prompt Engineering UI   Iterative Development: Streamlined process for trial and error without the overwhelming complexity. UI-Based Prototyping: Prototype, iterate, and refine prompts without diving deep into code. Accessible Engineering: Makes prompt engineering more user-friendly, speeding up experimentation. Optimized Configurations: Quickly hone in on the best model configurations for tasks like question answering or document summarization. Transparent Tracking:  Every model iteration and configuration is meticulously tracked. Ensures reproducibility and transparency in your development process.     Note The MLflow Prompt Engineering UI is in active development and has been marked as Experimental. Features and interfaces may evolve as feedback is gathered and the tool is refined.     Native MLflow Flavors for LLMs  Harnessing the power of LLMs becomes effortless with flavors designed specifically for working with LLM libraries and frameworks.  Native Support for Popular Packages: Standardized interfaces for tasks like saving, logging, and managing inference configurations. PyFunc Compatibility:  Load models as PyFuncs for broad compatibility across serving infrastructures. Strengthens the MLOps process for LLMs, ensuring smooth deployments.   Cohesive Ecosystem:  All essential tools and functionalities consolidated under MLflow. Focus on deriving value from LLMs without getting bogged down by interfacing and optimization intricacies.     Explore the Native LLM Flavors  Select the integration below to read the documentation on how to leverage MLflow’s native integration with these popular libraries:                             Learn about MLflow's native integration with the Transformers 🤗 library and see example notebooks that leverage                     MLflow and Transformers to build Open-Source LLM powered solutions.                                              Learn about MLflow's native integration with the OpenAI SDK and see example notebooks that leverage                     MLflow and OpenAI's advanced LLMs to build interesting and fun applications.                                              Learn about MLflow's native integration with the Sentence Transformers library and see example notebooks that leverage                     MLflow and Sentence Transformers to perform operations with encoded text such as semantic search, text similarity, and information retrieval.                                              Learn about MLflow's native integration with LangChain and see example notebooks that leverage                     MLflow and LangChain to build LLM-backed applications.                          LLM Tracking in MLflow    Empowering developers with advanced tracking capabilities, the MLflow LLM Tracking System stands out as the premier solution for managing and analyzing interactions with Large Language Models (LLMs).  Benefits of the MLflow LLM Tracking System   Robust Interaction Management: Comprehensive tracking of every LLM interaction for maximum insight. Tailor-Made for LLMs:  Unique features specifically designed for LLMs. From logging prompts to tracking dynamic data, MLflow has it covered.   Deep Model Insight:  Introduces ‘predictions’ as a core entity, alongside the existing artifacts, parameters, and metrics. Gain unparalleled understanding of text-generating model behavior and performance.   Clarity and Repeatability:  Ensures consistent and transparent tracking across all LLM interactions. Facilitates informed decision-making and optimization in LLM deployment and utilization.       Tutorials and Use Case Guides for LLMs in MLflow  Interested in learning how to leverage MLflow for your LLM projects? Look in the tutorials and guides below to learn more about interesting use cases that could help to make your journey into leveraging LLMs a bit easier! Note that there are additional tutorials within the “Explore the Native LLM Flavors” section above, so be sure to check those out as well!                            Evaluating LLMs                                       Learn how to evaluate LLMs with MLflow.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging, customizing, and deploying advanced LLMs in MLflow using custom PyFuncs.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                              Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
model-evaluation/index.html,"   Documentation  Model Evaluation       Model Evaluation   Harnessing the Power of Automation  In the evolving landscape of machine learning, the evaluation phase of model development is just as important as ever. Ensuring the accuracy, reliability, and efficiency of models is paramount to ensure that the model that has been trained has been as thoroughly validated as it can be prior to promoting it beyond the development phase. However, manual evaluation can be tedious, error-prone, and time-consuming. MLflow addresses these challenges head-on, offering a suite of automated tools that streamline the evaluation process, saving time and enhancing accuracy, helping you to have confidence that the solution that you’ve spent so much time working on will meet the needs of the problem you’re trying to solve.   LLM Model Evaluation  The rise of Large Language Models (LLMs) like ChatGPT has transformed the landscape of text generation, finding applications in question answering, translation, and text summarization. However, evaluating LLMs introduces unique challenges, primarily because there’s often no single ground truth to compare against. MLflow’s evaluation tools are tailored for LLMs, ensuring a streamlined and accurate evaluation process. Key Features:  Versatile Model Evaluation: MLflow supports evaluating various types of LLMs, whether it’s an MLflow pyfunc model, a URI pointing to a registered MLflow model, or any python callable representing your model. Comprehensive Metrics: MLflow offers a range of metrics for LLM evaluation. From metrics that rely on SaaS models like OpenAI for scoring (e.g., mlflow.metrics.genai.answer_relevance()) to function-based per-row metrics such as Rouge (mlflow.metrics.rougeL()) or Flesch Kincaid (mlflow.metrics.flesch_kincaid_grade_level()). Predefined Metric Collections: Depending on your LLM use case, MLflow provides predefined metric collections, such as “question-answering” or “text-summarization”, simplifying the evaluation process. Custom Metric Creation: Beyond the predefined metrics, MLflow allows users to create custom LLM evaluation metrics. Whether you’re looking to evaluate the professionalism of a response or any other custom criteria, MLflow provides the tools to define and implement these metrics. Evaluation with Static Datasets: As of MLflow 2.8.0, you can evaluate a static dataset without specifying a model. This is especially useful when you’ve saved model outputs in a dataset and want a swift evaluation without rerunning the model. Integrated Results View: MLflow’s mlflow.evaluate() returns comprehensive evaluation results, which can be viewed directly in the code or through the MLflow UI for a more visual representation.  Harnessing these features, MLflow’s LLM evaluation tools eliminate the complexities and ambiguities associated with evaluating large language models. By automating these critical evaluation tasks, MLflow ensures that users can confidently assess the performance of their LLMs, leading to more informed decisions in the deployment and application of these models.  Guides and Tutorials for LLM Model Evaluation  To learn more about how you can leverage MLflow’s evaluation features for your LLM-powered project work, see the tutorials below:                          RAG Evaluation                                       Learn how to evaluate a Retrieval Augmented Generation setup with MLflow Evaluate                                           Question-Answering Evaluation                                       See a working example of how to evaluate the quality of an LLM Question-Answering solution                                           RAG Question Generation Evaluation                                       See how to generate Questions for RAG generation and how to evaluate a RAG solution using MLflow                        Traditional ML Evaluation  Traditional machine learning techniques, from classification to regression, have been the bedrock of many industries. MLflow recognizes their significance and offers automated evaluation tools tailored for these classic techniques. Key Features:  Evaluating a Function: To get immediate results, you can evaluate a python function directly without logging the model. This is especially useful when you want a quick evaluation without the overhead of logging. Evaluating a Dataset: MLflow also supports evaluating a static dataset without specifying a model. This is invaluable when you’ve saved model outputs in a dataset and want a swift evaluation without having to rerun model inference. Evaluating a Model: With MLflow, you can set validation thresholds for your metrics. If a model doesn’t meet these thresholds compared to a baseline, MLflow will alert you. This automated validation ensures that only high-quality models progress to the next stages. Common Metrics and Visualizations: MLflow automatically logs common metrics like accuracy, precision, recall, and more. Additionally, visual graphs such as the confusion matrix, lift_curve_plot, and others are auto-logged, providing a comprehensive view of your model’s performance. SHAP Integration: MLflow is integrated with SHAP, allowing for the auto-logging of SHAP’s summarization importances validation visualizations when using the evaluate APIs.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
deep-learning/index.html,"   Documentation  Deep Learning       Deep Learning  The realm of deep learning has witnessed an unprecedented surge, revolutionizing numerous sectors with its ability to process vast amounts of data and capture intricate patterns. From the real-time object detection in autonomous vehicles to the generation of art through Generative Adversarial Networks, and from natural language processing applications in chatbots to predictive analytics in e-commerce, deep learning models are at the forefront of today’s AI-driven innovations. In the deep learning realm, libraries such as PyTorch, Keras, Tensorflow provide handy tools to build and train deep learning models. MLflow, on the other hand, targets the problem of experiment tracking in deep learning, including logging your experiment setup (learning rate, batch size, etc) along with training metrics (loss, accuracy, etc) and the model (architecture, weights, etc). MLflow provides native integrations with deep learning libraries, so you can plug MLflow into your existing deep learning workflow with minimal changes to your code, and view your experiments in the MLflow UI.  Why MLflow for Deep Learning?  MLflow offers a list of features that power your deep learning workflows:  Experiments Tracking: MLflow tracks your deep learning experiments, including parameters, metrics, and models. Your experiments will be stored in the MLflow server, so you can compare across different experiments and share them. Model Registry: You can register your trained deep learning models in the MLflow server, so you can easily retrieve them later for inference. Model Deployment: After training, you can serve the trained model with MLflow as a REST API endpoint, so you can easily integrate it with your application.   Experiments Tracking  Tracking is the cornerstone of the MLflow ecosystem, and especially vital for the iterative nature of deep learning:  Experiments and Runs: Organize your deep learning projects into experiments, with each experiment containing multiple runs. Each run captures essential data like metrics at various training steps, hyperparameters, and the code state. Artifacts: Store vital outputs such as deep learning models, visualizations, or even tensorboard logs. This artifact repository ensures traceability and easy access. Metrics at Steps: With deep learning’s iterative nature, MLflow allows logging metrics at various training steps, offering a granular view of the model’s progress. Dependencies and Environment: Capture the computational environment, including deep learning frameworks’ versions, ensuring reproducibility. Input Examples and Model Signatures: Define the expected format of the model’s inputs, crucial for complex data like images or sequences. UI Integration: The enhanced UI provides a visual overview of deep learning runs, facilitating comparison and insights into training progress. Search Functionality: Efficiently navigate through your deep learning experiments using robust search capabilities. APIs: Interact with the tracking system programmatically, integrating deep learning workflows seamlessly.    Chart ComparisonChart CustomizationRun ComparisonStatistical EvaluationRealtime Tracking  Easier DL Model Comparison with Charts Use charts to compare deep learning (DL) model training convergence easily. Quickly identify superior         configuration sets across training iterations.      Chart Customization for DL Models Easily customize charts for DL training run comparisons. Adjust visualizations to pinpoint optimal parameter         settings, displaying optimization metrics across iterations in a unified view.      Enhanced Parameter and Metric Comparison Analyze parameter relationships from a unified interface to refine tuning parameters, optimizing your DL models efficiently.      Statistical Evaluation of Categorical Parameters Leverage boxplot visualizations for categorical parameter evaluation. Quickly discern the most effective         settings for hyperparameter tuning.      Real-Time Training Tracking Automatically monitor DL training progress over epochs with the MLflow UI. Instantly track results to validate         your hypotheses, eliminating constant manual updates.        Model Registry  A centralized repository for your deep learning models:  Versioning: Handle multiple iterations and versions of deep learning models, facilitating comparison or reversion. Annotations: Attach notes, training datasets, or other relevant metadata to models. Lifecycle Stages: Clearly define the stage of each model version, ensuring clarity in deployment and further fine-tuning.    Model Deployment  Transition deep learning models from training to real-world applications:  Consistency: Ensure models, especially those with GPU dependencies, behave consistently across different deployment environments. Docker and GPU Support: Deploy in containerized environments, ensuring all dependencies, including GPU support, are encapsulated. Scalability: From deploying a single model to serving multiple distributed deep learning models, MLflow scales as per your requirements.     Native Library Support  MLflow has native integrations with common deep learning libraries, such as PyTorch, Keras and Tensorflow, so you can plug MLflow into your workflow easily to elevate your deep learning projects. For detailed guide on how to integrate MLflow with these libraries, refer to the following pages:                             Learn about MLflow's native integration with the Tensorflow library and see example notebooks that leverage                     MLflow and Tensorflow to build deep learning workflows.                                              Learn about MLflow's native integration with the PyTorch library and see example notebooks that leverage                     MLflow and PyTorch to build deep learning workflows.                                              Learn about MLflow's native integration with the Keras library and see example notebooks that leverage                     MLflow and Keras to build deep learning workflows.                                              Learn about MLflow's native integration with the Spacy library and see example code.                                              Learn about MLflow's native integration with the FastAI library and see example code.                              Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
traditional-ml/index.html,"   Documentation  Traditional ML       Traditional ML  In the dynamic landscape of machine learning, traditional techniques remain foundational, playing pivotal roles across various industries and research institutions. From the precision of classification algorithms in healthcare diagnostics to the predictive prowess of regression models in finance, and from the forecasting capabilities of time-series analyses in supply chain management to the insights drawn from statistical modeling in social sciences, these core methodologies underscore many of the technological advancements we witness today. MLflow recognizes the enduring significance of traditional machine learning. Designed with precision and a deep understanding of the challenges and intricacies faced by data scientists and ML practitioners, MLflow offers a comprehensive suite of tools tailor-made for these classic techniques. This platform not only streamlines the model development and deployment processes but also ensures reproducibility, scalability, and traceability. As we delve further, we’ll explore the multifaceted functionalities MLflow offers, showcasing how it enhances the efficacy, reliability, and insights derived from traditional ML models. Whether you’re a seasoned expert looking to optimize workflows or a newcomer eager to make a mark, MLflow stands as an invaluable ally in your machine learning journey.  Native Library Support  There are a number of natively supported traditional ML libraries within MLflow. Throughout the documentation, you may see these referred to as “flavors”, as they are specific implementations of native support for saving, logging, loading, and generic python function representation for the models that are produced from these libraries. There are distinct benefits to using the native versions of these implementations, as many have auto-logging functionality built in, as well as specific custom handling with serialization and deserialization that can greatly simplify your MLOps experiences when using these libraries. The officially supported integrations for traditional ML libraries include:                                         Tutorials and Guides                           Hyperparameter Tuning with MLflow and Optuna                                       Explore the integration of MLflow Tracking with Optuna for hyperparameter tuning. Dive into the capabilities of MLflow,                     understand parent-child run relationships, and compare different tuning runs to optimize model performance.                                           Custom Pyfunc Models with MLflow                                       Dive deep into the world of MLflow's Custom Pyfunc. Starting with basic model definitions, embark on a journey that                     showcases the versatility and power of Pyfunc. From simple mathematical curves to complex machine learning integrations,                     discover how Pyfunc offers standardized, reproducible, and efficient workflows for a variety of use cases.                                           Multi-Model Endpoints with PyFunc                                       Dive deep into custom multi-model inference via MLflow's custom PyFunc models. Learn how to                     simplify low-latency inference by passing additional inference parameters to a simple custom PyFunc implementation.                     This tutorial can serve as a jumping off point for many multi-model endpoing (MME) use cases!                         MLflow Tracking  Tracking is central to the MLflow ecosystem, facilitating the systematic organization of experiments and runs:  Experiments and Runs: Each experiment encapsulates a specific aspect of your research, and each experiment can house multiple runs. Runs document critical data like metrics, parameters, and the code state. Artifacts: Store crucial output from runs, be it models, visualizations, datasets, or other metadata. This repository of artifacts ensures traceability and easy access. Metrics and Parameters: By allowing users to log parameters and metrics, MLflow makes it straightforward to compare different runs, facilitating model optimization. Dependencies and Environment: The platform automatically captures the computational environment, ensuring that experiments are reproducible across different setups. Input Examples and Model Signatures: These features allow developers to define the expected format of the model’s inputs, making validation and debugging more straightforward. UI Integration: The integrated UI provides a visual overview of all runs, enabling easy comparison and deeper insights. Search Functionality: Efficiently sift through your experiments using MLflow’s robust search functionality. APIs: Comprehensive APIs are available, allowing users to interact with the tracking system programmatically, integrating it into existing workflows.    MLflow Recipes  Recipes in MLflow are predefined templates tailored for specific tasks:  Reduced Boilerplate: These templates help eliminate repetitive setup or initialization code, speeding up development. Best Practices: MLflow’s recipes are crafted keeping best practices in mind, ensuring that users are aligned with industry standards right from the get-go. Customizability: While recipes provide a structured starting point, they’re designed to be flexible, accommodating tweaks and modifications as needed.    MLflow Evaluate  Ensuring model quality is paramount:  Auto-generated Metrics: MLflow automatically evaluates models, providing key metrics for regression (like RMSE, MAE) and classification (such as F1-score, AUC-ROC). Visualization: Understand your model better with automatically generated plots. For instance, MLflow can produce confusion matrices, precision-recall curves, and more for classification tasks. Extensibility: While MLflow provides a rich set of evaluation tools out of the box, it’s also designed to accommodate custom metrics and visualizations.    Model Registry  This feature acts as a catalog for models:  Versioning: As models evolve, keeping track of versions becomes crucial. The Model Registry handles versioning, ensuring that users can revert to older versions or compare different iterations. Annotations: Models in the registry can be annotated with descriptions, use-cases, or other relevant metadata. Lifecycle Stages: Track the stage of each model version, be it ‘staging’, ‘production’, or ‘archived’. This ensures clarity in deployment and maintenance processes.    Deployment  MLflow simplifies the transition from development to production:  Consistency: By meticulously recording dependencies and the computational environment, MLflow ensures that models behave consistently across different deployment setups. Docker Support: Facilitate deployment in containerized environments using Docker, encapsulating all dependencies and ensuring a uniform runtime environment. Scalability: MLflow is designed to accommodate both small-scale deployments and large, distributed setups, ensuring that it scales with your needs.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
deployment/index.html,"   Documentation  Deployment       Deployment   Important This page describes the toolset for deploying your in-house MLflow Model. For information on the LLM Deployment Server (formerly known as AI Gateway), please refer to MLflow Deployment Server.  After training your machine learning model and ensuring its performance, the next step is deploying it to a production environment. This process can be complex, but MLflow simplifies it by offering an easy toolset for deploying your ML models to various targets, including local environments, cloud services, and Kubernetes clusters.    By using MLflow deployment toolset, you can enjoy the following benefits:  Effortless Deployment: MLflow provides a simple interface for deploying models to various targets, eliminating the need to write boilerplate code. Dependency and Environment Management: MLflow ensures that the deployment environment mirrors the training environment, capturing all dependencies. This guarantees that models run consistently, regardless of where they’re deployed. Packaging Models and Code: With MLflow, not just the model, but any supplementary code and configurations are packaged along with the deployment container. This ensures that the model can be executed seamlessly without any missing components. Avoid Vendor Lock-in: MLflow provides a standard format for packaging models and unified APIs for deployment. You can easily switch between deployment targets without having to rewrite your code.   Concepts   MLflow Model  MLflow Model is a standard format that packages a machine learning model with its metadata, such as dependencies and inference schema. You typically create a model as a result of training execution using the MLflow Tracking APIs, for instance, mlflow.pyfunc.log_model(). Alternatively, models can be registered and retrieved via the MLflow Model Registry. To use MLflow deployment, you must first create a model.   Container  Container plays a critical role for simplifying and standardizing the model deployment process. MLflow uses Docker containers to package models with their dependencies, enabling deployment to various destinations without environment compatibility issues. See Building a Docker Image for MLflow Model for more details on how to deploy your model as a container. If you’re new to Docker, you can start learning at “What is a Container”.   Deployment Target  Deployment target refers to the destination environment for your model. MLflow supports various targets, including local environments, cloud services (AWS, Azure), Kubernetes clusters, and others.    How it works  An MLflow Model already packages your model and its dependencies, hence MLflow can create either a virtual environment (for local deployment) or a Docker container image containing everything needed to run your model. Subsequently, MLflow launches an inference server with REST endpoints using frameworks like Flask, preparing it for deployment to various destinations to handle inference requests. Detailed information about the server and endpoints is available in Inference Server Specification. MLflow provides CLI commands and Python APIs to facilitate the deployment process. The required commands differ based on the deployment target, so please continue reading to the next section for more details about your specific target.   Supported Deployment Targets  MLflow offers support for a variety of deployment targets. For detailed information and tutorials on each, please follow the respective links below.                            Deploying a Model Locally                                      Deploying a model locally as an inference server is straightforward with MLflow, requiring just a single command mlflow models serve.                                              Amazon SageMaker is a fully managed service for scaling ML inference containers.                     MLflow simplifies the deployment process with easy-to-use commands, eliminating the need to write container definitions.                                              MLflow integrates seamlessly with Azure ML. You can deploy MLflow Model to the Azure ML managed online/batch endpoints,                     or to Azure Container Instances (ACI) / Azure Kubernetes Service (AKS).                                              Databricks Model Serving offers a fully managed service for serving MLflow models at scale,                     with added benefits of performance optimizations and monitoring capabilities.                                             MLflow Deployment integrates with Kubernetes-native ML serving frameworks                    such as Seldon Core and KServe (formerly KFServing).                                            Community Supported Targets                                       MLflow also supports more deployment targets such as Ray Serve, Redis AI, Torch Serve, Oracle Cloud Infrastructure (OCI), through community-supported plugins.                       API References   Command Line Interface  Deployment-related commands are primarily categorized under two modules:  mlflow models - typically used for local deployment. mlflow deployments - typically used for deploying to custom targets.  Note that these categories are not strictly separated and may overlap. Furthermore, certain targets require custom modules or plugins, for example, mlflow sagemaker is used for Amazon SageMaker deployments, and the azureml-mlflow library is required for Azure ML. Therefore, it is advisable to consult the specific documentation for your chosen target to identify the appropriate commands.   Python APIs  Almost all functionalities available in MLflow deployment can also be accessed via Python APIs. For more details, refer to the following API references:  mlflow.models mlflow.deployments mlflow.sagemaker     FAQ  If you encounter any dependency issues during model deployment, please refer to Model Dependencies FAQ for guidance on how to troubleshoot and validate fixes.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
tracking.html,"   Documentation  MLflow Tracking       MLflow Tracking  The MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results. MLflow Tracking provides Python, REST, R, and Java APIs.   A screenshot of the MLflow Tracking UI, showing a plot of validation loss metrics during model training.    Quickstart  If you haven’t used MLflow Tracking before, we strongly recommend going through the following quickstart tutorial.                          MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                       Concepts   Runs  MLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code, for example, a single python train.py execution. Each run records metadata (various information about your run such as metrics, parameters, start and end times) and artifacts (output files from the run such as model weights, images, etc).   Experiments  An experiment groups together runs for a specific task. You can create an experiment using the CLI, API, or UI. The MLflow API and UI also let you create and search for experiments. See Organizing Runs into Experiments for more details on how to organize your runs into experiments.    Tracking Runs    MLflow Tracking APIs provide a set of functions to track your runs. For example, you can call mlflow.start_run() to start a new run, then call Logging Functions such as mlflow.log_param() and mlflow.log_metric() to log a parameters and metrics respectively. Please visit the Tracking API documentation for more details about using these APIs. import mlflow  with mlflow.start_run():     mlflow.log_param(""lr"", 0.001)     # Your ml code     ...     mlflow.log_metric(""val_loss"", val_loss)   Alternatively, Auto-logging offers the ultra-quick setup for starting MLflow tracking. This powerful feature allows you to log metrics, parameters, and models without the need for explicit log statements - all you need to do is call mlflow.autolog() before your training code. Auto-logging supports popular libraries such as Scikit-learn, XGBoost, PyTorch, Keras, Spark, and more. See Automatic Logging Documentation for supported libraries and how to use auto-logging APIs with each of them. import mlflow  mlflow.autolog()  # Your training code...    Note By default, without any particular server/database configuration, MLflow Tracking logs data to the local mlruns directory. If you want to log your runs to a different location, such as a remote database and cloud storage, to share your results with your team, follow the instructions in the Set up MLflow Tracking Environment section.    Tracking Datasets    MLflow offers the ability to track datasets that are associated with model training events. These metadata associated with the Dataset can be stored through the use of the mlflow.log_input() API. To learn more, please visit the MLflow data documentation to see the features available in this API.   Explore Runs and Results   Tracking UI  The Tracking UI lets you visually explore your experiments and runs, as shown on top of this page.  Experiment-based run listing and comparison (including run comparison across multiple experiments) Searching for runs by parameter or metric value Visualizing run metrics Downloading run results (artifacts and metadata)  If you log runs to a local mlruns directory, run the following command in the directory above it, then access http://127.0.0.1:5000 in your browser. mlflow ui --port 5000   Alternatively, the MLflow Tracking Server serves the same UI and enables remote storage of run artifacts. In that case, you can view the UI at http://<IP address of your MLflow tracking server>:5000 from any machine that can connect to your tracking server.   Querying Runs Programmatically  You can also access all of the functions in the Tracking UI programmatically with MlflowClient. For example, the following code snippet search for runs that has the best validation loss among all runs in the experiment. client = mlflow.tracking.MlflowClient() experiment_id = ""0"" best_run = client.search_runs(     experiment_id, order_by=[""metrics.val_loss ASC""], max_results=1 )[0] print(best_run.info) # {'run_id': '...', 'metrics': {'val_loss': 0.123}, ...}      Set up the MLflow Tracking Environment   Note If you just want to log your experiment data and models to local files, you can skip this section.  MLflow Tracking supports many different scenarios for your development workflow. This section will guide you through how to set up the MLflow Tracking environment for your particular use case. From a bird’s-eye view, the MLflow Tracking environment consists of the following components.  Components   MLflow Tracking APIs  You can call MLflow Tracking APIs in your ML code to log runs and communicate with the MLflow Tracking Server if necessary.   Backend Store    The backend store persists various metadata for each Run, such as run ID, start and end times, parameters, metrics, etc. MLflow supports two types of storage for the backend: file-system-based like local files and database-based like PostgreSQL.   Artifact Store    Artifact store persists (typicaly large) arifacts for each run, such as model weights (e.g. a pickled scikit-learn model), images (e.g. PNGs), model and data files (e.g. Parquet file). MLflow stores artifacts ina a local file (mlruns) by default, but also supports different storage options such as Amazon S3 and Azure Blob Storage.   MLflow Tracking Server (Optional)    MLflow Tracking Server is a stand-alone HTTP server that provides REST APIs for accessing backend and/or artifact store. Tracking server also offers flexibility to configure what data to server, govern access control, versioning, and etc. Read MLflow Tracking Server documentation for more details.    Common Setups    By configuring these components properly, you can create an MLflow Tracking environment suitable for your team’s development workflow. The following diagram and table show a few common setups for the MLflow Tracking environment.             1. Localhost (default) 2. Local Tracking with Local Database 3. Remote Tracking with MLflow Tracking Server    Scenario Solo development Solo development Team development  Use Case By default, MLflow records metadata and artifacts for each run to a local directory, mlruns. This is the simplest way to get started with MLflow Tracking, without setting up any external server, database, and storage. The MLflow client can interface with a SQLAlchemy-compatible database (e.g., SQLite, PostgreSQL, MySQL) for the backend. Saving metadata to a database allows you cleaner management of your experiment data while skipping the effort of setting up a server. MLflow Tracking Server can be configured with an artifacts HTTP proxy, passing artifact requests through the tracking server to store and retrieve artifacts without having to interact with underlying object store services. This is particularly useful for team development scenarios where you want to store artifacts and experiment metadata in a shared location with proper access control.  Tutorial QuickStart Tracking Experiments using a Local Database Remote Experiment Tracking with MLflow Tracking Server      Other Configuration with MLflow Tracking Server  MLflow Tracking Server provides customizability for other special use cases. Please follow Remote Experiment Tracking with MLflow Tracking Server for learning the basic setup and continue to the following materials for advanced configurations to meet your needs.   Local Tracking ServerArtifacts-only ModeDirect Access to Artifacts  Using MLflow Tracking Server Locally You can of course run MLflow Tracking Server locally. While this doesn't provide much additional benefit over directly using           the local files or database, might useful for testing your team development workflow locally or running your machine learning           code on a container environment.      Running MLflow Tracking Server in Artifacts-only Mode  MLflow Tracking Server has --artifacts-only option, which lets the server to serve (proxy) only artifacts         and not metadata. This is particularly useful when you are in a large organization or training huge models, you might have high artifact          transfer volumes and want to split out the traffic for serving artifacts to not impact tracking functionality. Please read          Optionally using a Tracking Server instance exclusively for artifact handling for more details on how to use this mode.              Disable Artifact Proxying to Allow Direct Access to Artifacts MLflow Tracking Server, by default, serves both artifacts and only metadata. However, in some cases, you may want         to allow direct access to the remote artifacts storage to avoid the overhead of a proxy while preserving the functionality         of metadata tracking. This can be done by disabling artifact proxying by starting server with --no-serve-artifacts option.         Refer to Use Tracking Server without Proxying Artifacts Access for how to set this up.         FAQ   Can I launch multiple runs in parallel?  Yes, MLflow supports launching multiple runs in parallel e.g. multi processing / threading. See Launching Multiple Runs in One Program for more details.   How can I organize many MLflow Runs neatly?  MLflow provides a few ways to organize your runs:  Organize runs into experiments - Experiments are logical containers for your runs. You can create an experiment using the CLI, API, or UI. Create child runs - You can create child runs under a single parent run to group them together. For example, you can create a child run for each fold in a cross-validation experiment. Add tags to runs - You can associate arbitrary tags with each run, which allows you to filter and search runs based on tags.    Can I directly access remote storage without running the Tracking Server?  Yes, while it is best practice to have the MLflow Tracking Server as a proxy for artifacts access for team development workflows, you may not need that if you are using it for personal projects or testing. You can achieve this by following the workaround below:  Set up artifacts configuration such as credentials and endpoints, just like you would for the MLflow Tracking Server. See configure artifact storage for more details. Create an experiment with an explicit artifact location,  experiment_name = ""your_experiment_name"" mlflow.create_experiment(experiment_name, artifact_location=""s3://your-bucket"") mlflow.set_experiment(experiment_name)   Your runs under this experiment will log artifacts to the remote storage directly.   How to integrate MLflow Tracking with Model Registry?  To use the Model Registry functionality with MLflow tracking, you must use database backed store such as PostgresQL and log a model using the log_model methods of the corresponding model flavors. Once a model has been logged, you can add, modify, update, or delete the model in the Model Registry through the UI or the API. See Backend Stores and Common Setups for how to configures backend store properly for your workflow.   How to include additional decription texts about the run?  A system tag mlflow.note.content can be used to add descriptive note about this run. While the other system tags are set automatically, this tag is not set by default and users can override it to include additional information about the run. The content will be displayed on the run’s page under the Notes section.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
system-metrics/index.html,"   Documentation  System Metrics       System Metrics  MLflow allows users to log system metrics including CPU stats, GPU stats, memory usage, network traffic, and disk usage during the execution of an MLflow run. In this guide, we will walk through how to manage system metrics logging with MLflow.  Extra Dependencies  To log system metrics in MLflow, please install psutil. We explicitly don’t include psutil in MLflow’s dependencies because psutil wheel is not available for linux aarch64, and building from source fails intermittently. To install psutil, run the following command: pip install psutil   If you want to catch GPU metrics, you also need to install pynvml: pip install pynvml     Turn on/off System Metrics Logging  There are three ways to enable or disable system metrics logging:  Set the environment variable MLFLOW_LOG_SYSTEM_METRICS to false to turn off system metrics logging, or true to enable it for all MLflow runs. Use mlflow.enable_system_metrics_logging() to enable and mlflow.disable_system_metrics_logging() to disable system metrics logging for all MLflow runs. Use log_system_metrics parameter in mlflow.start_run() to control system metrics logging for the current MLflow run, i.e., mlflow.start_run(log_system_metrics=True) will enable system metrics logging.   Using the Environment Variable to Control System Metrics Logging  You can set the environment variable MLFLOW_LOG_SYSTEM_METRICS to true to turn on system metrics logging globally, as shown below: export MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true   However, if you are executing the command above from within Ipython notebook (Jupyter, Databricks notebook, Google Colab), the export command will not work due to the segregated state of the ephemeral shell. Instead you can use the following code: import os  os.environ[""MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING""] = ""true""   After setting the environment variable, you will see that starting an MLflow run will automatically collect and log the default system metrics. Try running the following code in your favorite environment and you should see system metrics existing in the logged run data. Please note that you don’t necessarilty need to start an MLflow server, as the metrics are logged locally. import mlflow import time  with mlflow.start_run() as run:     time.sleep(15)  print(mlflow.MlflowClient().get_run(run.info.run_id).data)   Your output should look like this: <RunData: metrics={'system/cpu_utilization_percentage': 12.4, 'system/disk_available_megabytes': 213744.0, 'system/disk_usage_megabytes': 28725.3, 'system/disk_usage_percentage': 11.8, 'system/network_receive_megabytes': 0.0, 'system/network_transmit_megabytes': 0.0, 'system/system_memory_usage_megabytes': 771.1, 'system/system_memory_usage_percentage': 5.7}, params={}, tags={'mlflow.runName': 'nimble-auk-61', 'mlflow.source.name': '/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'root'}>   To disable system metrics logging, you can use either of the following commands: export MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=""false""   import os  del os.environ[""MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING""]   Rerunning the MLflow code above will not log system metrics.   Using mlflow.enable_system_metrics_logging()  We also provide a pair of APIs mlflow.enable_system_metrics_logging() and mlflow.disable_system_metrics_logging() to turn on/off system metrics logging globally for environments in which you do not have the appropriate access to set an environment variable. Running the following code will have the same effect as setting MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING environment variable to true: import mlflow  mlflow.enable_system_metrics_logging()  with mlflow.start_run() as run:     time.sleep(15)  print(mlflow.MlflowClient().get_run(run.info.run_id).data)     Enabling System Metrics Logging for a Single Run  In addition to controlling system metrics logging globally, you can also control it for a single run. To do so, set log_system_metrics as True or False accordingly in mlflow.start_run(): with mlflow.start_run(log_system_metrics=True) as run:     time.sleep(15)  print(mlflow.MlflowClient().get_run(run.info.run_id).data)   Please also note that using log_system_metrics will ignore the global status of system metrics logging. In other words, the above code will log system metrics for the specific run even if you have disabled system metrics logging by setting MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING to false or calling mlflow.disable_system_metrics_logging().    Types of System Metrics  By default, MLflow logs the following system metrics:  cpu_utilization_percentage system_memory_usage_megabytes system_memory_usage_percentage gpu_utilization_percentage gpu_memory_usage_megabytes gpu_memory_usage_percentage network_receive_megabytes network_transmit_megabytes disk_usage_megabytes disk_available_megabytes  GPU metrics are only logged when a GPU is available and pynvml is installed. Every system metric has a prefix system/ when logged for grouping purpose. So the actual metric name that is logged will have system/ prepended, e.g, system/cpu_utilization_percentage, system/system_memory_usage_megabytes, etc.   Viewing System Metrics within the MLflow UI  System metrics are available within the MLflow UI under the metrics section. In order to view them, let’s start our MLflow UI server, and log some system metrics to it: mlflow ui   import mlflow import time  mlflow.set_tracking_uri(""http://127.0.0.1:5000"") with mlflow.start_run() as run:     time.sleep(15)   Navigate to http://127.0.0.1:5000 in your browser and open your run. You should see system metrics under the metrics section, similar as shown by the screenshot below:      Customizing System Metrics Logging   Customizing Logging Frequency  By default, system metrics are sampled every 10 seconds and are directly logged after sampling. You can customize the sampling frequency by setting environment variable MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL to an integer representing the logging frequency in seconds or by using mlflow.set_system_metrics_sampling_interval() to set the interval, as shown below. In addition to setting the frequency of system metrics logging, you can also customize the number of samples to aggregate. You can also customize the number of samples to aggregate before logging by setting environment variable MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING or using mlflow.set_system_metrics_samples_before_logging(). The actual logging time window is the product of MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL and MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING. For example, if you set sample interval to 2 seconds and samples before logging to 3, then system metrics will be collected every 2 seconds, then after 3 samples are collected (2 * 3 = 6s), we aggregate the metrics and log to MLflow server. The aggregation logic depends on different system metrics. For example, for cpu_utilization_percentage it’s the average of the samples. You will see system/cpu_utilization_percentage logged a few times.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
projects.html,"   Documentation  MLflow Projects       MLflow Projects  An MLflow Project is a format for packaging data science code in a reusable and reproducible way, based primarily on conventions. In addition, the Projects component includes an API and command-line tools for running projects, making it possible to chain together projects into workflows.  Table of Contents  Overview Specifying Projects Running Projects Iterating Quickly Building Multistep Workflows    Overview  At the core, MLflow Projects are just a convention for organizing and describing your code to let other data scientists (or automated tools) run it. Each project is simply a directory of files, or a Git repository, containing your code. MLflow can run some projects based on a convention for placing files in this directory (for example, a conda.yaml file is treated as a Conda environment), but you can describe your project in more detail by adding a MLproject file, which is a YAML formatted text file. Each project can specify several properties:  NameA human-readable name for the project.  Entry PointsCommands that can be run within the project, and information about their parameters. Most projects contain at least one entry point that you want other users to call. Some projects can also contain more than one entry point: for example, you might have a single Git repository containing multiple featurization algorithms. You can also call any .py or .sh file in the project as an entry point. If you list your entry points in a MLproject file, however, you can also specify parameters for them, including data types and default values.  EnvironmentThe software environment that should be used to execute project entry points. This includes all library dependencies required by the project code. See Project Environments for more information about the software environments supported by MLflow Projects, including Conda environments, Virtualenv environments, and Docker containers.   You can run any project from a Git URI or from a local directory using the mlflow run command-line tool, or the mlflow.projects.run() Python API. These APIs also allow submitting the project for remote execution on Databricks and Kubernetes.  Important By default, MLflow uses a new, temporary working directory for Git projects. This means that you should generally pass any file arguments to MLflow project using absolute, not relative, paths. If your project declares its parameters, MLflow automatically makes paths absolute for parameters of type path.    Specifying Projects  By default, any Git repository or local directory can be treated as an MLflow project; you can invoke any bash or Python script contained in the directory as a project entry point. The Project Directories section describes how MLflow interprets directories as projects. To provide additional control over a project’s attributes, you can also include an MLproject file in your project’s repository or directory. Finally, MLflow projects allow you to specify the software environment that is used to execute project entry points.  Project Environments  MLflow currently supports the following project environments: Virtualenv environment, conda environment, Docker container environment, and system environment.  Virtualenv environment (preferred)Virtualenv environments support Python packages available on PyPI. When an MLflow Project specifies a Virtualenv environment, MLflow will download the specified version of Python by using pyenv and create an isolated environment that contains the project dependencies using virtualenv, activating it as the execution environment prior to running the project code. You can specify a Virtualenv environment for your MLflow Project by including a python_env entry in your MLproject file. For details, see the Project Directories and Specifying an Environment sections.    Docker container environmentDocker containers allow you to capture non-Python dependencies such as Java libraries. When you run an MLflow project that specifies a Docker image, MLflow runs your image as is with the parameters specified in your MLproject file. In this case you’ll need to pre build your images with both environment and code to run it. To run the project with a new image that’s based on your image and contains the project’s contents in the /mlflow/projects/code directory, use the --build-image flag when running mlflow run. Environment variables, such as MLFLOW_TRACKING_URI, are propagated inside the Docker container during project execution. Additionally, runs and experiments created by the project are saved to the tracking server specified by your tracking URI. When running against a local tracking URI, MLflow mounts the host system’s tracking directory (e.g., a local mlruns directory) inside the container so that metrics, parameters, and artifacts logged during project execution are accessible afterwards. See Dockerized Model Training with MLflow for an example of an MLflow project with a Docker environment. To specify a Docker container environment, you must add an MLproject file to your project. For information about specifying a Docker container environment in an MLproject file, see Specifying an Environment.    Conda environmentConda environments support both Python packages and native libraries (e.g, CuDNN or Intel MKL). When an MLflow Project specifies a Conda environment, it is activated before project code is run.  Warning By using conda, you’re responsible for adhering to Anaconda’s terms of service.  By default, MLflow uses the system path to find and run the conda binary. You can use a different Conda installation by setting the MLFLOW_CONDA_HOME environment variable; in this case, MLflow attempts to run the binary at $MLFLOW_CONDA_HOME/bin/conda. You can specify a Conda environment for your MLflow project by including a conda.yaml file in the root of the project directory or by including a conda_env entry in your MLproject file. For details, see the Project Directories and Specifying an Environment sections. The mlflow run command supports running a conda environment project as a virtualenv environment project. To do this, run mlflow run with --env-manager virtualenv: mlflow run /path/to/conda/project --env-manager virtualenv    Warning When a conda environment project is executed as a virtualenv environment project, conda dependencies will be ignored and only pip dependencies will be installed.   System environmentYou can also run MLflow Projects directly in your current system environment. All of the project’s dependencies must be installed on your system prior to project execution. The system environment is supplied at runtime. It is not part of the MLflow Project’s directory contents or MLproject file. For information about using the system environment when running a project, see the Environment parameter description in the Running Projects section.     Project Directories  When running an MLflow Project directory or repository that does not contain an MLproject file, MLflow uses the following conventions to determine the project’s attributes:  The project’s name is the name of the directory. The Conda environment is specified in conda.yaml, if present. If no conda.yaml file is present, MLflow uses a Conda environment containing only Python (specifically, the latest Python available to Conda) when running the project. Any .py and .sh file in the project can be an entry point. MLflow uses Python to execute entry points with the .py extension, and it uses bash to execute entry points with the .sh extension. For more information about specifying project entrypoints at runtime, see Running Projects. By default, entry points do not have any parameters when an MLproject file is not included. Parameters can be supplied at runtime via the mlflow run CLI or the mlflow.projects.run() Python API. Runtime parameters are passed to the entry point on the command line using --key value syntax. For more information about running projects and with runtime parameters, see Running Projects.    MLproject File  You can get more control over an MLflow Project by adding an MLproject file, which is a text file in YAML syntax, to the project’s root directory. The following is an example of an MLproject file: name: My Project  python_env: python_env.yaml # or # conda_env: my_env.yaml # or # docker_env: #    image:  mlflow-docker-example  entry_points:   main:     parameters:       data_file: path       regularization: {type: float, default: 0.1}     command: ""python train.py -r {regularization} {data_file}""   validate:     parameters:       data_file: path     command: ""python validate.py {data_file}""   The file can specify a name and a Conda or Docker environment, as well as more detailed information about each entry point. Specifically, each entry point defines a command to run and parameters to pass to the command (including data types).  Specifying an Environment  This section describes how to specify Conda and Docker container environments in an MLproject file. MLproject files cannot specify both a Conda environment and a Docker environment.  Virtualenv environmentInclude a top-level python_env entry in the MLproject file. The value of this entry must be a relative path to a python_env YAML file within the MLflow project’s directory. The following is an example MLProject file with a python_env definition: python_env: files/config/python_env.yaml   python_env refers to an environment file located at <MLFLOW_PROJECT_DIRECTORY>/files/config/python_env.yaml, where <MLFLOW_PROJECT_DIRECTORY> is the path to the MLflow project’s root directory. The following is an example of a python_env.yaml file: # Python version required to run the project. python: ""3.8.15"" # Dependencies required to build packages. This field is optional. build_dependencies:   - pip   - setuptools   - wheel==0.37.1 # Dependencies required to run the project. dependencies:   - mlflow==2.3   - scikit-learn==1.0.2    Conda environmentInclude a top-level conda_env entry in the MLproject file. The value of this entry must be a relative path to a Conda environment YAML file within the MLflow project’s directory. In the following example: conda_env: files/config/conda_environment.yaml   conda_env refers to an environment file located at <MLFLOW_PROJECT_DIRECTORY>/files/config/conda_environment.yaml, where <MLFLOW_PROJECT_DIRECTORY> is the path to the MLflow project’s root directory.  Docker container environmentInclude a top-level docker_env entry in the MLproject file. The value of this entry must be the name of a Docker image that is accessible on the system executing the project; this image name may include a registry path and tags. Here are a couple of examples. Example 1: Image without a registry path docker_env:   image: mlflow-docker-example-environment   In this example, docker_env refers to the Docker image with name mlflow-docker-example-environment and default tag latest. Because no registry path is specified, Docker searches for this image on the system that runs the MLflow project. If the image is not found, Docker attempts to pull it from DockerHub. Example 2: Mounting volumes and specifying environment variables You can also specify local volumes to mount in the docker image (as you normally would with Docker’s -v option), and additional environment variables (as per Docker’s -e option). Environment variables can either be copied from the host system’s environment variables, or specified as new variables for the Docker environment. The environment field should be a list. Elements in this list can either be lists of two strings (for defining a new variable) or single strings (for copying variables from the host system). For example: docker_env:   image: mlflow-docker-example-environment   volumes: [""/local/path:/container/mount/path""]   environment: [[""NEW_ENV_VAR"", ""new_var_value""], ""VAR_TO_COPY_FROM_HOST_ENVIRONMENT""]   In this example our docker container will have one additional local volume mounted, and two additional environment variables: one newly-defined, and one copied from the host system. Example 3: Image in a remote registry docker_env:   image: 012345678910.dkr.ecr.us-west-2.amazonaws.com/mlflow-docker-example-environment:7.0   In this example, docker_env refers to the Docker image with name mlflow-docker-example-environment and tag 7.0 in the Docker registry with path 012345678910.dkr.ecr.us-west-2.amazonaws.com, which corresponds to an Amazon ECR registry. When the MLflow project is run, Docker attempts to pull the image from the specified registry. The system executing the MLflow project must have credentials to pull this image from  the specified registry. Example 4: Build a new image docker_env:   image: python:3.8   mlflow run ... --build-image   To build a new image that’s based on the specified image and files contained in the project directory, use the --build-image argument. In the above example, the image python:3.8 is pulled from Docker Hub if it’s not present locally, and a new image is built based on it. The project is executed in a container created from this image.     Command Syntax  When specifying an entry point in an MLproject file, the command can be any string in Python format string syntax. All of the parameters declared in the entry point’s parameters field are passed into this string for substitution. If you call the project with additional parameters not listed in the parameters field, MLflow passes them using --key value syntax, so you can use the MLproject file to declare types and defaults for just a subset of your parameters. Before substituting parameters in the command, MLflow escapes them using the Python shlex.quote function, so you don’t need to worry about adding quotes inside your command field.   Specifying Parameters  MLflow allows specifying a data type and default value for each parameter. You can specify just the data type by writing: parameter_name: data_type   in your YAML file, or add a default value as well using one of the following syntaxes (which are equivalent in YAML): parameter_name: {type: data_type, default: value}  # Short syntax  parameter_name:     # Long syntax   type: data_type   default: value   MLflow supports four parameter types, some of which it treats specially (for example, downloading data to local files). Any undeclared parameters are treated as string. The parameter types are:  stringA text string.  floatA real number. MLflow validates that the parameter is a number.  pathA path on the local file system. MLflow converts any relative path parameters to absolute paths. MLflow also downloads any paths passed as distributed storage URIs (s3://, dbfs://, gs://, etc.) to local files. Use this type for programs that can only read local files.  uriA URI for data either in a local or distributed storage system. MLflow converts relative paths to absolute paths, as in the path type. Use this type for programs that know how to read from distributed storage (e.g., programs that use Spark).       Running Projects  MLflow provides two ways to run projects: the mlflow run command-line tool, or the mlflow.projects.run() Python API. Both tools take the following parameters:  Project URIA directory on the local file system or a Git repository path, specified as a URI of the form https://<repo> (to use HTTPS) or user@host:path (to use Git over SSH). To run against an MLproject file located in a subdirectory of the project, add a ‘#’ to the end of the URI argument, followed by the relative path from the project’s root directory to the subdirectory containing the desired project.  Project VersionFor Git-based projects, the commit hash or branch name in the Git repository.  Entry PointThe name of the entry point, which defaults to main. You can use any entry point named in the MLproject file, or any .py or .sh file in the project, given as a path from the project root (for example, src/test.py).  ParametersKey-value parameters. Any parameters with declared types are validated and transformed if needed.  Deployment Mode Both the command-line and API let you launch projects remotely in a Databricks environment. This includes setting cluster parameters such as a VM type. Of course, you can also run projects on any other computing infrastructure of your choice using the local version of the mlflow run command (for example, submit a script that does mlflow run to a standard job queueing system). You can also launch projects remotely on Kubernetes clusters using the mlflow run CLI (see Run an MLflow Project on Kubernetes).   EnvironmentBy default, MLflow Projects are run in the environment specified by the project directory or the MLproject file (see Specifying Project Environments). You can ignore a project’s specified environment and run the project in the current system environment by supplying the --env-manager=local flag, but this can lead to unexpected results if there are dependency mismatches between the project environment and the current system environment.   For example, the tutorial creates and publishes an MLflow Project that trains a linear model. The project is also published on GitHub at https://github.com/mlflow/mlflow-example. To run this project: mlflow run git@github.com:mlflow/mlflow-example.git -P alpha=0.5   There are also additional options for disabling the creation of a Conda environment, which can be useful if you quickly want to test a project in your existing shell environment.  Run an MLflow Project on Databricks  You can run MLflow Projects remotely on Databricks. To use this feature, you must have an enterprise Databricks account (Community Edition is not supported) and you must have set up the Databricks CLI. Find detailed instructions in the Databricks docs (Azure Databricks, Databricks on AWS).   Run an MLflow Project on Kubernetes  You can run MLflow Projects with Docker environments on Kubernetes. The following sections provide an overview of the feature, including a simple Project execution guide with examples. To see this feature in action, you can also refer to the Docker example, which includes the required Kubernetes backend configuration (kubernetes_backend.json) and Kubernetes Job Spec (kubernetes_job_template.yaml) files.  How it works  When you run an MLflow Project on Kubernetes, MLflow constructs a new Docker image containing the Project’s contents; this image inherits from the Project’s Docker environment. MLflow then pushes the new Project image to your specified Docker registry and starts a Kubernetes Job on your specified Kubernetes cluster. This Kubernetes Job downloads the Project image and starts a corresponding Docker container. Finally, the container invokes your Project’s entry point, logging parameters, tags, metrics, and artifacts to your MLflow tracking server.   Execution guide  You can run your MLflow Project on Kubernetes by following these steps:  Add a Docker environment to your MLflow Project, if one does not already exist. For reference, see Specifying an Environment. Create a backend configuration JSON file with the following entries:  kube-context The Kubernetes context where MLflow will run the job. If not provided, MLflow will use the current context. If no context is available, MLflow will assume it is running in a Kubernetes cluster and it will use the Kubernetes service account running the current pod (‘in-cluster’ configuration). repository-uri The URI of the docker repository where the Project execution Docker image will be uploaded (pushed). Your Kubernetes cluster must have access to this repository in order to run your MLflow Project. kube-job-template-path The path to a YAML configuration file for your Kubernetes Job - a Kubernetes Job Spec. MLflow reads the Job Spec and replaces certain fields to facilitate job execution and monitoring; MLflow does not modify the original template file. For more information about writing Kubernetes Job Spec templates for use with MLflow, see the Job Templates section.     Example Kubernetes backend configuration {   ""kube-context"": ""docker-for-desktop"",   ""repository-uri"": ""username/mlflow-kubernetes-example"",   ""kube-job-template-path"": ""/Users/username/path/to/kubernetes_job_template.yaml"" }     If necessary, obtain credentials to access your Project’s Docker and Kubernetes resources, including:  The Docker environment image specified in the MLproject file. The Docker repository referenced by repository-uri in your backend configuration file. The Kubernetes context referenced by kube-context in your backend configuration file.  MLflow expects these resources to be accessible via the docker and kubectl CLIs before running the Project.  Run the Project using the MLflow Projects CLI or Python API, specifying your Project URI and the path to your backend configuration file. For example: mlflow run <project_uri> --backend kubernetes --backend-config examples/docker/kubernetes_config.json   where <project_uri> is a Git repository URI or a folder.     Job Templates  MLflow executes Projects on Kubernetes by creating Kubernetes Job resources. MLflow creates a Kubernetes Job for an MLflow Project by reading a user-specified Job Spec. When MLflow reads a Job Spec, it formats the following fields:  metadata.name Replaced with a string containing the name of the MLflow Project and the time of Project execution spec.template.spec.container[0].name Replaced with the name of the MLflow Project spec.template.spec.container[0].image Replaced with the URI of the Docker image created during Project execution. This URI includes the Docker image’s digest hash. spec.template.spec.container[0].command Replaced with the Project entry point command specified when executing the MLflow Project.  The following example shows a simple Kubernetes Job Spec that is compatible with MLflow Project execution. Replaced fields are indicated using bracketed text. Example Kubernetes Job Spec apiVersion: batch/v1 kind: Job metadata:   name: ""{replaced with MLflow Project name}""   namespace: mlflow spec:   ttlSecondsAfterFinished: 100   backoffLimit: 0   template:     spec:       containers:       - name: ""{replaced with MLflow Project name}""         image: ""{replaced with URI of Docker image created during Project execution}""         command: [""{replaced with MLflow Project entry point command}""]         env: [""{appended with MLFLOW_TRACKING_URI, MLFLOW_RUN_ID and MLFLOW_EXPERIMENT_ID}""]         resources:           limits:             memory: 512Mi           requests:             memory: 256Mi       restartPolicy: Never   The container.name, container.image, and container.command fields are only replaced for the first container defined in the Job Spec. Further, the MLFLOW_TRACKING_URI, MLFLOW_RUN_ID and MLFLOW_EXPERIMENT_ID are appended to container.env. Use KUBE_MLFLOW_TRACKING_URI to pass a different tracking URI to the job container from the standard MLFLOW_TRACKING_URI. All subsequent container definitions are applied without modification.     Iterating Quickly  If you want to rapidly develop a project, we recommend creating an MLproject file with your main program specified as the main entry point, and running it with mlflow run .. To avoid having to write parameters repeatedly, you can add default parameters in your MLproject file.   Building Multistep Workflows  The mlflow.projects.run() API, combined with mlflow.client, makes it possible to build multi-step workflows with separate projects (or entry points in the same project) as the individual steps. Each call to mlflow.projects.run() returns a run object, that you can use with mlflow.client to determine when the run has ended and get its output artifacts. These artifacts can then be passed into another step that takes path or uri parameters. You can coordinate all of the workflow in a single Python program that looks at the results of each step and decides what to submit next using custom code. Some example use cases for multi-step workflows include:  Modularizing Your Data Science CodeDifferent users can publish reusable steps for data featurization, training, validation, and so on, that other users or team can run in their workflows. Because MLflow supports Git versioning, another team can lock their workflow to a specific version of a project, or upgrade to a new one on their own schedule.  Hyperparameter TuningUsing mlflow.projects.run() you can launch multiple runs in parallel either on the local machine or on a cloud platform like Databricks. Your driver program can then inspect the metrics from each run in real time to cancel runs, launch new ones, or select the best performing run on a target metric.  Cross-validationSometimes you want to run the same training code on different random splits of training and validation data. With MLflow Projects, you can package the project in a way that allows this, for example, by taking a random seed for the train/validation split as a parameter, or by calling another project first that can split the input data.   For an example of how to construct such a multistep workflow, see the MLflow Multistep Workflow Example project.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
models.html,"   Documentation  MLflow Models       MLflow Models  An MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools—for example, real-time serving through a REST API or batch inference on Apache Spark. The format defines a convention that lets you save a model in different “flavors” that can be understood by different downstream tools.  Table of Contents  Storage Format Managing Model Dependencies Model Signatures And Input Examples Model API Built-In Model Flavors Model Evaluation Model Customization Built-In Deployment Tools Export a python_function model as an Apache Spark UDF Deployment to Custom Targets Community Model Flavors    Storage Format  Each MLflow Model is a directory containing arbitrary files, together with an MLmodel file in the root of the directory that can define multiple flavors that the model can be viewed in. Flavors are the key concept that makes MLflow Models powerful: they are a convention that deployment tools can use to understand the model, which makes it possible to write tools that work with models from any ML library without having to integrate each tool with each library. MLflow defines several “standard” flavors that all of its built-in deployment tools support, such as a “Python function” flavor that describes how to run the model as a Python function. However, libraries can also define and use other flavors. For example, MLflow’s mlflow.sklearn library allows loading models back as a scikit-learn Pipeline object for use in code that is aware of scikit-learn, or as a generic Python function for use in tools that just need to apply the model (for example, the mlflow deployments tool with the option -t sagemaker for deploying models to Amazon SageMaker).  MLmodel file  All of the flavors that a particular model supports are defined in its MLmodel file in YAML format. For example, mlflow.sklearn outputs models as follows: # Directory written by mlflow.sklearn.save_model(model, ""my_model"") my_model/ ├── MLmodel ├── model.pkl ├── conda.yaml ├── python_env.yaml └── requirements.txt   And its MLmodel file describes two flavors: time_created: 2018-05-25T17:28:53.35  flavors:   sklearn:     sklearn_version: 0.19.1     pickled_model: model.pkl   python_function:     loader_module: mlflow.sklearn   Apart from a flavors field listing the model flavors, the MLmodel YAML format can contain the following fields:  time_created: Date and time when the model was created, in UTC ISO 8601 format. run_id: ID of the run that created the model, if the model was saved using MLflow Tracking. signature: model signature in JSON format. input_example: reference to an artifact with input example. databricks_runtime: Databricks runtime version and type, if the model was trained in a Databricks notebook or job. mlflow_version: The version of MLflow that was used to log the model.    Additional Logged Files  For environment recreation, we automatically log conda.yaml, python_env.yaml, and requirements.txt files whenever a model is logged. These files can then be used to reinstall dependencies using conda or virtualenv with pip. Please see How MLflow Model Records Dependencies for more details about these files.  Note When a model registered in the MLflow Model Registry is downloaded, a YAML file named registered_model_meta is added to the model directory on the downloader’s side. This file contains the name and version of the model referenced in the MLflow Model Registry, and will be used for deployment and other purposes.       Managing Model Dependencies  An MLflow Model infers dependencies required for the model flavor and automatically logs them. However, it also allows you to define extra dependencies or custom Python code, and offer a tool to validate them in a sandbox environment. Please refer to Managing Dependencies in MLflow Models for more details.   Model Signatures And Input Examples    In MLflow, understanding the intricacies of model signatures and input examples is crucial for effective model management and deployment.  Model Signature: Defines the schema for model inputs, outputs, and additional inference parameters, promoting a standardized interface for model interaction. Model Input Example: Provides a concrete instance of valid model input, aiding in understanding and testing model requirements. Additionally, if an input example is provided when logging a model, a model signature will be automatically inferred and stored if not explicitly provided.  Our documentation delves into several key areas:  Supported Signature Types: We cover the different data types that are supported, such as tabular data for traditional machine learning models and tensors for deep learning models. Signature Enforcement: Discusses how MLflow enforces schema compliance, ensuring that the provided inputs match the model’s expectations. Logging Models with Signatures: Guides on how to incorporate signatures when logging models, enhancing clarity and reliability in model operations.  For a detailed exploration of these concepts, including examples and best practices, visit the Model Signatures and Examples Guide. If you would like to see signature enforcement in action, see the notebook tutorial on Model Signatures to learn more.   Model API  You can save and load MLflow Models in multiple ways. First, MLflow includes integrations with several common libraries. For example, mlflow.sklearn contains save_model, log_model, and load_model functions for scikit-learn models. Second, you can use the mlflow.models.Model class to create and write models. This class has four key functions:  add_flavor to add a flavor to the model. Each flavor has a string name and a dictionary of key-value attributes, where the values can be any object that can be serialized to YAML. save to save the model to a local directory. log to log the model as an artifact in the current run using MLflow Tracking. load to load a model from a local directory or from an artifact in a previous run.    Built-In Model Flavors  MLflow provides several standard flavors that might be useful in your applications. Specifically, many of its deployment tools support these flavors, so you can export your own model in one of these flavors to benefit from all these tools:   Python Function (python_function) R Function (crate) H2O (h2o) Keras (keras) MLeap (mleap) PyTorch (pytorch) Scikit-learn (sklearn) Spark MLlib (spark) TensorFlow (tensorflow) ONNX (onnx) MXNet Gluon (gluon) XGBoost (xgboost) LightGBM (lightgbm) CatBoost (catboost) Spacy(spaCy) Fastai(fastai) Statsmodels (statsmodels) Prophet (prophet) Pmdarima (pmdarima) OpenAI (openai) (Experimental) LangChain (langchain) (Experimental) John Snow Labs (johnsnowlabs) (Experimental) Diviner (diviner) Transformers (transformers) (Experimental) SentenceTransformers (sentence_transformers) (Experimental)    Python Function (python_function)  The python_function model flavor serves as a default model interface for MLflow Python models. Any MLflow Python model is expected to be loadable as a python_function model. This enables other MLflow tools to work with any python model regardless of which persistence module or framework was used to produce the model. This interoperability is very powerful because it allows any Python model to be productionized in a variety of environments. In addition, the python_function model flavor defines a generic filesystem model format for Python models and provides utilities for saving and loading models to and from this format. The format is self-contained in the sense that it includes all the information necessary to load and use a model. Dependencies are stored either directly with the model or referenced via conda environment. This model format allows other tools to integrate their models with MLflow.  How To Save Model As Python Function  Most python_function models are saved as part of other model flavors - for example, all mlflow built-in flavors include the python_function flavor in the exported models. In addition, the mlflow.pyfunc module defines functions for creating python_function models explicitly. This module also includes utilities for creating custom Python models, which is a convenient way of adding custom python code to ML models. For more information, see the custom Python models documentation.   How To Load And Score Python Function Models  You can load python_function models in Python by calling the mlflow.pyfunc.load_model() function. Note that the load_model function assumes that all dependencies are already available and will not check nor install any dependencies ( see model deployment section for tools to deploy models with automatic dependency management). Once loaded, you can score the model by calling the predict method, which has the following signature: predict(data: Union[pandas.(Series | DataFrame), numpy.ndarray, csc_matrix, csr_matrix, List[Any], Dict[str, Any], str],         params: Optional[Dict[str, Any]] = None) → Union[pandas.(Series | DataFrame), numpy.ndarray, list, str]   All PyFunc models will support pandas.DataFrame as an input. In addition to pandas.DataFrame, DL PyFunc models will also support tensor inputs in the form of numpy.ndarrays. To verify whether a model flavor supports tensor inputs, please check the flavor’s documentation. For models with a column-based schema, inputs are typically provided in the form of a pandas.DataFrame. If a dictionary mapping column name to values is provided as input for schemas with named columns or if a python List or a numpy.ndarray is provided as input for schemas with unnamed columns, MLflow will cast the input to a DataFrame. Schema enforcement and casting with respect to the expected data types is performed against the DataFrame. For models with a tensor-based schema, inputs are typically provided in the form of a numpy.ndarray or a dictionary mapping the tensor name to its np.ndarray value. Schema enforcement will check the provided input’s shape and type against the shape and type specified in the model’s schema and throw an error if they do not match. For models where no schema is defined, no changes to the model inputs and outputs are made. MLflow will propagate any errors raised by the model if the model does not accept the provided input type. The python environment that a PyFunc model is loaded into for prediction or inference may differ from the environment in which it was trained. In the case of an environment mismatch, a warning message will be printed when calling mlflow.pyfunc.load_model(). This warning statement will identify the packages that have a version mismatch between those used during training and the current environment.  In order to get the full dependencies of the environment in which the model was trained, you can call mlflow.pyfunc.get_model_dependencies(). Furthermore, if you want to run model inference in the same environment used in model training, you can call mlflow.pyfunc.spark_udf() with the env_manager argument set as “conda”. This will generate the environment from the conda.yaml file, ensuring that the python UDF will execute with the exact package versions that were used during training. Some PyFunc models may accept model load configuration, which controls how the model is loaded and predictions computed. You can learn which configuration the model supports by inspecting the model’s flavor metadata: model_info = mlflow.models.get_model_info(model_uri) model_info.flavors[mlflow.pyfunc.FLAVOR_NAME][mlflow.pyfunc.MODEL_CONFIG]   Alternatively, you can load the PyFunc model and inspect the model_config property: pyfunc_model = mlflow.pyfunc.load_model(model_uri) pyfunc_model.model_config   Model configuration can be changed at loading time by indicating model_config parameter in the mlflow.pyfunc.load_model() method: pyfunc_model = mlflow.pyfunc.load_model(model_uri, model_config=dict(temperature=0.93))   When a model configuration value is changed, those values the configuration the model was saved with. Indicating an invalid model configuration key for a model results in that configuration being ignored. A warning is displayed mentioning the ignored entries.  Note Model configuration vs parameters with default values in signatures: Use model configuration when you need to provide model publishers for a way to change how the model is loaded into memory and how predictions are computed for all the samples. For instance, a key like user_gpu. Model consumers are not able to change those values at predict time. Use parameters with default values in the signature to provide a users the ability to change how predictions are computed on each data sample.     R Function (crate)  The crate model flavor defines a generic model format for representing an arbitrary R prediction function as an MLflow model using the crate function from the carrier package. The prediction function is expected to take a dataframe as input and produce a dataframe, a vector or a list with the predictions as output. This flavor requires R to be installed in order to be used.  crate usage  For a minimal crate model, an example configuration for the predict function is: library(mlflow) library(carrier) # Load iris dataset data(""iris"")  # Learn simple linear regression model model <- lm(Sepal.Width~Sepal.Length, data = iris)  # Define a crate model # call package functions with an explicit :: namespace. crate_model <- crate(   function(new_obs)  stats::predict(model, data.frame(""Sepal.Length"" = new_obs)),   model = model )  # log the model model_path <- mlflow_log_model(model = crate_model, artifact_path = ""iris_prediction"")  # load the logged model and make a prediction model_uri <- paste0(mlflow_get_run()$artifact_uri, ""/iris_prediction"") mlflow_model <- mlflow_load_model(model_uri = model_uri,                                   flavor = NULL,                                   client = mlflow_client())  prediction <- mlflow_predict(model = mlflow_model, data = 5) print(prediction)      H2O (h2o)  The h2o model flavor enables logging and loading H2O models. The mlflow.h2o module defines save_model() and log_model() methods in python, and mlflow_save_model and mlflow_log_model in R for saving H2O models in MLflow Model format. These methods produce MLflow Models with the python_function flavor, allowing you to load them as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can be scored with only DataFrame input. When you load MLflow Models with the h2o flavor using mlflow.pyfunc.load_model(), the h2o.init() method is called. Therefore, the correct version of h2o(-py) must be installed in the loader’s environment. You can customize the arguments given to h2o.init() by modifying the init entry of the persisted H2O model’s YAML configuration file: model.h2o/h2o.yaml. Finally, you can use the mlflow.h2o.load_model() method to load MLflow Models with the h2o flavor as H2O model objects. For more information, see mlflow.h2o.  h2o pyfunc usage  For a minimal h2o model, here is an example of the pyfunc predict() method in a classification scenario : import mlflow import h2o  h2o.init() from h2o.estimators.glm import H2OGeneralizedLinearEstimator  # import the prostate data df = h2o.import_file(     ""http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv.zip"" )  # convert the columns to factors df[""CAPSULE""] = df[""CAPSULE""].asfactor() df[""RACE""] = df[""RACE""].asfactor() df[""DCAPS""] = df[""DCAPS""].asfactor() df[""DPROS""] = df[""DPROS""].asfactor()  # split the data train, test, valid = df.split_frame(ratios=[0.7, 0.15])  # generate a GLM model glm_classifier = H2OGeneralizedLinearEstimator(     family=""binomial"", lambda_=0, alpha=0.5, nfolds=5, compute_p_values=True )  with mlflow.start_run():     glm_classifier.train(         y=""CAPSULE"", x=[""AGE"", ""RACE"", ""VOL"", ""GLEASON""], training_frame=train     )     metrics = glm_classifier.model_performance()     metrics_to_track = [""MSE"", ""RMSE"", ""r2"", ""logloss""]     metrics_to_log = {         key: value         for key, value in metrics._metric_json.items()         if key in metrics_to_track     }     params = glm_classifier.params     mlflow.log_params(params)     mlflow.log_metrics(metrics_to_log)     model_info = mlflow.h2o.log_model(glm_classifier, artifact_path=""h2o_model_info"")  # load h2o model and make a prediction h2o_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri) test_df = test.as_data_frame() predictions = h2o_pyfunc.predict(test_df) print(predictions)  # it is also possible to load the model and predict using h2o methods on the h2o frame  # h2o_model = mlflow.h2o.load_model(model_info.model_uri) # predictions = h2o_model.predict(test)      Keras (keras)  The keras model flavor enables logging and loading Keras models. It is available in both Python and R clients. In R, you can save or log the model using mlflow_save_model and mlflow_log_model. These functions serialize Keras models as HDF5 files using the Keras library’s built-in model persistence functions. You can use mlflow_load_model function in R to load MLflow Models with the keras flavor as Keras Model objects.  Keras pyfunc usage  For a minimal Sequential model, an example configuration for the pyfunc predict() method is: import mlflow import numpy as np import pathlib import shutil from tensorflow import keras  mlflow.tensorflow.autolog()  X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1) y = np.array([0, 0, 1, 1, 1, 0]) model = keras.Sequential(     [         keras.Input(shape=(1,)),         keras.layers.Dense(1, activation=""sigmoid""),     ] ) model.compile(loss=""binary_crossentropy"", optimizer=""adam"", metrics=[""accuracy""]) model.fit(X, y, batch_size=3, epochs=5, validation_split=0.2)  local_artifact_dir = ""/tmp/mlflow/keras_model"" pathlib.Path(local_artifact_dir).mkdir(parents=True, exist_ok=True)  model_uri = f""runs:/{mlflow.last_active_run().info.run_id}/model"" keras_pyfunc = mlflow.pyfunc.load_model(     model_uri=model_uri, dst_path=local_artifact_dir )  data = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1) predictions = keras_pyfunc.predict(data)  shutil.rmtree(local_artifact_dir)      MLeap (mleap)   Warning The mleap model flavor is deprecated as of MLflow 2.6.0 and will be removed in a future release.  The mleap model flavor supports saving Spark models in MLflow format using the MLeap persistence mechanism. MLeap is an inference-optimized format and execution engine for Spark models that does not depend on SparkContext to evaluate inputs.  Note You can save Spark models in MLflow format with the mleap flavor by specifying the sample_input argument of the mlflow.spark.save_model() or mlflow.spark.log_model() method (recommended). For more details see Spark MLlib.  The mlflow.mleap module also defines save_model() and log_model() methods for saving MLeap models in MLflow format, but these methods do not include the python_function flavor in the models they produce. Similarly, mleap models can be saved in R with mlflow_save_model and loaded with mlflow_load_model, with mlflow_save_model requiring sample_input to be specified as a sample Spark dataframe containing input data to the model is required by MLeap for data schema inference. A companion module for loading MLflow Models with the MLeap flavor is available in the mlflow/java package. For more information, see mlflow.spark, mlflow.mleap, and the MLeap documentation.   PyTorch (pytorch)  The pytorch model flavor enables logging and loading PyTorch models. The mlflow.pytorch module defines utilities for saving and loading MLflow Models with the pytorch flavor. You can use the mlflow.pytorch.save_model() and mlflow.pytorch.log_model() methods to save PyTorch models in MLflow format; both of these functions use the torch.save() method to serialize PyTorch models. Additionally, you can use the mlflow.pytorch.load_model() method to load MLflow Models with the pytorch flavor as PyTorch model objects. This loaded PyFunc model can be scored with both DataFrame input and numpy array input. Finally, models produced by mlflow.pytorch.save_model() and mlflow.pytorch.log_model() contain the python_function flavor, allowing you to load them as generic Python functions for inference via mlflow.pyfunc.load_model().  Note When using the PyTorch flavor, if a GPU is available at prediction time, the default GPU will be used to run inference. To disable this behavior, users can use the MLFLOW_DEFAULT_PREDICTION_DEVICE or pass in a device with the device parameter for the predict function.   Note In case of multi gpu training, ensure to save the model only with global rank 0 gpu. This avoids logging multiple copies of the same model.   PyTorch pyfunc usage  For a minimal PyTorch model, an example configuration for the pyfunc predict() method is: import numpy as np import mlflow from mlflow.models import infer_signature import torch from torch import nn   net = nn.Linear(6, 1) loss_function = nn.L1Loss() optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)  X = torch.randn(6) y = torch.randn(1)  epochs = 5 for epoch in range(epochs):     optimizer.zero_grad()     outputs = net(X)      loss = loss_function(outputs, y)     loss.backward()      optimizer.step()  with mlflow.start_run() as run:     signature = infer_signature(X.numpy(), net(X).detach().numpy())     model_info = mlflow.pytorch.log_model(net, ""model"", signature=signature)  pytorch_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)  predictions = pytorch_pyfunc.predict(torch.randn(6).numpy()) print(predictions)   For more information, see mlflow.pytorch.    Scikit-learn (sklearn)  The sklearn model flavor provides an easy-to-use interface for saving and loading scikit-learn models. The mlflow.sklearn module defines save_model() and log_model() functions that save scikit-learn models in MLflow format, using either Python’s pickle module (Pickle) or CloudPickle for model serialization. These functions produce MLflow Models with the python_function flavor, allowing them to be loaded as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. Finally, you can use the mlflow.sklearn.load_model() method to load MLflow Models with the sklearn flavor as scikit-learn model objects.  Scikit-learn pyfunc usage  For a Scikit-learn LogisticRegression model, an example configuration for the pyfunc predict() method is: import mlflow from mlflow.models import infer_signature import numpy as np from sklearn.linear_model import LogisticRegression  with mlflow.start_run():     X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)     y = np.array([0, 0, 1, 1, 1, 0])     lr = LogisticRegression()     lr.fit(X, y)     signature = infer_signature(X, lr.predict(X))      model_info = mlflow.sklearn.log_model(         sk_model=lr, artifact_path=""model"", signature=signature     )  sklearn_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)  data = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1)  predictions = sklearn_pyfunc.predict(data)   For more information, see mlflow.sklearn.    Spark MLlib (spark)  The spark model flavor enables exporting Spark MLlib models as MLflow Models. The mlflow.spark module defines  save_model() to save a Spark MLlib model to a DBFS path. log_model() to upload a Spark MLlib model to the tracking server. mlflow.spark.load_model() to load MLflow Models with the spark flavor as Spark MLlib pipelines.  MLflow Models produced by these functions contain the python_function flavor, allowing you to load them as generic Python functions via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. When a model with the spark flavor is loaded as a Python function via mlflow.pyfunc.load_model(), a new SparkContext is created for model inference; additionally, the function converts all Pandas DataFrame inputs to Spark DataFrames before scoring. While this initialization overhead and format translation latency is not ideal for high-performance use cases, it enables you to easily deploy any MLlib PipelineModel to any production environment supported by MLflow (SageMaker, AzureML, etc).  Spark MLlib pyfunc usage  from pyspark.ml.classification import LogisticRegression from pyspark.ml.linalg import Vectors from pyspark.sql import SparkSession import mlflow  # Prepare training data from a list of (label, features) tuples. spark = SparkSession.builder.appName(""LogisticRegressionExample"").getOrCreate() training = spark.createDataFrame(     [         (1.0, Vectors.dense([0.0, 1.1, 0.1])),         (0.0, Vectors.dense([2.0, 1.0, -1.0])),         (0.0, Vectors.dense([2.0, 1.3, 1.0])),         (1.0, Vectors.dense([0.0, 1.2, -0.5])),     ],     [""label"", ""features""], )  # Create and fit a LogisticRegression instance lr = LogisticRegression(maxIter=10, regParam=0.01) lr_model = lr.fit(training)  # Serialize the Model with mlflow.start_run():     model_info = mlflow.spark.log_model(lr_model, ""spark-model"")  # Load saved model lr_model_saved = mlflow.pyfunc.load_model(model_info.model_uri)  # Make predictions on test data. # The DataFrame used in the predict method must be a Pandas DataFrame test = spark.createDataFrame(     [         (1.0, Vectors.dense([-1.0, 1.5, 1.3])),         (0.0, Vectors.dense([3.0, 2.0, -0.1])),         (1.0, Vectors.dense([0.0, 2.2, -1.5])),     ],     [""label"", ""features""], ).toPandas()  prediction = lr_model_saved.predict(test)    Note Note that when the sample_input parameter is provided to log_model() or save_model(), the Spark model is automatically saved as an mleap flavor by invoking mlflow.mleap.add_to_model(). For example, the follow code block: training_df = spark.createDataFrame([     (0, ""a b c d e spark"", 1.0),     (1, ""b d"", 0.0),     (2, ""spark f g h"", 1.0),     (3, ""hadoop mapreduce"", 0.0) ], [""id"", ""text"", ""label""])  tokenizer = Tokenizer(inputCol=""text"", outputCol=""words"") hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=""features"") lr = LogisticRegression(maxIter=10, regParam=0.001) pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) model = pipeline.fit(training_df)  mlflow.spark.log_model(model, ""spark-model"", sample_input=training_df)   results in the following directory structure logged to the MLflow Experiment: # Directory written by with the addition of mlflow.mleap.add_to_model(model, ""spark-model"", training_df) # Note the addition of the mleap directory spark-model/ ├── mleap ├── sparkml ├── MLmodel ├── conda.yaml ├── python_env.yaml └── requirements.txt   For more information, see mlflow.mleap.  For more information, see mlflow.spark.    TensorFlow (tensorflow)  The simple example below shows how to log params and metrics in mlflow for a custom training loop using low-level TensorFlow API. See tf-keras-example. for an example of mlflow and tf.keras models. import numpy as np import tensorflow as tf  import mlflow  x = np.linspace(-4, 4, num=512) y = 3 * x + 10  # estimate w and b where y = w * x + b learning_rate = 0.1 x_train = tf.Variable(x, trainable=False, dtype=tf.float32) y_train = tf.Variable(y, trainable=False, dtype=tf.float32)  # initial values w = tf.Variable(1.0) b = tf.Variable(1.0)  with mlflow.start_run():     mlflow.log_param(""learning_rate"", learning_rate)      for i in range(1000):         with tf.GradientTape(persistent=True) as tape:             # calculate MSE = 0.5 * (y_predict - y_train)^2             y_predict = w * x_train + b             loss = 0.5 * tf.reduce_mean(tf.square(y_predict - y_train))             mlflow.log_metric(""loss"", value=loss.numpy(), step=i)          # Update the trainable variables         # w = w - learning_rate * gradient of loss function w.r.t. w         # b = b - learning_rate * gradient of loss function w.r.t. b         w.assign_sub(learning_rate * tape.gradient(loss, w))         b.assign_sub(learning_rate * tape.gradient(loss, b))  print(f""W = {w.numpy():.2f}, b = {b.numpy():.2f}"")     ONNX (onnx)  The onnx model flavor enables logging of ONNX models in MLflow format via the mlflow.onnx.save_model() and mlflow.onnx.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can be scored with both DataFrame input and numpy array input. The python_function representation of an MLflow ONNX model uses the ONNX Runtime execution engine for evaluation. Finally, you can use the mlflow.onnx.load_model() method to load MLflow Models with the onnx flavor in native ONNX format. For more information, see mlflow.onnx and http://onnx.ai/.  Warning The default behavior for saving ONNX files is to use the ONNX save option save_as_external_data=True in order to support model files that are in excess of 2GB. For edge deployments of small model files, this may create issues. If you need to save a small model as a single file for such deployment considerations, you can set the parameter save_as_external_data=False in either mlflow.onnx.save_model() or mlflow.onnx.log_model() to force the serialization of the model as a small file. Note that if the model is in excess of 2GB, saving as a single file will not work.   ONNX pyfunc usage example  For an ONNX model, an example configuration that uses pytorch to train a dummy model, converts it to ONNX, logs to mlflow and makes a prediction using pyfunc predict() method is: import numpy as np import mlflow from mlflow.models import infer_signature import onnx import torch from torch import nn  # define a torch model net = nn.Linear(6, 1) loss_function = nn.L1Loss() optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)  X = torch.randn(6) y = torch.randn(1)  # run model training epochs = 5 for epoch in range(epochs):     optimizer.zero_grad()     outputs = net(X)      loss = loss_function(outputs, y)     loss.backward()      optimizer.step()  # convert model to ONNX and load it torch.onnx.export(net, X, ""model.onnx"") onnx_model = onnx.load_model(""model.onnx"")  # log the model into a mlflow run with mlflow.start_run():     signature = infer_signature(X.numpy(), net(X).detach().numpy())     model_info = mlflow.onnx.log_model(onnx_model, ""model"", signature=signature)  # load the logged model and make a prediction onnx_pyfunc = mlflow.pyfunc.load_model(model_info.model_uri)  predictions = onnx_pyfunc.predict(X.numpy()) print(predictions)      MXNet Gluon (gluon)   Warning The gluon model flavor is deprecated and will be removed in a future release.  The gluon model flavor enables logging of Gluon models in MLflow format via the mlflow.gluon.save_model() and mlflow.gluon.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can be scored with both DataFrame input and numpy array input. You can also use the mlflow.gluon.load_model() method to load MLflow Models with the gluon flavor in native Gluon format.  Gluon pyfunc usage  For a minimal gluon model, here is an example of the pyfunc predict() method with a logistic regression model : import mlflow import mxnet as mx from mxnet import nd, autograd, gluon from mxnet.gluon import nn, Trainer from mxnet.gluon.data import DataLoader, ArrayDataset import numpy as np  # this example requires a compatible version of numpy : numpy == 1.23.1 # `pip uninstall numpy`  `python -m pip install numpy==1.23.1`   def get_random_data(size, ctx):     x = nd.normal(0, 1, shape=(size, 10), ctx=ctx)     y = x.sum(axis=1) > 3     return x, y   # use cpu for this example, gpu could be used with ctx=gpu() ctx = mx.cpu() train_data_size = 1000 val_data_size = 100 batch_size = 10  train_x, train_ground_truth_class = get_random_data(train_data_size, ctx) train_dataset = ArrayDataset(train_x, train_ground_truth_class) train_dataloader = DataLoader(     train_dataset,     batch_size=batch_size,     shuffle=True, )  val_x, val_ground_truth_class = get_random_data(val_data_size, ctx) val_dataset = ArrayDataset(val_x, val_ground_truth_class) val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)  net = nn.HybridSequential()  with net.name_scope():     net.add(nn.Dense(units=10, activation=""relu""))  # input layer     net.add(nn.Dense(units=10, activation=""relu""))  # inner layer 1     net.add(nn.Dense(units=10, activation=""relu""))  # inner layer 2     net.add(nn.Dense(units=1))  # output layer: must have only 1 neuron  net.initialize(mx.init.Xavier())  loss = gluon.loss.SigmoidBinaryCrossEntropyLoss() trainer = Trainer(     params=net.collect_params(),     optimizer=""sgd"",     optimizer_params={""learning_rate"": 0.1}, )  accuracy = mx.metric.Accuracy() f1 = mx.metric.F1() threshold = 0.5   def train_model():     cumulative_train_loss = 0      for i, (data, label) in enumerate(train_dataloader):         with autograd.record():             # do forward pass on a batch of training data             output = net(data)             # calculate loss for the training data batch             loss_result = loss(output, label)         # calculate gradients         loss_result.backward()         # update parameters of the network         trainer.step(batch_size)         # sum losses of every batch         cumulative_train_loss += nd.sum(loss_result).asscalar()      return cumulative_train_loss   def validate_model(threshold):     cumulative_val_loss = 0      for i, (val_data, val_ground_truth_class) in enumerate(val_dataloader):         # do forward pass on a batch of validation data         output = net(val_data)         # calculate cumulative validation loss         cumulative_val_loss += nd.sum(loss(output, val_ground_truth_class)).asscalar()         # prediction as a sigmoid         prediction = net(val_data).sigmoid()         # converting neuron outputs to classes         predicted_classes = mx.nd.ceil(prediction - threshold)         # update validation accuracy         accuracy.update(val_ground_truth_class, predicted_classes.reshape(-1))         # calculate probabilities of belonging to different classes         prediction = prediction.reshape(-1)         probabilities = mx.nd.stack(1 - prediction, prediction, axis=1)          f1.update(val_ground_truth_class, probabilities)      return cumulative_val_loss   # train model and get metrics cumulative_train_loss = train_model() cumulative_val_loss = validate_model(threshold) net.collect_params().initialize() metrics_to_log = {     ""training_loss"": cumulative_train_loss,     ""val_loss"": cumulative_val_loss,     ""f1"": f1.get()[1],     ""accuracy"": accuracy.get()[1], } params_to_log = {""learning_rate"": trainer.learning_rate, ""threshold"": threshold}  # the model needs to be hybridized and run forward at least once before export is called net.hybridize() net.forward(train_x)  with mlflow.start_run():     mlflow.log_params(params_to_log)     mlflow.log_metrics(metrics_to_log)     model_info = mlflow.gluon.log_model(net, ""model"")  # load the model pytorch_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)  # make a prediction X = np.random.randn(10, 10) predictions = pytorch_pyfunc.predict(X) print(predictions)   For more information, see mlflow.gluon.    XGBoost (xgboost)  The xgboost model flavor enables logging of XGBoost models in MLflow format via the mlflow.xgboost.save_model() and mlflow.xgboost.log_model() methods in python and mlflow_save_model and mlflow_log_model in R respectively. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. You can also use the mlflow.xgboost.load_model() method to load MLflow Models with the xgboost model flavor in native XGBoost format. Note that the xgboost model flavor only supports an instance of xgboost.Booster, not models that implement the scikit-learn API.  XGBoost pyfunc usage  The example below  Loads the IRIS dataset from scikit-learn Trains an XGBoost Classifier Logs the model and params using mlflow Loads the logged model and makes predictions  from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from xgboost import XGBClassifier import mlflow from mlflow.models import infer_signature  data = load_iris() X_train, X_test, y_train, y_test = train_test_split(     data[""data""], data[""target""], test_size=0.2 )  xgb_classifier = XGBClassifier(     n_estimators=10,     max_depth=3,     learning_rate=1,     objective=""binary:logistic"",     random_state=123, )  # log fitted model and XGBClassifier parameters with mlflow.start_run():     xgb_classifier.fit(X_train, y_train)     clf_params = xgb_classifier.get_xgb_params()     mlflow.log_params(clf_params)     signature = infer_signature(X_train, xgb_classifier.predict(X_train))     model_info = mlflow.xgboost.log_model(         xgb_classifier, ""iris-classifier"", signature=signature     )  # Load saved model and make predictions xgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri) y_pred = xgb_classifier_saved.predict(X_test)   For more information, see mlflow.xgboost.    LightGBM (lightgbm)  The lightgbm model flavor enables logging of LightGBM models in MLflow format via the mlflow.lightgbm.save_model() and mlflow.lightgbm.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). You can also use the mlflow.lightgbm.load_model() method to load MLflow Models with the lightgbm model flavor in native LightGBM format. Note that the scikit-learn API for LightGBM is now supported. For more information, see mlflow.lightgbm.  LightGBM pyfunc usage  The example below  Loads the IRIS dataset from scikit-learn Trains a LightGBM LGBMClassifier Logs the model and feature importance’s using mlflow Loads the logged model and makes predictions  from lightgbm import LGBMClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import mlflow from mlflow.models import infer_signature  data = load_iris()  # Remove special characters from feature names to be able to use them as keys for mlflow metrics feature_names = [     name.replace("" "", ""_"").replace(""("", """").replace("")"", """")     for name in data[""feature_names""] ] X_train, X_test, y_train, y_test = train_test_split(     data[""data""], data[""target""], test_size=0.2 ) # create model instance lgb_classifier = LGBMClassifier(     n_estimators=10,     max_depth=3,     learning_rate=1,     objective=""binary:logistic"",     random_state=123, )  # Fit and save model and LGBMClassifier feature importances as mlflow metrics with mlflow.start_run():     lgb_classifier.fit(X_train, y_train)     feature_importances = dict(zip(feature_names, lgb_classifier.feature_importances_))     feature_importance_metrics = {         f""feature_importance_{feature_name}"": imp_value         for feature_name, imp_value in feature_importances.items()     }     mlflow.log_metrics(feature_importance_metrics)     signature = infer_signature(X_train, lgb_classifier.predict(X_train))     model_info = mlflow.lightgbm.log_model(         lgb_classifier, ""iris-classifier"", signature=signature     )  # Load saved model and make predictions lgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri) y_pred = lgb_classifier_saved.predict(X_test) print(y_pred)      CatBoost (catboost)  The catboost model flavor enables logging of CatBoost models in MLflow format via the mlflow.catboost.save_model() and mlflow.catboost.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). You can also use the mlflow.catboost.load_model() method to load MLflow Models with the catboost model flavor in native CatBoost format. For more information, see mlflow.catboost.  CatBoost pyfunc usage  For a CatBoost Classifier model, an example configuration for the pyfunc predict() method is: import mlflow from mlflow.models import infer_signature from catboost import CatBoostClassifier from sklearn import datasets  # prepare data X, y = datasets.load_wine(as_frame=False, return_X_y=True)  # train the model model = CatBoostClassifier(     iterations=5,     loss_function=""MultiClass"",     allow_writing_files=False, ) model.fit(X, y)  # create model signature predictions = model.predict(X) signature = infer_signature(X, predictions)  # log the model into a mlflow run with mlflow.start_run():     model_info = mlflow.catboost.log_model(model, ""model"", signature=signature)  # load the logged model and make a prediction catboost_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri) print(catboost_pyfunc.predict(X[:5]))      Spacy(spaCy)  The spaCy model flavor enables logging of spaCy models in MLflow format via the mlflow.spacy.save_model() and mlflow.spacy.log_model() methods. Additionally, these methods add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. You can also use the mlflow.spacy.load_model() method to load MLflow Models with the spacy model flavor in native spaCy format. For more information, see mlflow.spacy.  Spacy pyfunc usage  The example below shows how to train a Spacy TextCategorizer model, log the model artifact and metrics to the mlflow tracking server and then load the saved model to make predictions. For this example, we will be using the Polarity 2.0 dataset available in the nltk package. This dataset consists of 10000 positive and 10000 negative short movie reviews. First we convert the texts and sentiment labels (“pos” or “neg”) from NLTK native format to Spacy’s DocBin format: import pandas as pd import spacy from nltk.corpus import movie_reviews from spacy import Language from spacy.tokens import DocBin  nltk.download(""movie_reviews"")   def get_sentences(sentiment_type: str) -> pd.DataFrame:     """"""Reconstruct the sentences from the word lists for each review record for a specific ``sentiment_type``     as a pandas DataFrame with two columns: 'sentence' and 'sentiment'.     """"""     file_ids = movie_reviews.fileids(sentiment_type)     sent_df = []     for file_id in file_ids:         sentence = "" "".join(movie_reviews.words(file_id))         sent_df.append({""sentence"": sentence, ""sentiment"": sentiment_type})     return pd.DataFrame(sent_df)   def convert(data_df: pd.DataFrame, target_file: str):     """"""Convert a DataFrame with 'sentence' and 'sentiment' columns to a     spacy DocBin object and save it to 'target_file'.     """"""     nlp = spacy.blank(""en"")     sentiment_labels = data_df.sentiment.unique()     spacy_doc = DocBin()      for _, row in data_df.iterrows():         sent_tokens = nlp.make_doc(row[""sentence""])         # To train a Spacy TextCategorizer model, the label must be attached to the ""cats"" dictionary of the ""Doc""         # object, e.g. {""pos"": 1.0, ""neg"": 0.0} for a ""pos"" label.         for label in sentiment_labels:             sent_tokens.cats[label] = 1.0 if label == row[""sentiment""] else 0.0         spacy_doc.add(sent_tokens)      spacy_doc.to_disk(target_file)   # Build a single DataFrame with both positive and negative reviews, one row per review review_data = [get_sentences(sentiment_type) for sentiment_type in (""pos"", ""neg"")] review_data = pd.concat(review_data, axis=0)  # Split the DataFrame into a train and a dev set train_df = review_data.groupby(""sentiment"", group_keys=False).apply(     lambda x: x.sample(frac=0.7, random_state=100) ) dev_df = review_data.loc[review_data.index.difference(train_df.index), :]  # Save the train and dev data files to the current directory as ""corpora.train"" and ""corpora.dev"", respectively convert(train_df, ""corpora.train"") convert(dev_df, ""corpora.dev"")   To set up the training job, we first need to generate a configuration file as described in the Spacy Documentation For simplicity, we will only use a TextCategorizer in the pipeline. python -m spacy init config --pipeline textcat --lang en mlflow-textcat.cfg   Change the default train and dev paths in the config file to the current directory:   [paths] - train = null - dev = null + train = ""."" + dev = "".""   In Spacy, the training loop is defined internally in Spacy’s code. Spacy provides a “logging” extension point where we can use mlflow. To do this,  We have to define a function to write metrics / model input to mlfow Register it as a logger in Spacy’s component registry Change the default console logger in the Spacy’s configuration file (mlflow-textcat.cfg)  from typing import IO, Callable, Tuple, Dict, Any, Optional import spacy from spacy import Language import mlflow   @spacy.registry.loggers(""mlflow_logger.v1"") def mlflow_logger():     """"""Returns a function, ``setup_logger`` that returns two functions:      * ``log_step`` is called internally by Spacy for every evaluation step. We can log the intermediate train and     validation scores to the mlflow tracking server here.     * ``finalize``: is called internally by Spacy after training is complete. We can log the model artifact to the     mlflow tracking server here.     """"""      def setup_logger(         nlp: Language,         stdout: IO = sys.stdout,         stderr: IO = sys.stderr,     ) -> Tuple[Callable, Callable]:         def log_step(info: Optional[Dict[str, Any]]):             if info:                 step = info[""step""]                 score = info[""score""]                 metrics = {}                  for pipe_name in nlp.pipe_names:                     loss = info[""losses""][pipe_name]                     metrics[f""{pipe_name}_loss""] = loss                     metrics[f""{pipe_name}_score""] = score                 mlflow.log_metrics(metrics, step=step)          def finalize():             uri = mlflow.spacy.log_model(nlp, ""mlflow_textcat_example"")             mlflow.end_run()          return log_step, finalize      return setup_logger   Check the spacy-loggers library <https://pypi.org/project/spacy-loggers/> _ for a more complete implementation. Point to our mlflow logger in Spacy configuration file. For this example, we will lower the number of training steps and eval frequency:   [training.logger] - @loggers = ""spacy.ConsoleLogger.v1"" - dev = null + @loggers = ""mlflow_logger.v1""    [training] - max_steps = 20000 - eval_frequency = 100 + max_steps = 100 + eval_frequency = 10   Train our model: from spacy.cli.train import train as spacy_train  spacy_train(""mlflow-textcat.cfg"")   To make predictions, we load the saved model from the last run: from mlflow import MlflowClient  # look up the last run info from mlflow client = MlflowClient() last_run = client.search_runs(experiment_ids=[""0""], max_results=1)[0]  # We need to append the spacy model directory name to the artifact uri spacy_model = mlflow.pyfunc.load_model(     f""{last_run.info.artifact_uri}/mlflow_textcat_example"" ) predictions_in = dev_df.loc[:, [""sentence""]] predictions_out = spacy_model.predict(predictions_in).squeeze().tolist() predicted_labels = [     ""pos"" if row[""pos""] > row[""neg""] else ""neg"" for row in predictions_out ] print(dev_df.assign(predicted_sentiment=predicted_labels))      Fastai(fastai)  The fastai model flavor enables logging of fastai Learner models in MLflow format via the mlflow.fastai.save_model() and mlflow.fastai.log_model() methods. Additionally, these methods add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. You can also use the mlflow.fastai.load_model() method to load MLflow Models with the fastai model flavor in native fastai format. The interface for utilizing a fastai model loaded as a pyfunc type for generating predictions uses a Pandas DataFrame argument. This example runs the fastai tabular tutorial, logs the experiments, saves the model in fastai format and loads the model to get predictions using a fastai data loader: from fastai.data.external import URLs, untar_data from fastai.tabular.core import Categorify, FillMissing, Normalize, TabularPandas from fastai.tabular.data import TabularDataLoaders from fastai.tabular.learner import tabular_learner from fastai.data.transforms import RandomSplitter from fastai.metrics import accuracy from fastcore.basics import range_of import pandas as pd import mlflow import mlflow.fastai   def print_auto_logged_info(r):     tags = {k: v for k, v in r.data.tags.items() if not k.startswith(""mlflow."")}     artifacts = [         f.path for f in mlflow.MlflowClient().list_artifacts(r.info.run_id, ""model"")     ]     print(f""run_id: {r.info.run_id}"")     print(f""artifacts: {artifacts}"")     print(f""params: {r.data.params}"")     print(f""metrics: {r.data.metrics}"")     print(f""tags: {tags}"")   def main(epochs=5, learning_rate=0.01):     path = untar_data(URLs.ADULT_SAMPLE)     path.ls()      df = pd.read_csv(path / ""adult.csv"")      dls = TabularDataLoaders.from_csv(         path / ""adult.csv"",         path=path,         y_names=""salary"",         cat_names=[             ""workclass"",             ""education"",             ""marital-status"",             ""occupation"",             ""relationship"",             ""race"",         ],         cont_names=[""age"", ""fnlwgt"", ""education-num""],         procs=[Categorify, FillMissing, Normalize],     )      splits = RandomSplitter(valid_pct=0.2)(range_of(df))      to = TabularPandas(         df,         procs=[Categorify, FillMissing, Normalize],         cat_names=[             ""workclass"",             ""education"",             ""marital-status"",             ""occupation"",             ""relationship"",             ""race"",         ],         cont_names=[""age"", ""fnlwgt"", ""education-num""],         y_names=""salary"",         splits=splits,     )      dls = to.dataloaders(bs=64)      model = tabular_learner(dls, metrics=accuracy)      mlflow.fastai.autolog()      with mlflow.start_run() as run:         model.fit(5, 0.01)         mlflow.fastai.log_model(model, ""model"")      print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))      model_uri = f""runs:/{run.info.run_id}/model""     loaded_model = mlflow.fastai.load_model(model_uri)      test_df = df.copy()     test_df.drop([""salary""], axis=1, inplace=True)     dl = learn.dls.test_dl(test_df)      predictions, _ = loaded_model.get_preds(dl=dl)     px = pd.DataFrame(predictions).astype(""float"")     px.head(5)   main()   Output (Pandas DataFrame):        Index Probability of first class Probability of second class    0 0.545088 0.454912  1 0.503172 0.496828  2 0.962663 0.037337  3 0.206107 0.793893  4 0.807599 0.192401    Alternatively, when using the python_function flavor, get predictions from a DataFrame. from fastai.data.external import URLs, untar_data from fastai.tabular.core import Categorify, FillMissing, Normalize, TabularPandas from fastai.tabular.data import TabularDataLoaders from fastai.tabular.learner import tabular_learner from fastai.data.transforms import RandomSplitter from fastai.metrics import accuracy from fastcore.basics import range_of import pandas as pd import mlflow import mlflow.fastai  model_uri = ...  path = untar_data(URLs.ADULT_SAMPLE) df = pd.read_csv(path / ""adult.csv"") test_df = df.copy() test_df.drop([""salary""], axis=1, inplace=True)  loaded_model = mlflow.pyfunc.load_model(model_uri) loaded_model.predict(test_df)   Output (Pandas DataFrame):       Index Probability of first class, Probability of second class    0 [0.5450878, 0.45491222]  1 [0.50317234, 0.49682766]  2 [0.9626626, 0.037337445]  3 [0.20610662, 0.7938934]  4 [0.8075987, 0.19240129]    For more information, see mlflow.fastai.   Statsmodels (statsmodels)  The statsmodels model flavor enables logging of Statsmodels models in MLflow format via the mlflow.statsmodels.save_model() and mlflow.statsmodels.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. You can also use the mlflow.statsmodels.load_model() method to load MLflow Models with the statsmodels model flavor in native statsmodels format. As for now, automatic logging is restricted to parameters, metrics and models generated by a call to fit on a statsmodels model.  Statsmodels pyfunc usage  The following 2 examples illustrate usage of a basic regression model (OLS) and an ARIMA time series model from the following statsmodels apis : statsmodels.formula.api and statsmodels.tsa.api For a minimal statsmodels regression model, here is an example of the pyfunc predict() method : import mlflow import pandas as pd from sklearn.datasets import load_diabetes import statsmodels.formula.api as smf  # load the diabetes dataset from sklearn diabetes = load_diabetes()  # create X and y dataframes for the features and target X = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names) y = pd.DataFrame(data=diabetes.target, columns=[""target""])  # concatenate X and y dataframes df = pd.concat([X, y], axis=1)  # create the linear regression model (ordinary least squares) model = smf.ols(     formula=""target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6"", data=df )  mlflow.statsmodels.autolog(     log_models=True,     disable=False,     exclusive=False,     disable_for_unsupported_versions=False,     silent=False,     registered_model_name=None, )  with mlflow.start_run():     res = model.fit(method=""pinv"", use_t=True)     model_info = mlflow.statsmodels.log_model(res, artifact_path=""OLS_model"")  # load the pyfunc model statsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)  # generate predictions predictions = statsmodels_pyfunc.predict(X) print(predictions)   For a minimal time series ARIMA model, here is an example of the pyfunc predict() method : import mlflow import numpy as np import pandas as pd from statsmodels.tsa.arima.model import ARIMA  # create a time series dataset with seasonality np.random.seed(0)  # generate a time index with a daily frequency dates = pd.date_range(start=""2022-12-01"", end=""2023-12-01"", freq=""D"")  # generate the seasonal component (weekly) seasonality = np.sin(np.arange(len(dates)) * (2 * np.pi / 365.25) * 7)  # generate the trend component trend = np.linspace(-5, 5, len(dates)) + 2 * np.sin(     np.arange(len(dates)) * (2 * np.pi / 365.25) * 0.1 )  # generate the residual component residuals = np.random.normal(0, 1, len(dates))  # generate the final time series by adding the components time_series = seasonality + trend + residuals  # create a dataframe from the time series data = pd.DataFrame({""date"": dates, ""value"": time_series}) data.set_index(""date"", inplace=True)  order = (1, 0, 0) # create the ARIMA model model = ARIMA(data, order=order)  mlflow.statsmodels.autolog(     log_models=True,     disable=False,     exclusive=False,     disable_for_unsupported_versions=False,     silent=False,     registered_model_name=None, )  with mlflow.start_run():     res = model.fit()     mlflow.log_params(         {             ""order"": order,             ""trend"": model.trend,             ""seasonal_order"": model.seasonal_order,         }     )     mlflow.log_params(res.params)     mlflow.log_metric(""aic"", res.aic)     mlflow.log_metric(""bic"", res.bic)     model_info = mlflow.statsmodels.log_model(res, artifact_path=""ARIMA_model"")  # load the pyfunc model statsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)  # prediction dataframes for a TimeSeriesModel must have exactly one row and include columns called start and end start = pd.to_datetime(""2024-01-01"") end = pd.to_datetime(""2024-01-07"")  # generate predictions prediction_data = pd.DataFrame({""start"": start, ""end"": end}, index=[0]) predictions = statsmodels_pyfunc.predict(prediction_data) print(predictions)   For more information, see mlflow.statsmodels.    Prophet (prophet)  The prophet model flavor enables logging of Prophet models in MLflow format via the mlflow.prophet.save_model() and mlflow.prophet.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame input. You can also use the mlflow.prophet.load_model() method to load MLflow Models with the prophet model flavor in native prophet format.  Prophet pyfunc usage  This example uses a time series dataset from Prophet’s GitHub repository, containing log number of daily views to Peyton Manning’s Wikipedia page for several years. A sample of the dataset is as follows:       ds y    2007-12-10 9.59076113897809  2007-12-11 8.51959031601596  2007-12-12 8.18367658262066  2007-12-13 8.07246736935477    import numpy as np import pandas as pd from prophet import Prophet from prophet.diagnostics import cross_validation, performance_metrics  import mlflow from mlflow.models import infer_signature  # starts on 2007-12-10, ends on 2016-01-20 train_df = pd.read_csv(     ""https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv"" )  # Create a ""test"" DataFrame with the ""ds"" column containing 10 days after the end date in train_df test_dates = pd.date_range(start=""2016-01-21"", end=""2016-01-31"", freq=""D"") test_df = pd.Series(data=test_dates.values, name=""ds"").to_frame()  prophet_model = Prophet(changepoint_prior_scale=0.5, uncertainty_samples=7)  with mlflow.start_run():     prophet_model.fit(train_df)      # extract and log parameters such as changepoint_prior_scale in the mlflow run     model_params = {         name: value for name, value in vars(prophet_model).items() if np.isscalar(value)     }     mlflow.log_params(model_params)      # cross validate with 900 days of data initially, predictions for next 30 days     # walk forward by 30 days     cv_results = cross_validation(         prophet_model, initial=""900 days"", period=""30 days"", horizon=""30 days""     )      # Calculate metrics from cv_results, then average each metric across all backtesting windows and log to mlflow     cv_metrics = [""mse"", ""rmse"", ""mape""]     metrics_results = performance_metrics(cv_results, metrics=cv_metrics)     average_metrics = metrics_results.loc[:, cv_metrics].mean(axis=0).to_dict()     mlflow.log_metrics(average_metrics)      # Calculate model signature     train = prophet_model.history     predictions = prophet_model.predict(prophet_model.make_future_dataframe(30))     signature = infer_signature(train, predictions)      model_info = mlflow.prophet.log_model(         prophet_model, ""prophet-model"", signature=signature     )  # Load saved model prophet_model_saved = mlflow.pyfunc.load_model(model_info.model_uri) predictions = prophet_model_saved.predict(test_df)   Output (Pandas DataFrame):          Index ds yhat yhat_upper yhat_lower    0 2016-01-21 8.526513 8.827397 8.328563  1 2016-01-22 8.541355 9.434994 8.112758  2 2016-01-23 8.308332 8.633746 8.201323  3 2016-01-24 8.676326 9.534593 8.020874  4 2016-01-25 8.983457 9.430136 8.121798    For more information, see mlflow.prophet.    Pmdarima (pmdarima)  The pmdarima model flavor enables logging of pmdarima models in MLflow format via the mlflow.pmdarima.save_model() and mlflow.pmdarima.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the model to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with a DataFrame input. You can also use the mlflow.pmdarima.load_model() method to load MLflow Models with the pmdarima model flavor in native pmdarima formats. The interface for utilizing a pmdarima model loaded as a pyfunc type for generating forecast predictions uses a single-row Pandas DataFrame configuration argument. The following columns in this configuration Pandas DataFrame are supported:   n_periods (required) - specifies the number of future periods to generate starting from the last datetime valueof the training dataset, utilizing the frequency of the input training series when the model was trained. (for example, if the training data series elements represent one value per hour, in order to forecast 3 days of future data, set the column n_periods to 72.     X (optional) - exogenous regressor values (only supported in pmdarima version >= 1.8.0) a 2D array of values forfuture time period events. For more information, read the underlying library explanation.     return_conf_int (optional) - a boolean (Default: False) for whether to return confidence interval values.See above note.    alpha (optional) - the significance value for calculating confidence intervals. (Default: 0.05)  An example configuration for the pyfunc predict of a pmdarima model is shown below, with a future period prediction count of 100, a confidence interval calculation generation, no exogenous regressor elements, and a default alpha of 0.05:        Index n_periods return_conf_int    0 100 True     Warning The Pandas DataFrame passed to a pmdarima pyfunc flavor must only contain 1 row.   Note When predicting a pmdarima flavor, the predict method’s DataFrame configuration column return_conf_int’s value controls the output format. When the column’s value is set to False or None (which is the default if this column is not supplied in the configuration DataFrame), the schema of the returned Pandas DataFrame is a single column: [""yhat""]. When set to True, the schema of the returned DataFrame is: [""yhat"", ""yhat_lower"", ""yhat_upper""] with the respective lower (yhat_lower) and upper (yhat_upper) confidence intervals added to the forecast predictions (yhat).  Example usage of pmdarima artifact loaded as a pyfunc with confidence intervals calculated: import pmdarima import mlflow import pandas as pd  data = pmdarima.datasets.load_airpassengers()  with mlflow.start_run():     model = pmdarima.auto_arima(data, seasonal=True)     mlflow.pmdarima.save_model(model, ""/tmp/model.pmd"")  loaded_pyfunc = mlflow.pyfunc.load_model(""/tmp/model.pmd"")  prediction_conf = pd.DataFrame(     [{""n_periods"": 4, ""return_conf_int"": True, ""alpha"": 0.1}] )  predictions = loaded_pyfunc.predict(prediction_conf)   Output (Pandas DataFrame):         Index yhat yhat_lower yhat_upper    0 467.573731 423.30995 511.83751  1 490.494467 416.17449 564.81444  2 509.138684 420.56255 597.71117  3 492.554714 397.30634 587.80309     Warning Signature logging for pmdarima will not function correctly if return_conf_int is set to True from a non-pyfunc artifact. The output of the native ARIMA.predict() when returning confidence intervals is not a recognized signature type.    OpenAI (openai) (Experimental)  The full guide, including tutorials and detailed documentation for using the openai flavor can be viewed here.   LangChain (langchain) (Experimental)  The full guide, including tutorials and detailed documentation for using the langchain flavor can be viewed here.   John Snow Labs (johnsnowlabs) (Experimental)   Attention The johnsnowlabs flavor is in active development and is marked as Experimental. Public APIs may change and new features are subject to be added as additional functionality is brought to the flavor.  The johnsnowlabs model flavor gives you access to 20.000+ state-of-the-art enterprise NLP models in 200+ languages for medical, finance, legal and many more domains. You can use mlflow.johnsnowlabs.log_model() to log and export your model as mlflow.pyfunc.PyFuncModel. This enables you to integrate any John Snow Labs model into the MLflow framework. You can easily deploy your models for inference with MLflows serve functionalities. Models are interpreted as a generic Python function for inference via mlflow.pyfunc.load_model(). You can also use the mlflow.johnsnowlabs.load_model() function to load a saved or logged MLflow Model with the johnsnowlabs flavor from an stored artifact. Features include: LLM’s, Text Summarization, Question Answering, Named Entity Recognition, Relation Extraction, Sentiment Analysis, Spell Checking, Image Classification, Automatic Speech Recognition and much more, powered by the latest Transformer Architectures. The models are provided by John Snow Labs and requires a John Snow Labs Enterprise NLP License. You can reach out to us for a research or industry license. Example: Export a John Snow Labs to MLflow format import json import os  import pandas as pd from johnsnowlabs import nlp  import mlflow from mlflow.pyfunc import spark_udf  # 1) Write your raw license.json string into the 'JOHNSNOWLABS_LICENSE_JSON' env variable for MLflow creds = {     ""AWS_ACCESS_KEY_ID"": ""..."",     ""AWS_SECRET_ACCESS_KEY"": ""..."",     ""SPARK_NLP_LICENSE"": ""..."",     ""SECRET"": ""..."", } os.environ[""JOHNSNOWLABS_LICENSE_JSON""] = json.dumps(creds)  # 2) Install enterprise libraries nlp.install() # 3) Start a Spark session with enterprise libraries spark = nlp.start()  # 4) Load a model and test it nlu_model = ""en.classify.bert_sequence.covid_sentiment"" model_save_path = ""my_model"" johnsnowlabs_model = nlp.load(nlu_model) johnsnowlabs_model.predict([""I hate COVID,"", ""I love COVID""])  # 5) Export model with pyfunc and johnsnowlabs flavors with mlflow.start_run():     model_info = mlflow.johnsnowlabs.log_model(johnsnowlabs_model, model_save_path)  # 6) Load model with johnsnowlabs flavor mlflow.johnsnowlabs.load_model(model_info.model_uri)  # 7) Load model with pyfunc flavor mlflow.pyfunc.load_model(model_save_path)  pandas_df = pd.DataFrame({""text"": [""Hello World""]}) spark_df = spark.createDataFrame(pandas_df).coalesce(1) pyfunc_udf = spark_udf(     spark=spark,     model_uri=model_save_path,     env_manager=""virtualenv"",     result_type=""string"", ) new_df = spark_df.withColumn(""prediction"", pyfunc_udf(*pandas_df.columns))  # 9) You can now use the mlflow models serve command to serve the model see next section  # 10)  You can also use x command to deploy model inside of a container see next section    To deploy the John Snow Labs model as a container   Start the Docker Container  docker run -p 5001:8080 -e JOHNSNOWLABS_LICENSE_JSON=your_json_string ""mlflow-pyfunc""    Query server  curl http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '{   ""dataframe_split"": {       ""columns"": [""text""],       ""data"": [[""I hate covid""], [""I love covid""]]   } }'     To deploy the John Snow Labs model without a container   Export env variable and start server  export JOHNSNOWLABS_LICENSE_JSON=your_json_string mlflow models serve -m <model_uri>    Query server  curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{   ""dataframe_split"": {       ""columns"": [""text""],       ""data"": [[""I hate covid""], [""I love covid""]]   } }'      Diviner (diviner)  The diviner model flavor enables logging of diviner models in MLflow format via the mlflow.diviner.save_model() and mlflow.diviner.log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the model to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with a DataFrame input. You can also use the mlflow.diviner.load_model() method to load MLflow Models with the diviner model flavor in native diviner formats.  Diviner Types  Diviner is a library that provides an orchestration framework for performing time series forecasting on groups of related series. Forecasting in diviner is accomplished through wrapping popular open source libraries such as prophet and pmdarima. The diviner library offers a simplified set of APIs to simultaneously generate distinct time series forecasts for multiple data groupings using a single input DataFrame and a unified high-level API.   Metrics and Parameters logging for Diviner  Unlike other flavors that are supported in MLflow, Diviner has the concept of grouped models. As a collection of many (perhaps thousands) of individual forecasting models, the burden to the tracking server to log individual metrics and parameters for each of these models is significant. For this reason, metrics and parameters are exposed for retrieval from Diviner’s APIs as Pandas DataFrames, rather than discrete primitive values. To illustrate, let us assume we are forecasting hourly electricity consumption from major cities around the world. A sample of our input data looks like this:         country city datetime watts    US NewYork 2022-03-01 00:01:00 23568.9  US NewYork 2022-03-01 00:02:00 22331.7  US Boston 2022-03-01 00:01:00 14220.1  US Boston 2022-03-01 00:02:00 14183.4  CA Toronto 2022-03-01 00:01:00 18562.2  CA Toronto 2022-03-01 00:02:00 17681.6  MX MexicoCity 2022-03-01 00:01:00 19946.8  MX MexicoCity 2022-03-01 00:02:00 19444.0    If we were to fit a model on this data, supplying the grouping keys as: grouping_keys = [""country"", ""city""]   We will have a model generated for each of the grouping keys that have been supplied: [(""US"", ""NewYork""), (""US"", ""Boston""), (""CA"", ""Toronto""), (""MX"", ""MexicoCity"")]   With a model constructed for each of these, entering each of their metrics and parameters wouldn’t be an issue for the MLflow tracking server. What would become a problem, however, is if we modeled each major city on the planet and ran this forecasting scenario every day. If we were to adhere to the conditions of the World Bank, that would mean just over 10,000 models as of 2022. After a mere few weeks of running this forecasting every day we would have a very large metrics table. To eliminate this issue for large-scale forecasting, the metrics and parameters for diviner are extracted as a grouping key indexed Pandas DataFrame, as shown below for example (float values truncated for visibility):              grouping_key_columns country city mse rmse mae mape mdape smape    “(‘country’, ‘city’)” CA Toronto 8276851.6 2801.7 2417.7 0.16 0.16 0.159  “(‘country’, ‘city’)” MX MexicoCity 3548872.4 1833.8 1584.5 0.15 0.16 0.159  “(‘country’, ‘city’)” US NewYork 3167846.4 1732.4 1498.2 0.15 0.16 0.158  “(‘country’, ‘city’)” US Boston 14082666.4 3653.2 3156.2 0.15 0.16 0.159    There are two recommended means of logging the metrics and parameters from a diviner model :  Writing the DataFrames to local storage and using mlflow.log_artifacts()  import os import mlflow import tempfile  with tempfile.TemporaryDirectory() as tmpdir:     params = model.extract_model_params()     metrics = model.cross_validate_and_score(         horizon=""72 hours"",         period=""240 hours"",         initial=""480 hours"",         parallel=""threads"",         rolling_window=0.1,         monthly=False,     )     params.to_csv(f""{tmpdir}/params.csv"", index=False, header=True)     metrics.to_csv(f""{tmpdir}/metrics.csv"", index=False, header=True)      mlflow.log_artifacts(tmpdir, artifact_path=""data"")    Writing directly as a JSON artifact using mlflow.log_dict()   Note The parameters extract from diviner models may require casting (or dropping of columns) if using the pd.DataFrame.to_dict() approach due to the inability of this method to serialize objects.  import mlflow  params = model.extract_model_params() metrics = model.cross_validate_and_score(     horizon=""72 hours"",     period=""240 hours"",     initial=""480 hours"",     parallel=""threads"",     rolling_window=0.1,     monthly=False, ) params[""t_scale""] = params[""t_scale""].astype(str) params[""start""] = params[""start""].astype(str) params = params.drop(""stan_backend"", axis=1)  mlflow.log_dict(params.to_dict(), ""params.json"") mlflow.log_dict(metrics.to_dict(), ""metrics.json"")   Logging of the model artifact is shown in the pyfunc example below.   Diviner pyfunc usage  The MLflow Diviner flavor includes an implementation of the pyfunc interface for Diviner models. To control prediction behavior, you can specify configuration arguments in the first row of a Pandas DataFrame input. As this configuration is dependent upon the underlying model type (i.e., the diviner.GroupedProphet.forecast() method has a different signature than does diviner.GroupedPmdarima.predict()), the Diviner pyfunc implementation attempts to coerce arguments to the types expected by the underlying model.  Note Diviner models support both “full group” and “partial group” forecasting. If a column named ""groups"" is present in the configuration DataFrame submitted to the pyfunc flavor, the grouping key values in the first row will be used to generate a subset of forecast predictions. This functionality removes the need to filter a subset from the full output of all groups forecasts if the results of only a few (or one) groups are needed.  For a GroupedPmdarima model, an example configuration for the pyfunc predict() method is: import mlflow import pandas as pd from pmdarima.arima.auto import AutoARIMA from diviner import GroupedPmdarima  with mlflow.start_run():     base_model = AutoARIMA(out_of_sample_size=96, maxiter=200)     model = GroupedPmdarima(model_template=base_model).fit(         df=df,         group_key_columns=[""country"", ""city""],         y_col=""watts"",         datetime_col=""datetime"",         silence_warnings=True,     )      mlflow.diviner.save_model(diviner_model=model, path=""/tmp/diviner_model"")  diviner_pyfunc = mlflow.pyfunc.load_model(model_uri=""/tmp/diviner_model"")  predict_conf = pd.DataFrame(     {         ""n_periods"": 120,         ""groups"": [             (""US"", ""NewYork""),             (""CA"", ""Toronto""),             (""MX"", ""MexicoCity""),         ],  # NB: List of tuples required.         ""predict_col"": ""wattage_forecast"",         ""alpha"": 0.1,         ""return_conf_int"": True,         ""on_error"": ""warn"",     },     index=[0], )  subset_forecasts = diviner_pyfunc.predict(predict_conf)    Note There are several instances in which a configuration DataFrame submitted to the pyfunc predict() method will cause an MlflowException to be raised:   If neither horizon or n_periods are provided. The value of n_periods or horizon is not an integer. If the model is of type GroupedProphet, frequency as a string type must be provided. If both horizon and n_periods are provided with different values.       Transformers (transformers) (Experimental)  The full guide, including tutorials and detailed documentation for using the transformers flavor is available at this location.   SentenceTransformers (sentence_transformers) (Experimental)   Attention The sentence_transformers flavor is in active development and is marked as Experimental. Public APIs may change and new features are subject to be added as additional functionality is brought to the flavor.  The sentence_transformers model flavor enables logging of sentence-transformers models in MLflow format via the mlflow.sentence_transformers.save_model() and mlflow.sentence_transformers.log_model() functions. Use of these functions also adds the python_function flavor to the MLflow Models that they produce, allowing the model to be interpreted as a generic Python function for inference via mlflow.pyfunc.load_model(). You can also use the mlflow.sentence_transformers.load_model() function to load a saved or logged MLflow Model with the sentence_transformers flavor as a native sentence-transformers model. Example: from sentence_transformers import SentenceTransformer  import mlflow import mlflow.sentence_transformers  model = SentenceTransformer(""all-MiniLM-L6-v2"")  example_sentences = [""This is a sentence."", ""This is another sentence.""]  # Define the signature signature = mlflow.models.infer_signature(     model_input=example_sentences,     model_output=model.encode(example_sentences), )  # Log the model using mlflow with mlflow.start_run():     logged_model = mlflow.sentence_transformers.log_model(         model=model,         artifact_path=""sbert_model"",         signature=signature,         input_example=example_sentences,     )  # Load option 1: mlflow.pyfunc.load_model returns a PyFuncModel loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri) embeddings1 = loaded_model.predict([""hello world"", ""i am mlflow""])  # Load option 2: mlflow.sentence_transformers.load_model returns a SentenceTransformer loaded_model = mlflow.sentence_transformers.load_model(logged_model.model_uri) embeddings2 = loaded_model.encode([""hello world"", ""i am mlflow""])  print(embeddings1)  """""" >> [[-3.44772562e-02  3.10232025e-02  6.73496164e-03  2.61089969e-02   ...   2.37922110e-02 -2.28897743e-02  3.89375277e-02  3.02067865e-02]  [ 4.81191138e-03 -9.33756605e-02  6.95968643e-02  8.09735525e-03   ...    6.57437667e-02 -2.72239652e-02  4.02687863e-02 -1.05599344e-01]] """"""      Model Evaluation  After building and training your MLflow Model, you can use the mlflow.evaluate() API to evaluate its performance on one or more datasets of your choosing. mlflow.evaluate() currently supports evaluation of MLflow Models with the python_function (pyfunc) model flavor for classification, regression, and numerous language modeling tasks (see Evaluating with LLMs), computing a variety of task-specific performance metrics, model performance plots, and model explanations. Evaluation results are logged to MLflow Tracking. The following example from the MLflow GitHub Repository uses mlflow.evaluate() to evaluate the performance of a classifier on the UCI Adult Data Set, logging a comprehensive collection of MLflow Metrics and Artifacts that provide insight into model performance and behavior: import xgboost import shap import mlflow from mlflow.models import infer_signature from sklearn.model_selection import train_test_split  # Load the UCI Adult Dataset X, y = shap.datasets.adult()  # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.33, random_state=42 )  # Fit an XGBoost binary classifier on the training data split model = xgboost.XGBClassifier().fit(X_train, y_train)  # Create a model signature signature = infer_signature(X_test, model.predict(X_test))  # Build the Evaluation Dataset from the test set eval_data = X_test eval_data[""label""] = y_test  with mlflow.start_run() as run:     # Log the baseline model to MLflow     mlflow.sklearn.log_model(model, ""model"", signature=signature)     model_uri = mlflow.get_artifact_uri(""model"")      # Evaluate the logged model     result = mlflow.evaluate(         model_uri,         eval_data,         targets=""label"",         model_type=""classifier"",         evaluators=[""default""],     )      Evaluating with LLMs  As of MLflow 2.4.0, mlflow.evaluate() has built-in support for a variety of tasks with LLMs, including text summarization, text classification, question answering, and text generation. The following example uses mlflow.evaluate() to evaluate a model that answers questions about MLflow (note that you must have the OPENAI_API_TOKEN environment variable set in your current system environment in order to run the example): import os import pandas as pd  import mlflow import openai  # Create a question answering model using prompt engineering with OpenAI. Log the # prompt and the model to MLflow Tracking mlflow.start_run() system_prompt = (     ""Your job is to answer questions about MLflow. When you are asked a question about MLflow,""     "" respond to it. Make sure to include code examples. If the question is not related to""     "" MLflow, refuse to answer and say that the question is unrelated."" ) mlflow.log_param(""system_prompt"", system_prompt) logged_model = mlflow.openai.log_model(     model=""gpt-3.5-turbo"",     task=openai.chat.completions,     artifact_path=""model"",     messages=[         {""role"": ""system"", ""content"": system_prompt},         {""role"": ""user"", ""content"": ""{question}""},     ], )  # Evaluate the model on some example questions questions = pd.DataFrame(     {         ""question"": [             ""How do you create a run with MLflow?"",             ""How do you log a model with MLflow?"",             ""What is the capital of France?"",         ]     } ) mlflow.evaluate(     model=logged_model.model_uri,     model_type=""question-answering"",     data=questions, )  # Load and inspect the evaluation results results: pd.DataFrame = mlflow.load_table(     ""eval_results_table.json"", extra_columns=[""run_id"", ""params.system_prompt""] ) print(""Evaluation results:"") print(results)   MLflow also provides an Artifact View UI for comparing inputs and outputs across multiple models built with LLMs. For example, after evaluating multiple prompts for question answering (see the MLflow OpenAI question answering full example), you can navigate to the Artifact View to view the questions and compare the answers for each model:      For additional examples demonstrating the use of mlflow.evaluate() with LLMs, check out the MLflow LLMs example repository.   Evaluating with Extra Metrics  If the default set of metrics is insufficient, you can supply extra_metrics and custom_artifacts to mlflow.evaluate() to produce extra metrics and artifacts for the model(s) that you’re evaluating. To define an extra metric, you should define an eval_fn function that takes in predictions and targets as arguments and outputs a MetricValue object. predictions and targets are pandas.Series objects. If predictions or targets specified in mlflow.evaluate() is either numpy.array or List, they will be converted to pandas.Series. To use values from other metrics to compute your custom metric, include the name of the metric as an argument to eval_fn. This argument will contain a MetricValue object which contains the values calculated from the specified metric and can be used to compute your custom metric. {     ""accuracy_score"": MetricValue(         scores=None, justifications=None, aggregate_results={""accuracy_score"": 1.0}     ) }   The MetricValue class has three attributes:  scores: a list that contains per-row metrics. aggregate_results: a dictionary that maps the aggregation method names to the corresponding aggregated values. This is intended to be used to aggregate scores. justifications: a list that contains per-row justifications of the values in scores. This is optional, and is usually used with genai metrics.  The code block below demonstrates how to define a custom metric evaluation function: from mlflow.metrics import MetricValue   def my_metric_eval_fn(predictions, targets):     scores = np.abs(predictions - targets)     return MetricValue(         scores=list(scores),         aggregate_results={             ""mean"": np.mean(scores),             ""variance"": np.var(scores),             ""median"": np.median(scores),         },     )   Once you have defined an eval_fn, you then use make_metric() to wrap this eval_fn function into a metric. In addition to eval_fn, make_metric() requires an additional parameter , greater_is_better, for optimization purposes. This parameter indicates whether this is a metric we want to maximize or minimize. from mlflow.metrics import make_metric  mymetric = make_metric(eval_fn=my_metric_eval_fn, greater_is_better=False)   The extra metric allows you to either evaluate a model directly, or to evaluate an output dataframe. To evaluate the model directly, you will have to provide mlflow.evaluate() either a pyfunc model instance, a URI referring to a pyfunc model, or a callable function that takes in the data as input and outputs the predictions. def model(x):     return x[""inputs""]   eval_dataset = pd.DataFrame(     {         ""targets"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],         ""inputs"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],     } )  mlflow.evaluate(model, eval_dataset, targets=""targets"", extra_metrics=[mymetric])    To directly evaluate an output dataframe, you can omit the model parameter. However, you will needto set the predictions parameter in mlflow.evaluate() in order to evaluate an inference output dataframe.   eval_dataset = pd.DataFrame(     {         ""targets"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],         ""predictions"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],     } )  result = mlflow.evaluate(     data=eval_dataset,     predictions=""predictions"",     targets=""targets"",     extra_metrics=[mymetric], )   When your model has multiple outputs, the model must return a pandas DataFrame with multiple columns. You must specify one column among the model output columns as the predictions column using the predictions parameter, and other output columns of the model will be accessible from the eval_fn based on their column names. For example, if your model has two outputs retrieved_context and answer, you can specify answer as the predictions column, and retrieved_context column will be accessible as the context parameter from eval_fn via col_mapping: def eval_fn(predictions, targets, context):     scores = (predictions == targets) + context     return MetricValue(         scores=list(scores),         aggregate_results={""mean"": np.mean(scores), ""sum"": np.sum(scores)},     )   mymetric = make_metric(eval_fn=eval_fn, greater_is_better=False, name=""mymetric"")   def model(x):     return pd.DataFrame({""retrieved_context"": x[""inputs""] + 1, ""answer"": x[""inputs""]})   eval_dataset = pd.DataFrame(     {         ""targets"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],         ""inputs"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],     } )  config = {""col_mapping"": {""context"": ""retrieved_context""}}  result = mlflow.evaluate(     model,     eval_dataset,     predictions=""answer"",     targets=""targets"",     extra_metrics=[mymetric],     evaluator_config=config, )   However, you can also avoid using col_mapping if the parameter of eval_fn is the same as the output column name of the model. def eval_fn(predictions, targets, retrieved_context):     scores = (predictions == targets) + retrieved_context     return MetricValue(         scores=list(scores),         aggregate_results={""mean"": np.mean(scores), ""sum"": np.sum(scores)},     )   mymetric = make_metric(eval_fn=eval_fn, greater_is_better=False, name=""mymetric"")   def model(x):     return pd.DataFrame({""retrieved_context"": x[""inputs""] + 1, ""answer"": x[""inputs""]})   eval_dataset = pd.DataFrame(     {         ""targets"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],         ""inputs"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],     } )  result = mlflow.evaluate(     model,     eval_dataset,     predictions=""answer"",     targets=""targets"",     extra_metrics=[mymetric], )   col_mapping also allows you to pass additional parameters to the extra metric function, in this case passing a value k. def eval_fn(predictions, targets, k):     scores = k * (predictions == targets)     return MetricValue(scores=list(scores), aggregate_results={""mean"": np.mean(scores)})   weighted_mymetric = make_metric(eval_fn=eval_fn, greater_is_better=False)   def model(x):     return x[""inputs""]   eval_dataset = pd.DataFrame(     {         ""targets"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],         ""inputs"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],     } )  config = {""col_mapping"": {""k"": 5}} mlflow.evaluate(     model,     eval_dataset,     targets=""targets"",     extra_metrics=[weighted_mymetric],     evaluator_config=config, )   You can also add the name of other metrics as an argument to the extra metric function, which will pass in the MetricValue calculated for that metric. def eval_fn(predictions, targets, retrieved_context):     scores = (predictions == targets) + retrieved_context     return MetricValue(         scores=list(scores),         aggregate_results={""mean"": np.mean(scores), ""sum"": np.sum(scores)},     )   mymetric = make_metric(eval_fn=eval_fn, greater_is_better=False, name=""mymetric"")   def eval_fn_2(predictions, targets, mymetric):     scores = [""true"" if score else ""false"" for score in mymetric.scores]     return MetricValue(         scores=list(scores),     )   mymetric2 = make_metric(eval_fn=eval_fn_2, greater_is_better=False, name=""mymetric2"")   def model(x):     return pd.DataFrame({""retrieved_context"": x[""inputs""] + 1, ""answer"": x[""inputs""]})   eval_dataset = pd.DataFrame(     {         ""targets"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],         ""inputs"": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],     } )  result = mlflow.evaluate(     model,     eval_dataset,     predictions=""answer"",     targets=""targets"",     extra_metrics=[mymetric, mymetric2], )   The following short example from the MLflow GitHub Repository uses mlflow.evaluate() with an extra metric function to evaluate the performance of a regressor on the California Housing Dataset. import os  import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  import mlflow from mlflow.models import infer_signature, make_metric  # loading the California housing dataset cali_housing = fetch_california_housing(as_frame=True)  # split the dataset into train and test partitions X_train, X_test, y_train, y_test = train_test_split(     cali_housing.data, cali_housing.target, test_size=0.2, random_state=123 )  # train the model lin_reg = LinearRegression().fit(X_train, y_train)  # Infer model signature predictions = lin_reg.predict(X_train) signature = infer_signature(X_train, predictions)  # creating the evaluation dataframe eval_data = X_test.copy() eval_data[""target""] = y_test   def squared_diff_plus_one(eval_df, _builtin_metrics):     """"""     This example custom metric function creates a metric based on the ``prediction`` and     ``target`` columns in ``eval_df`.     """"""     return np.sum(np.abs(eval_df[""prediction""] - eval_df[""target""] + 1) ** 2)   def sum_on_target_divided_by_two(_eval_df, builtin_metrics):     """"""     This example custom metric function creates a metric derived from existing metrics in     ``builtin_metrics``.     """"""     return builtin_metrics[""sum_on_target""] / 2   def prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):     """"""     This example custom artifact generates and saves a scatter plot to ``artifacts_dir`` that     visualizes the relationship between the predictions and targets for the given model to a     file as an image artifact.     """"""     plt.scatter(eval_df[""prediction""], eval_df[""target""])     plt.xlabel(""Targets"")     plt.ylabel(""Predictions"")     plt.title(""Targets vs. Predictions"")     plot_path = os.path.join(artifacts_dir, ""example_scatter_plot.png"")     plt.savefig(plot_path)     return {""example_scatter_plot_artifact"": plot_path}   with mlflow.start_run() as run:     mlflow.sklearn.log_model(lin_reg, ""model"", signature=signature)     model_uri = mlflow.get_artifact_uri(""model"")     result = mlflow.evaluate(         model=model_uri,         data=eval_data,         targets=""target"",         model_type=""regressor"",         evaluators=[""default""],         extra_metrics=[             make_metric(                 eval_fn=squared_diff_plus_one,                 greater_is_better=False,             ),             make_metric(                 eval_fn=sum_on_target_divided_by_two,                 greater_is_better=True,             ),         ],         custom_artifacts=[prediction_target_scatter],     )  print(f""metrics:\n{result.metrics}"") print(f""artifacts:\n{result.artifacts}"")   For a more comprehensive extra metrics usage example, refer to this example from the MLflow GitHub Repository.   Evaluating with a Function  As of MLflow 2.8.0, mlflow.evaluate() supports evaluating a python function without requiring logging the model to MLflow. This is useful when you don’t want to log the model and just want to evaluate it. The requirements for the function’s input and output are the same as the requirements for a model’s input and output. The following example uses mlflow.evaluate() to evaluate a function: import shap import xgboost from sklearn.model_selection import train_test_split  import mlflow  # Load the UCI Adult Dataset X, y = shap.datasets.adult()  # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)  # Fit an XGBoost binary classifier on the training data split model = xgboost.XGBClassifier().fit(X_train, y_train)  # Build the Evaluation Dataset from the test set eval_data = X_test eval_data[""label""] = y_test   # Define a function that calls the model's predict method def fn(X):     return model.predict(X)   with mlflow.start_run() as run:     # Evaluate the function without logging the model     result = mlflow.evaluate(         fn,         eval_data,         targets=""label"",         model_type=""classifier"",         evaluators=[""default""],     )  print(f""metrics:\n{result.metrics}"") print(f""artifacts:\n{result.artifacts}"")     Evaluating with a Static Dataset  As of MLflow 2.8.0, mlflow.evaluate() supports evaluating a static dataset without specifying a model. This is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and want to evaluate the static dataset without re-running the model. If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the top-level predictions parameter in mlflow.evaluate(): # Assume that the model output is saved to the pandas_df[""model_output""] column mlflow.evaluate(data=pandas_df, predictions=""model_output"", ...)   If you are using an MLflow PandasDataset, you must specify the column name that contains the model output using the predictions parameter in mlflow.data.from_pandas(), and specify None for the predictions parameter in mlflow.evaluate(): # Assume that the model output is saved to the pandas_df[""model_output""] column dataset = mlflow.data.from_pandas(pandas_df, predictions=""model_output"") mlflow.evaluate(data=pandas_df, predictions=None, ...)   When your model has multiple outputs, you must specify one column among the model output columns as the predictions column. The other output columns of the model will be treated as “input” columns. For example, if your model has two outputs named retrieved_context and answer, you can specify answer as the predictions column. The retrieved_context column will be treated as an “input” column when calculating the metrics. The following example uses mlflow.evaluate() to evaluate a static dataset: import shap import xgboost from sklearn.model_selection import train_test_split  import mlflow  # Load the UCI Adult Dataset X, y = shap.datasets.adult()  # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)  # Fit an XGBoost binary classifier on the training data split model = xgboost.XGBClassifier().fit(X_train, y_train)  # Build the Evaluation Dataset from the test set y_test_pred = model.predict(X=X_test) eval_data = X_test eval_data[""label""] = y_test eval_data[""predictions""] = y_test_pred   with mlflow.start_run() as run:     # Evaluate the static dataset without providing a model     result = mlflow.evaluate(         data=eval_data,         targets=""label"",         predictions=""predictions"",         model_type=""classifier"",     )  print(f""metrics:\n{result.metrics}"") print(f""artifacts:\n{result.artifacts}"")     Performing Model Validation  You can also use the mlflow.evaluate() API to perform some checks on the metrics generated during model evaluation to validate the quality of your model. By specifying a validation_thresholds dictionary mapping metric names to mlflow.models.MetricThreshold objects, you can specify value thresholds that your model’s evaluation metrics must exceed as well as absolute and relative gains your model must have in comparison to a specified baseline_model. If your model fails to clear specified thresholds, mlflow.evaluate() will throw a ModelValidationFailedException detailing the validation failure. import xgboost import shap from sklearn.model_selection import train_test_split from sklearn.dummy import DummyClassifier import mlflow from mlflow.models import infer_signature from mlflow.models import MetricThreshold  # load UCI Adult Data Set; segment it into training and test sets X, y = shap.datasets.adult() X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.33, random_state=42 )  # train a candidate XGBoost model candidate_model = xgboost.XGBClassifier().fit(X_train, y_train)  # train a baseline dummy model baseline_model = DummyClassifier(strategy=""uniform"").fit(X_train, y_train)  # create signature that is shared by the two models signature = infer_signature(X_test, y_test)  # construct an evaluation dataset from the test set eval_data = X_test eval_data[""label""] = y_test  # Define criteria for model to be validated against thresholds = {     ""accuracy_score"": MetricThreshold(         threshold=0.8,  # accuracy should be >=0.8         min_absolute_change=0.05,  # accuracy should be at least 0.05 greater than baseline model accuracy         min_relative_change=0.05,  # accuracy should be at least 5 percent greater than baseline model accuracy         greater_is_better=True,     ), }  with mlflow.start_run() as run:     candidate_model_uri = mlflow.sklearn.log_model(         candidate_model, ""candidate_model"", signature=signature     ).model_uri     baseline_model_uri = mlflow.sklearn.log_model(         baseline_model, ""baseline_model"", signature=signature     ).model_uri      mlflow.evaluate(         candidate_model_uri,         eval_data,         targets=""label"",         model_type=""classifier"",         validation_thresholds=thresholds,         baseline_model=baseline_model_uri,     )   Refer to mlflow.models.MetricThreshold to see details on how the thresholds are specified and checked. For a more comprehensive demonstration on how to use mlflow.evaluate() to perform model validation, refer to the Model Validation example from the MLflow GitHub Repository. The logged output within the MLflow UI for the comprehensive example is shown below. Note the two model artifacts that have been logged: ‘baseline_model’ and ‘candidate_model’ for comparison purposes in the example.   Note Limitations (when the default evaluator is used):  Model validation results are not included in the active MLflow run. No metrics are logged nor artifacts produced for the baseline model in the active MLflow run.   Additional information about model evaluation behaviors and outputs is available in the mlflow.evaluate() API docs.  Note There are plugins that support in-depth model validation with features that are not supported directly in MLflow. To learn more, see:  Model Validation with Giskard’s plugin Model Validation with Trubrics’ plugin.    Note Differences in the computation of Area under Curve Precision Recall score (metric name precision_recall_auc) between multi and binary classifiers: Multiclass classifier models, when evaluated, utilize the standard scoring metric from sklearn: sklearn.metrics.roc_auc_score to calculate the area under the precision recall curve. This algorithm performs a linear interpolation calculation utilizing the trapezoidal rule to estimate the area under the precision recall curve. It is well-suited for use in evaluating multi-class classification models to provide a single numeric value of the quality of fit. Binary classifier models, on the other hand, use the sklearn.metrics.average_precision_score to avoid the shortcomings of the roc_auc_score implementation when applied to heavily imbalanced classes in binary classification. Usage of the roc_auc_score for imbalanced datasets can give a misleading result (optimistically better than the model’s actual ability to accurately predict the minority class membership). For additional information on the topic of why different algorithms are employed for this, as well as links to the papers that informed the implementation of these metrics within the sklearn.metrics module, refer to the documentation. For simplicity purposes, both methodologies evaluation metric results (whether for multi-class or binary classification) are unified in the single metric: precision_recall_auc.    Model Validation with Giskard’s plugin  To extend the validation capabilities of MLflow and anticipate issues before they go to production, a plugin has been built by Giskard allowing users to:   scan a model in order to detect hidden vulnerabilities such as Performance bias, Unrobustness, Overconfidence, Underconfidence, Ethical bias, Data leakage, Stochasticity, Spurious correlation, and others explore samples in the data that highlight the vulnerabilities found log the vulnerabilities as well-defined and quantified metrics compare the metrics across different models   See the following plugin example notebooks for a demo:   Tabular ML models Text ML models (LLMs)   For more information on the plugin, see the giskard-mlflow docs.   Model Validation with Trubrics’ plugin  To extend the validation capabilities of MLflow, a plugin has been built by Trubrics allowing users:   to use a large number of out-of-the-box validations to validate a run with any custom python functions to view all validation results in a .json file, for diagnosis of why an MLflow run could have failed   See the plugin example notebook for a demo. For more information on the plugin, see the trubrics-mlflow docs.    Model Customization  While MLflow’s built-in model persistence utilities are convenient for packaging models from various popular ML libraries in MLflow Model format, they do not cover every use case. For example, you may want to use a model from an ML library that is not explicitly supported by MLflow’s built-in flavors. Alternatively, you may want to package custom inference code and data to create an MLflow Model. Fortunately, MLflow provides two solutions that can be used to accomplish these tasks: Custom Python Models and Custom Flavors.  In this section:  Custom Python Models  Example: Creating a custom “add n” model Example: Saving an XGBoost model in MLflow format Example: Logging a transformers model with hf:/ schema to avoid copying large files   Custom Flavors  Example: Creating a custom “sktime” flavor Example: Using the custom “sktime” flavor      Custom Python Models  The mlflow.pyfunc module provides save_model() and log_model() utilities for creating MLflow Models with the python_function flavor that contain user-specified code and artifact (file) dependencies. These artifact dependencies may include serialized models produced by any Python ML library. Because these custom models contain the python_function flavor, they can be deployed to any of MLflow’s supported production environments, such as SageMaker, AzureML, or local REST endpoints. The following examples demonstrate how you can use the mlflow.pyfunc module to create custom Python models. For additional information about model customization with MLflow’s python_function utilities, see the python_function custom models documentation.  Example: Creating a custom “add n” model  This example defines a class for a custom model that adds a specified numeric value, n, to all columns of a Pandas DataFrame input. Then, it uses the mlflow.pyfunc APIs to save an instance of this model with n = 5 in MLflow Model format. Finally, it loads the model in python_function format and uses it to evaluate a sample input. import mlflow.pyfunc   # Define the model class class AddN(mlflow.pyfunc.PythonModel):     def __init__(self, n):         self.n = n      def predict(self, context, model_input, params=None):         return model_input.apply(lambda column: column + self.n)   # Construct and save the model model_path = ""add_n_model"" add5_model = AddN(n=5) mlflow.pyfunc.save_model(path=model_path, python_model=add5_model)  # Load the model in `python_function` format loaded_model = mlflow.pyfunc.load_model(model_path)  # Evaluate the model import pandas as pd  model_input = pd.DataFrame([range(10)]) model_output = loaded_model.predict(model_input) assert model_output.equals(pd.DataFrame([range(5, 15)]))     Example: Saving an XGBoost model in MLflow format  This example begins by training and saving a gradient boosted tree model using the XGBoost library. Next, it defines a wrapper class around the XGBoost model that conforms to MLflow’s python_function inference API. Then, it uses the wrapper class and the saved XGBoost model to construct an MLflow Model that performs inference using the gradient boosted tree. Finally, it loads the MLflow Model in python_function format and uses it to evaluate test data. # Load training and test datasets from sys import version_info import xgboost as xgb from sklearn import datasets from sklearn.model_selection import train_test_split  PYTHON_VERSION = f""{version_info.major}.{version_info.minor}.{version_info.micro}"" iris = datasets.load_iris() x = iris.data[:, 2:] y = iris.target x_train, x_test, y_train, _ = train_test_split(x, y, test_size=0.2, random_state=42) dtrain = xgb.DMatrix(x_train, label=y_train)  # Train and save an XGBoost model xgb_model = xgb.train(params={""max_depth"": 10}, dtrain=dtrain, num_boost_round=10) xgb_model_path = ""xgb_model.pth"" xgb_model.save_model(xgb_model_path)  # Create an `artifacts` dictionary that assigns a unique name to the saved XGBoost model file. # This dictionary will be passed to `mlflow.pyfunc.save_model`, which will copy the model file # into the new MLflow Model's directory. artifacts = {""xgb_model"": xgb_model_path}  # Define the model class import mlflow.pyfunc   class XGBWrapper(mlflow.pyfunc.PythonModel):     def load_context(self, context):         import xgboost as xgb          self.xgb_model = xgb.Booster()         self.xgb_model.load_model(context.artifacts[""xgb_model""])      def predict(self, context, model_input, params=None):         input_matrix = xgb.DMatrix(model_input.values)         return self.xgb_model.predict(input_matrix)   # Create a Conda environment for the new MLflow Model that contains all necessary dependencies. import cloudpickle  conda_env = {     ""channels"": [""defaults""],     ""dependencies"": [         f""python={PYTHON_VERSION}"",         ""pip"",         {             ""pip"": [                 f""mlflow=={mlflow.__version__}"",                 f""xgboost=={xgb.__version__}"",                 f""cloudpickle=={cloudpickle.__version__}"",             ],         },     ],     ""name"": ""xgb_env"", }  # Save the MLflow Model mlflow_pyfunc_model_path = ""xgb_mlflow_pyfunc"" mlflow.pyfunc.save_model(     path=mlflow_pyfunc_model_path,     python_model=XGBWrapper(),     artifacts=artifacts,     conda_env=conda_env, )  # Load the model in `python_function` format loaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)  # Evaluate the model import pandas as pd  test_predictions = loaded_model.predict(pd.DataFrame(x_test)) print(test_predictions)     Example: Logging a transformers model with hf:/ schema to avoid copying large files  This example shows how to use a special schema hf:/ to log a transformers model from huggingface hub directly. This is useful when the model is too large and especially when you want to serve the model directly, but it doesn’t save extra space if you want to download and test the model locally. import mlflow from mlflow.models import infer_signature import numpy as np import transformers   # Define a custom PythonModel class QAModel(mlflow.pyfunc.PythonModel):     def load_context(self, context):         """"""         This method initializes the tokenizer and language model         using the specified snapshot location from model context.         """"""         snapshot_location = context.artifacts[""bert-tiny-model""]         # Initialize tokenizer and language model         tokenizer = transformers.AutoTokenizer.from_pretrained(snapshot_location)         model = transformers.BertForQuestionAnswering.from_pretrained(snapshot_location)         self.pipeline = transformers.pipeline(             task=""question-answering"", model=model, tokenizer=tokenizer         )      def predict(self, context, model_input, params=None):         question = model_input[""question""][0]         if isinstance(question, np.ndarray):             question = question.item()         ctx = model_input[""context""][0]         if isinstance(ctx, np.ndarray):             ctx = ctx.item()         return self.pipeline(question=question, context=ctx)   # Log the model data = {""question"": ""Who's house?"", ""context"": ""The house is owned by Run.""} pyfunc_artifact_path = ""question_answering_model"" with mlflow.start_run() as run:     model_info = mlflow.pyfunc.log_model(         artifact_path=pyfunc_artifact_path,         python_model=QAModel(),         artifacts={""bert-tiny-model"": ""hf:/prajjwal1/bert-tiny""},         input_example=data,         signature=infer_signature(data, [""Run""]),         extra_pip_requirements=[""torch"", ""accelerate"", ""transformers"", ""numpy""],     )      Custom Flavors  You can also create custom MLflow Models by writing a custom flavor. As discussed in the Model API and Storage Format sections, an MLflow Model is defined by a directory of files that contains an MLmodel configuration file. This MLmodel file describes various model attributes, including the flavors in which the model can be interpreted. The MLmodel file contains an entry for each flavor name; each entry is a YAML-formatted collection of flavor-specific attributes. To create a new flavor to support a custom model, you define the set of flavor-specific attributes to include in the MLmodel configuration file, as well as the code that can interpret the contents of the model directory and the flavor’s attributes. A detailed example of constructing a custom model flavor and its usage is shown below. New custom flavors not considered for official inclusion into MLflow should be introduced as separate GitHub repositories with documentation provided in the Community Model Flavors page.  Example: Creating a custom “sktime” flavor  This example illustrates the creation of a custom flavor for sktime time series library. The library provides a unified interface for multiple learning  tasks including time series forecasting. While the custom flavor in this example is specific in terms of the sktime inference API and model serialization format, its interface design is similar to many of the existing built-in flavors. Particularly, the interface for utilizing the custom model loaded as a python_function flavor for generating predictions uses a single-row Pandas DataFrame configuration argument to expose the paramters of the sktime inference API. The complete code for this example is included in the flavor.py module of the sktime example directory. Let’s examine the custom flavor module in more detail. The first step is to import several modules inluding sktime library, various MLflow utilities as well as the MLflow pyfunc module which is required to add the pyfunc specification to the MLflow model configuration. Note also the import of the flavor module itself. This will be passed to the mlflow.models.Model.log() method to log the model as an artifact to the current MLflow run. import logging import os import pickle  import flavor import mlflow import numpy as np import pandas as pd import sktime import yaml from mlflow import pyfunc from mlflow.exceptions import MlflowException from mlflow.models import Model from mlflow.models.model import MLMODEL_FILE_NAME from mlflow.models.utils import _save_example from mlflow.protos.databricks_pb2 import INTERNAL_ERROR, INVALID_PARAMETER_VALUE from mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS from mlflow.tracking.artifact_utils import _download_artifact_from_uri from mlflow.utils.environment import (     _CONDA_ENV_FILE_NAME,     _CONSTRAINTS_FILE_NAME,     _PYTHON_ENV_FILE_NAME,     _REQUIREMENTS_FILE_NAME,     _mlflow_conda_env,     _process_conda_env,     _process_pip_requirements,     _PythonEnv,     _validate_env_arguments, ) from mlflow.utils.file_utils import write_to from mlflow.utils.model_utils import (     _add_code_from_conf_to_system_path,     _get_flavor_configuration,     _validate_and_copy_code_paths,     _validate_and_prepare_target_save_path, ) from mlflow.utils.requirements_utils import _get_pinned_requirement from sktime.utils.multiindex import flatten_multiindex  _logger = logging.getLogger(__name__)   We continue by defining a set of important variables used throughout the code that follows. The flavor name needs to be provided for every custom flavor and should reflect the name of the library to be supported. It is saved as part of the flavor-specific attributes to the MLmodel configuration file. This example also defines some sktime specific variables. For illustration purposes, only a subset of the available predict methods to be exposed via the _SktimeModelWrapper class is included when loading the model in its python_function flavor (additional methods could be added in a similar fashion). Additionaly, the model serialization formats, namely pickle (default) and cloudpickle, are defined. Note that both serialization modules require using the same python environment (version) in whatever environment this model is used for inference to ensure that the model will load with the appropriate version of pickle (cloudpickle). FLAVOR_NAME = ""sktime""  SKTIME_PREDICT = ""predict"" SKTIME_PREDICT_INTERVAL = ""predict_interval"" SKTIME_PREDICT_QUANTILES = ""predict_quantiles"" SKTIME_PREDICT_VAR = ""predict_var"" SUPPORTED_SKTIME_PREDICT_METHODS = [     SKTIME_PREDICT,     SKTIME_PREDICT_INTERVAL,     SKTIME_PREDICT_QUANTILES,     SKTIME_PREDICT_VAR, ]  SERIALIZATION_FORMAT_PICKLE = ""pickle"" SERIALIZATION_FORMAT_CLOUDPICKLE = ""cloudpickle"" SUPPORTED_SERIALIZATION_FORMATS = [     SERIALIZATION_FORMAT_PICKLE,     SERIALIZATION_FORMAT_CLOUDPICKLE, ]   Similar to the MLflow built-in flavors, a custom flavor logs the model in MLflow format via the save_model() and log_model() functions. In the save_model() function, the sktime model is saved to a specified output directory. Additionally, save_model() leverages the mlflow.models.Model.add_flavor() and mlflow.models.Model.save() methods to produce the MLmodel configuration containing the sktime and the python_function flavor. The resulting configuration has several flavor-specific attributes, such as the flavor name and sktime_version, which denotes the version of the sktime library that was used to train the model. An example of the output directoy for the custom sktime model is shown below: # Directory written by flavor.save_model(model, ""my_model"") my_model/ ├── MLmodel ├── conda.yaml ├── model.pkl ├── python_env.yaml └── requirements.txt   And its YAML-formatted MLmodel file describes the two flavors: flavors:   python_function:     env:       conda: conda.yaml       virtualenv: python_env.yaml     loader_module: flavor     model_path: model.pkl     python_version: 3.8.15   sktime:     code: null     pickled_model: model.pkl     serialization_format: pickle     sktime_version: 0.16.0   The save_model() function also provides flexibility to add additional paramters which can be added as flavor-specific attributes to the model configuration. In this example there is only one flavor-specific parameter for specifying the model serialization format. All other paramters are non-flavor specific (for a detailed description of these parameters take a look at mlflow.sklearn.save_model). Note: When creating your own custom flavor, be sure rename the sktime_model parameter in both the save_model() and log_model() functions to reflect the name of your custom model flavor. def save_model(     sktime_model,     path,     conda_env=None,     code_paths=None,     mlflow_model=None,     signature=None,     input_example=None,     pip_requirements=None,     extra_pip_requirements=None,     serialization_format=SERIALIZATION_FORMAT_PICKLE, ):     _validate_env_arguments(conda_env, pip_requirements, extra_pip_requirements)      if serialization_format not in SUPPORTED_SERIALIZATION_FORMATS:         raise MlflowException(             message=(                 f""Unrecognized serialization format: {serialization_format}. ""                 ""Please specify one of the following supported formats: ""                 ""{SUPPORTED_SERIALIZATION_FORMATS}.""             ),             error_code=INVALID_PARAMETER_VALUE,         )      _validate_and_prepare_target_save_path(path)     code_dir_subpath = _validate_and_copy_code_paths(code_paths, path)      if mlflow_model is None:         mlflow_model = Model()     if signature is not None:         mlflow_model.signature = signature     if input_example is not None:         _save_example(mlflow_model, input_example, path)      model_data_subpath = ""model.pkl""     model_data_path = os.path.join(path, model_data_subpath)     _save_model(         sktime_model, model_data_path, serialization_format=serialization_format     )      pyfunc.add_to_model(         mlflow_model,         loader_module=""flavor"",         model_path=model_data_subpath,         conda_env=_CONDA_ENV_FILE_NAME,         python_env=_PYTHON_ENV_FILE_NAME,         code=code_dir_subpath,     )      mlflow_model.add_flavor(         FLAVOR_NAME,         pickled_model=model_data_subpath,         sktime_version=sktime.__version__,         serialization_format=serialization_format,         code=code_dir_subpath,     )     mlflow_model.save(os.path.join(path, MLMODEL_FILE_NAME))      if conda_env is None:         if pip_requirements is None:             include_cloudpickle = (                 serialization_format == SERIALIZATION_FORMAT_CLOUDPICKLE             )             default_reqs = get_default_pip_requirements(include_cloudpickle)             inferred_reqs = mlflow.models.infer_pip_requirements(                 path, FLAVOR_NAME, fallback=default_reqs             )             default_reqs = sorted(set(inferred_reqs).union(default_reqs))         else:             default_reqs = None         conda_env, pip_requirements, pip_constraints = _process_pip_requirements(             default_reqs, pip_requirements, extra_pip_requirements         )     else:         conda_env, pip_requirements, pip_constraints = _process_conda_env(conda_env)      with open(os.path.join(path, _CONDA_ENV_FILE_NAME), ""w"") as f:         yaml.safe_dump(conda_env, stream=f, default_flow_style=False)      if pip_constraints:         write_to(os.path.join(path, _CONSTRAINTS_FILE_NAME), ""\n"".join(pip_constraints))      write_to(os.path.join(path, _REQUIREMENTS_FILE_NAME), ""\n"".join(pip_requirements))      _PythonEnv.current().to_yaml(os.path.join(path, _PYTHON_ENV_FILE_NAME))   def _save_model(model, path, serialization_format):     with open(path, ""wb"") as out:         if serialization_format == SERIALIZATION_FORMAT_PICKLE:             pickle.dump(model, out)         else:             import cloudpickle              cloudpickle.dump(model, out)   The save_model() function also writes the model dependencies to a requirements.txt and conda.yaml file in the model output directory. For this purpose the set of pip dependecies produced by this flavor need to be added to the get_default_pip_requirements() function. In this example only the minimum required dependencies are provided. In practice, additional requirements needed for preprocessing or post-processing steps could be included. Note that for any custom flavor, the mlflow.models.infer_pip_requirements() method in the save_model() function will return the default requirements defined in get_default_pip_requirements() as package imports are only inferred for built-in flavors. def get_default_pip_requirements(include_cloudpickle=False):     pip_deps = [_get_pinned_requirement(""sktime"")]     if include_cloudpickle:         pip_deps += [_get_pinned_requirement(""cloudpickle"")]      return pip_deps   def get_default_conda_env(include_cloudpickle=False):     return _mlflow_conda_env(         additional_pip_deps=get_default_pip_requirements(include_cloudpickle)     )   Next, we add the log_model() function. This function is little more than a wrapper around the mlflow.models.Model.log() method to enable logging our custom model as an artifact to the curren MLflow run. Any flavor-specific parameters (e.g. serialization_format) introduced in the save_model() function also need to be added in the log_model() function. We also need to pass the flavor module to the mlflow.models.Model.log() method which internally calls the save_model() function from above to persist the model. def log_model(     sktime_model,     artifact_path,     conda_env=None,     code_paths=None,     registered_model_name=None,     signature=None,     input_example=None,     await_registration_for=DEFAULT_AWAIT_MAX_SLEEP_SECONDS,     pip_requirements=None,     extra_pip_requirements=None,     serialization_format=SERIALIZATION_FORMAT_PICKLE,     **kwargs, ):     return Model.log(         artifact_path=artifact_path,         flavor=flavor,         registered_model_name=registered_model_name,         sktime_model=sktime_model,         conda_env=conda_env,         code_paths=code_paths,         signature=signature,         input_example=input_example,         await_registration_for=await_registration_for,         pip_requirements=pip_requirements,         extra_pip_requirements=extra_pip_requirements,         serialization_format=serialization_format,         **kwargs,     )   To interpret model directories produced by save_model(), the custom flavor must also define a load_model() function. The load_model() function reads the MLmodel configuration from the specified model directory and uses the configuration attributes to load and return the sktime model from its serialized representation. def load_model(model_uri, dst_path=None):     local_model_path = _download_artifact_from_uri(         artifact_uri=model_uri, output_path=dst_path     )     flavor_conf = _get_flavor_configuration(         model_path=local_model_path, flavor_name=FLAVOR_NAME     )     _add_code_from_conf_to_system_path(local_model_path, flavor_conf)     sktime_model_file_path = os.path.join(         local_model_path, flavor_conf[""pickled_model""]     )     serialization_format = flavor_conf.get(         ""serialization_format"", SERIALIZATION_FORMAT_PICKLE     )     return _load_model(         path=sktime_model_file_path, serialization_format=serialization_format     )   def _load_model(path, serialization_format):     with open(path, ""rb"") as pickled_model:         if serialization_format == SERIALIZATION_FORMAT_PICKLE:             return pickle.load(pickled_model)         elif serialization_format == SERIALIZATION_FORMAT_CLOUDPICKLE:             import cloudpickle              return cloudpickle.load(pickled_model)   The _load_pyfunc() function will be called by the mlflow.pyfunc.load_model() method to load the custom model flavor as a pyfunc type. The MLmodel flavor configuration is used to pass any flavor-specific attributes to the _load_model() function (i.e., the path to the python_function flavor in the model directory and the model serialization format). def _load_pyfunc(path):     try:         sktime_flavor_conf = _get_flavor_configuration(             model_path=path, flavor_name=FLAVOR_NAME         )         serialization_format = sktime_flavor_conf.get(             ""serialization_format"", SERIALIZATION_FORMAT_PICKLE         )     except MlflowException:         _logger.warning(             ""Could not find sktime flavor configuration during model ""             ""loading process. Assuming 'pickle' serialization format.""         )         serialization_format = SERIALIZATION_FORMAT_PICKLE      pyfunc_flavor_conf = _get_flavor_configuration(         model_path=path, flavor_name=pyfunc.FLAVOR_NAME     )     path = os.path.join(path, pyfunc_flavor_conf[""model_path""])      return _SktimeModelWrapper(         _load_model(path, serialization_format=serialization_format)     )   The final step is to create the model wrapper class defining the python_function flavor. The design of the wrapper class determines how the flavor’s inference API is exposed when making predictions using the python_function flavor. Just like the built-in flavors, the predict() method of the sktime wrapper class accepts a single-row Pandas DataFrame configuration argument. For an example of how to construct this configuration DataFrame refer to the usage example in the next section. A detailed description of the supported paramaters and input formats is provided in the flavor.py module docstrings. class _SktimeModelWrapper:     def __init__(self, sktime_model):         self.sktime_model = sktime_model      def predict(self, dataframe, params=None) -> pd.DataFrame:         df_schema = dataframe.columns.values.tolist()          if len(dataframe) > 1:             raise MlflowException(                 f""The provided prediction pd.DataFrame contains {len(dataframe)} rows. ""                 ""Only 1 row should be supplied."",                 error_code=INVALID_PARAMETER_VALUE,             )          # Convert the configuration dataframe into a dictionary to simplify the         # extraction of parameters passed to the sktime predcition methods.         attrs = dataframe.to_dict(orient=""index"").get(0)         predict_method = attrs.get(""predict_method"")          if not predict_method:             raise MlflowException(                 f""The provided prediction configuration pd.DataFrame columns ({df_schema}) do not ""                 ""contain the required column `predict_method` for specifying the prediction method."",                 error_code=INVALID_PARAMETER_VALUE,             )          if predict_method not in SUPPORTED_SKTIME_PREDICT_METHODS:             raise MlflowException(                 ""Invalid `predict_method` value.""                 f""The supported prediction methods are {SUPPORTED_SKTIME_PREDICT_METHODS}"",                 error_code=INVALID_PARAMETER_VALUE,             )          # For inference parameters 'fh', 'X', 'coverage', 'alpha', and 'cov'         # the respective sktime default value is used if the value was not         # provided in the configuration dataframe.         fh = attrs.get(""fh"", None)          # Any model that is trained with exogenous regressor elements will need         # to provide `X` entries as a numpy ndarray to the predict method.         X = attrs.get(""X"", None)          # When the model is served via REST API the exogenous regressor must be         # provided as a list to the configuration DataFrame to be JSON serializable.         # Below we convert the list back to ndarray type as required by sktime         # predict methods.         if isinstance(X, list):             X = np.array(X)          # For illustration purposes only a subset of the available sktime prediction         # methods is exposed. Additional methods (e.g. predict_proba) could be added         # in a similar fashion.         if predict_method == SKTIME_PREDICT:             predictions = self.sktime_model.predict(fh=fh, X=X)          if predict_method == SKTIME_PREDICT_INTERVAL:             coverage = attrs.get(""coverage"", 0.9)             predictions = self.sktime_model.predict_interval(                 fh=fh, X=X, coverage=coverage             )          if predict_method == SKTIME_PREDICT_QUANTILES:             alpha = attrs.get(""alpha"", None)             predictions = self.sktime_model.predict_quantiles(fh=fh, X=X, alpha=alpha)          if predict_method == SKTIME_PREDICT_VAR:             cov = attrs.get(""cov"", False)             predictions = self.sktime_model.predict_var(fh=fh, X=X, cov=cov)          # Methods predict_interval() and predict_quantiles() return a pandas         # MultiIndex column structure. As MLflow signature inference does not         # support MultiIndex column structure the columns must be flattened.         if predict_method in [SKTIME_PREDICT_INTERVAL, SKTIME_PREDICT_QUANTILES]:             predictions.columns = flatten_multiindex(predictions)          return predictions     Example: Using the custom “sktime” flavor  This example trains a sktime NaiveForecaster model using the Longley dataset for forecasting with exogenous variables. It shows a custom model type implementation that logs the training hyper-parameters, evaluation metrics and the trained model as an artifact. The single-row configuration DataFrame for this example defines an interval forecast with nominal coverage values [0.9,0.95], a future forecast horizon of four periods, and an exogenous regressor. import json  import flavor import pandas as pd from sktime.datasets import load_longley from sktime.forecasting.model_selection import temporal_train_test_split from sktime.forecasting.naive import NaiveForecaster from sktime.performance_metrics.forecasting import (     mean_absolute_error,     mean_absolute_percentage_error, )  import mlflow  ARTIFACT_PATH = ""model""  with mlflow.start_run() as run:     y, X = load_longley()     y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)      forecaster = NaiveForecaster()     forecaster.fit(         y_train,         X=X_train,     )      # Extract parameters     parameters = forecaster.get_params()      # Evaluate model     y_pred = forecaster.predict(fh=[1, 2, 3, 4], X=X_test)     metrics = {         ""mae"": mean_absolute_error(y_test, y_pred),         ""mape"": mean_absolute_percentage_error(y_test, y_pred),     }      print(f""Parameters: \n{json.dumps(parameters, indent=2)}"")     print(f""Metrics: \n{json.dumps(metrics, indent=2)}"")      # Log parameters and metrics     mlflow.log_params(parameters)     mlflow.log_metrics(metrics)      # Log model using custom model flavor with pickle serialization (default).     flavor.log_model(         sktime_model=forecaster,         artifact_path=ARTIFACT_PATH,         serialization_format=""pickle"",     )     model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)  # Load model in native sktime flavor and pyfunc flavor loaded_model = flavor.load_model(model_uri=model_uri) loaded_pyfunc = flavor.pyfunc.load_model(model_uri=model_uri)  # Convert test data to 2D numpy array so it can be passed to pyfunc predict using # a single-row Pandas DataFrame configuration argument X_test_array = X_test.to_numpy()  # Create configuration DataFrame predict_conf = pd.DataFrame(     [         {             ""fh"": [1, 2, 3, 4],             ""predict_method"": ""predict_interval"",             ""coverage"": [0.9, 0.95],             ""X"": X_test_array,         }     ] )  # Generate interval forecasts with native sktime flavor and pyfunc flavor print(     f""\nNative sktime 'predict_interval':\n${loaded_model.predict_interval(fh=[1, 2, 3], X=X_test, coverage=[0.9, 0.95])}"" ) print(f""\nPyfunc 'predict_interval':\n${loaded_pyfunc.predict(predict_conf)}"")  # Print the run id wich is used for serving the model to a local REST API endpoint print(f""\nMLflow run id:\n{run.info.run_id}"")   When opening the MLflow runs detail page the serialized model artifact  will show up, such as:      To serve the model to a local REST API endpoint run the following MLflow CLI command substituting the run id printed during execution of the previous block (for more details refer to the Deploy MLflow models section): mlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1   An example of requesting a prediction from the served model is shown below. The exogenous regressor needs to be provided as a list to be JSON serializable. The wrapper instance will convert the list back to numpy ndarray type as required by sktime inference API. import pandas as pd import requests  from sktime.datasets import load_longley from sktime.forecasting.model_selection import temporal_train_test_split  y, X = load_longley() y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)  # Define local host and endpoint url host = ""127.0.0.1"" url = f""http://{host}:5000/invocations""  # Create configuration DataFrame X_test_list = X_test.to_numpy().tolist() predict_conf = pd.DataFrame(     [         {             ""fh"": [1, 2, 3, 4],             ""predict_method"": ""predict_interval"",             ""coverage"": [0.9, 0.95],             ""X"": X_test_list,         }     ] )  # Create dictionary with pandas DataFrame in the split orientation json_data = {""dataframe_split"": predict_conf.to_dict(orient=""split"")}  # Score model response = requests.post(url, json=json_data) print(f""\nPyfunc 'predict_interval':\n${response.json()}"")       Built-In Deployment Tools  This information is moved to MLflow Deployment page.   Export a python_function model as an Apache Spark UDF  You can output a python_function model as an Apache Spark UDF, which can be uploaded to a Spark cluster and used to score the model. Example from pyspark.sql.functions import struct from pyspark.sql import SparkSession  spark = SparkSession.builder.getOrCreate() pyfunc_udf = mlflow.pyfunc.spark_udf(spark, ""<path-to-model>"") df = spark_df.withColumn(""prediction"", pyfunc_udf(struct([...])))   If a model contains a signature, the UDF can be called without specifying column name arguments. In this case, the UDF will be called with column names from signature, so the evaluation dataframe’s column names must match the model signature’s column names. Example from pyspark.sql import SparkSession  spark = SparkSession.builder.getOrCreate() pyfunc_udf = mlflow.pyfunc.spark_udf(spark, ""<path-to-model-with-signature>"") df = spark_df.withColumn(""prediction"", pyfunc_udf())   If a model contains a signature with tensor spec inputs, you will need to pass a column of array type as a corresponding UDF argument. The values in this column must be comprised of one-dimensional arrays. The UDF will reshape the array values to the required shape with ‘C’ order (i.e. read / write the elements using C-like index order) and cast the values as the required tensor spec type. For example, assuming a model requires input ‘a’ of shape (-1, 2, 3) and input ‘b’ of shape (-1, 4, 5). In order to perform inference on this data, we need to prepare a Spark DataFrame with column ‘a’ containing arrays of length 6 and column ‘b’ containing arrays of length 20. We can then invoke the UDF like following example code: Example from pyspark.sql import SparkSession  spark = SparkSession.builder.getOrCreate() # Assuming the model requires input 'a' of shape (-1, 2, 3) and input 'b' of shape (-1, 4, 5) model_path = ""<path-to-model-requiring-multidimensional-inputs>"" pyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_path) # The `spark_df` has column 'a' containing arrays of length 6 and # column 'b' containing arrays of length 20 df = spark_df.withColumn(""prediction"", pyfunc_udf(struct(""a"", ""b"")))   The resulting UDF is based on Spark’s Pandas UDF and is currently limited to producing either a single value, an array of values, or a struct containing multiple field values of the same type per observation. By default, we return the first numeric column as a double. You can control what result is returned by supplying result_type argument. The following values are supported:  'int' or IntegerType: The leftmost integer that can fit in int32 result is returned or an exception is raised if there are none. 'long' or LongType: The leftmost long integer that can fit in int64 result is returned or an exception is raised if there are none. ArrayType (IntegerType | LongType): Return all integer columns that can fit into the requested size. 'float' or FloatType: The leftmost numeric result cast to float32 is returned or an exception is raised if there are no numeric columns. 'double' or DoubleType: The leftmost numeric result cast to double is returned or an exception is raised if there are no numeric columns. ArrayType ( FloatType | DoubleType ): Return all numeric columns cast to the requested type. An exception is raised if there are no numeric columns. 'string' or StringType: Result is the leftmost column cast as string. ArrayType ( StringType ): Return all columns cast as string. 'bool' or 'boolean' or BooleanType: The leftmost column cast to bool is returned or an exception is raised if the values cannot be coerced. 'field1 FIELD1_TYPE, field2 FIELD2_TYPE, ...': A struct type containing multiple fields separated by comma, each field type must be one of types listed above.  Example from pyspark.sql import SparkSession  spark = SparkSession.builder.getOrCreate() # Suppose the PyFunc model `predict` method returns a dict like: # `{'prediction': 1-dim_array, 'probability': 2-dim_array}` # You can supply result_type to be a struct type containing # 2 fields 'prediction' and 'probability' like following. pyfunc_udf = mlflow.pyfunc.spark_udf(     spark, ""<path-to-model>"", result_type=""prediction float, probability: array<float>"" ) df = spark_df.withColumn(""prediction"", pyfunc_udf())   Example from pyspark.sql.types import ArrayType, FloatType from pyspark.sql.functions import struct from pyspark.sql import SparkSession  spark = SparkSession.builder.getOrCreate() pyfunc_udf = mlflow.pyfunc.spark_udf(     spark, ""path/to/model"", result_type=ArrayType(FloatType()) ) # The prediction column will contain all the numeric columns returned by the model as floats df = spark_df.withColumn(""prediction"", pyfunc_udf(struct(""name"", ""age"")))   If you want to use conda to restore the python environment that was used to train the model, set the env_manager argument when calling mlflow.pyfunc.spark_udf(). Example from pyspark.sql.types import ArrayType, FloatType from pyspark.sql.functions import struct from pyspark.sql import SparkSession  spark = SparkSession.builder.getOrCreate() pyfunc_udf = mlflow.pyfunc.spark_udf(     spark,     ""path/to/model"",     result_type=ArrayType(FloatType()),     env_manager=""conda"",  # Use conda to restore the environment used in training ) df = spark_df.withColumn(""prediction"", pyfunc_udf(struct(""name"", ""age"")))     Deployment to Custom Targets  In addition to the built-in deployment tools, MLflow provides a pluggable mlflow.deployments Python API and mlflow deployments CLI for deploying models to custom targets and environments. To deploy to a custom target, you must first install an appropriate third-party Python plugin. See the list of known community-maintained plugins here.  Commands  The mlflow deployments CLI contains the following commands, which can also be invoked programmatically using the mlflow.deployments Python API:  Create: Deploy an MLflow model to a specified custom target Delete: Delete a deployment Update: Update an existing deployment, for example to deploy a new model version or change the deployment’s configuration (e.g. increase replica count) List: List IDs of all deployments Get: Print a detailed description of a particular deployment Run Local: Deploy the model locally for testing Help: Show the help string for the specified target  For more info, see: mlflow deployments --help mlflow deployments create --help mlflow deployments delete --help mlflow deployments update --help mlflow deployments list --help mlflow deployments get --help mlflow deployments run-local --help mlflow deployments help --help      Community Model Flavors  Go to the Community Model Flavors page to get an overview of other useful MLflow flavors, which are developed and maintained by the MLflow community.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
model-registry.html,"   Documentation  MLflow Model Registry       MLflow Model Registry  The MLflow Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow experiment and run produced the model), model versioning, model aliasing, model tagging, and annotations.  Table of Contents  Concepts Model Registry Workflows  UI Workflow  Register a Model Find Registered Models Deploy and Organize Models   API Workflow  Adding an MLflow Model to the Model Registry Deploy and Organize Models with Aliases and Tags Fetching an MLflow Model from the Model Registry Serving an MLflow Model from Model Registry Promoting an MLflow Model across environments Adding or Updating an MLflow Model Descriptions Renaming an MLflow Model Listing and Searching MLflow Models Deleting MLflow Models Registering a Model Saved Outside MLflow Registering an Unsupported Machine Learning Model Deprecated: Using Model Stages     Migrating from Stages  New model deployment tools Migrating models away from stages      Concepts  The Model Registry introduces a few concepts that describe and facilitate the full lifecycle of an MLflow Model.  ModelAn MLflow Model is created from an experiment or run that is logged with one of the model flavor’s mlflow.<model_flavor>.log_model() methods. Once logged, this model can then be registered with the Model Registry.  Registered ModelAn MLflow Model can be registered with the Model Registry. A registered model has a unique name, contains versions, aliases, tags, and other metadata.  Model VersionEach registered model can have one or many versions. When a new model is added to the Model Registry, it is added as version 1. Each new model registered to the same model name increments the version number. Model versions have tags, which can be useful for tracking attributes of the model version (e.g. pre_deploy_checks: “PASSED”)    Model AliasModel aliases allow you to assign a mutable, named reference to a particular version of a registered model. By assigning an alias to a specific model version, you can use the alias to refer that model version via a model URI or the model registry API. For example, you can create an alias named champion that points to version 1 of a model named MyModel. You can then refer to version 1 of MyModel by using the URI models:/MyModel@champion. Aliases are especially useful for deploying models. For example, you could assign a champion alias to the model version intended for production traffic and target this alias in production workloads. You can then update the model serving production traffic by reassigning the champion alias to a different model version.  TagsTags are key-value pairs that you associate with registered models and model versions, allowing you to label and categorize them by function or status. For example, you could apply a tag with key ""task"" and value ""question-answering"" (displayed in the UI as task:question-answering) to registered models intended for question answering tasks. At the model version level, you could tag versions undergoing pre-deployment validation with validation_status:pending and those cleared for deployment with validation_status:approved.  Annotations and DescriptionsYou can annotate the top-level model and each version individually using Markdown, including description and any relevant information useful for the team such as algorithm descriptions, dataset employed or methodology.     Model Registry Workflows  If running your own MLflow server, you must use a database-backed backend store in order to access the model registry via the UI or API. See here for more information. Before you can add a model to the Model Registry, you must log it using the log_model methods of the corresponding model flavors. Once a model has been logged, you can add, modify, update, or delete the model in the Model Registry through the UI or the API.  UI Workflow  This section demonstrates how to use the MLflow Model Registry UI to manage your MLflow models.  Register a Model  Follow the steps below to register your MLflow model in the Model Registry.  Open the details page for the MLflow Run containing the logged MLflow model you’d like to register. Select the model folder containing the intended MLflow model in the Artifacts section.        Click the Register Model button, which will trigger a form to pop up. In the Model dropdown menu on the form, you can either select “Create New Model”, which creates a new registered model with your MLflow model as its initial version, or select an existing registered model, which registers your model under it as a new version. The screenshot below demonstrates registering the MLflow model to a new registered model named ""iris_model_testing"".         Find Registered Models  After you’ve registered your models in the Model Registry, you can navigate to them in the following ways.  Navigate to the Registered Models page, which links to your registered models and correponding model versions.     Go to the Artifacts section of your MLflow Runs details page, click the model folder, and then click the model version at the top right to view the version created from that model.        Deploy and Organize Models  You can deploy and organize your models in the Model Registry using model aliases and tags. To set aliases and tags for model versions in your registered model, navigate to the overview page of your registered model, such as the one below.    You can add or edit aliases and tags for a specific model version by clicking on the corresponding Add link or pencil icon in the model verison table.    To learn more about a specific model version, navigate to the details page for that model version.    In this page, you can inspect model version details like the model signature, MLflow source run, and creation timestamp. You can also view and configure the verion’s aliases, tags, and description.    API Workflow  An alternative way to interact with Model Registry is using the MLflow model flavor or MLflow Client Tracking API interface. In particular, you can register a model during an MLflow experiment run or after all your experiment runs.  Adding an MLflow Model to the Model Registry  There are three programmatic ways to add a model to the registry. First, you can use the mlflow.<model_flavor>.log_model() method. For example, in your code: from sklearn.datasets import make_regression from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split  import mlflow import mlflow.sklearn from mlflow.models import infer_signature  with mlflow.start_run() as run:     X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)     X_train, X_test, y_train, y_test = train_test_split(         X, y, test_size=0.2, random_state=42     )     params = {""max_depth"": 2, ""random_state"": 42}     model = RandomForestRegressor(**params)     model.fit(X_train, y_train)      # Infer the model signature     y_pred = model.predict(X_test)     signature = infer_signature(X_test, y_pred)      # Log parameters and metrics using the MLflow APIs     mlflow.log_params(params)     mlflow.log_metrics({""mse"": mean_squared_error(y_test, y_pred)})      # Log the sklearn model and register as version 1     mlflow.sklearn.log_model(         sk_model=model,         artifact_path=""sklearn-model"",         signature=signature,         registered_model_name=""sk-learn-random-forest-reg-model"",     )   In the above code snippet, if a registered model with the name doesn’t exist, the method registers a new model and creates Version 1. If a registered model with the name exists, the method creates a new model version. The second way is to use the mlflow.register_model() method, after all your experiment runs complete and when you have decided which model is most suitable to add to the registry. For this method, you will need the run_id as part of the runs:URI argument. result = mlflow.register_model(     ""runs:/d16076a3ec534311817565e6527539c0/sklearn-model"", ""sk-learn-random-forest-reg"" )   If a registered model with the name doesn’t exist, the method registers a new model, creates Version 1, and returns a ModelVersion MLflow object. If a registered model with the name exists, the method creates a new model version and returns the version object. And finally, you can use the create_registered_model() to create a new registered model. If the model name exists, this method will throw an MlflowException because creating a new registered model requires a unique name. from mlflow import MlflowClient  client = MlflowClient() client.create_registered_model(""sk-learn-random-forest-reg-model"")   The method above creates an empty registered model with no version associated. You can use create_model_version() as shown below to create a new version of the model. client = MlflowClient() result = client.create_model_version(     name=""sk-learn-random-forest-reg-model"",     source=""mlruns/0/d16076a3ec534311817565e6527539c0/artifacts/sklearn-model"",     run_id=""d16076a3ec534311817565e6527539c0"", )     Deploy and Organize Models with Aliases and Tags  Model aliases and tags help you deploy and organize your models in the Model Registry. Set and delete aliases on models To set, update, and delete aliases using the MLflow Client API, see the examples below: from mlflow import MlflowClient  client = MlflowClient()  # create ""champion"" alias for version 1 of model ""example-model"" client.set_registered_model_alias(""example-model"", ""champion"", 1)  # reassign the ""Champion"" alias to version 2 client.set_registered_model_alias(""example-model"", ""Champion"", 2)  # get a model version by alias client.get_model_version_by_alias(""example-model"", ""Champion"")  # delete the alias client.delete_registered_model_alias(""example-model"", ""Champion"")   Set and delete tags on models To set and delete tags using the MLflow Client API, see the examples below: from mlflow import MlflowClient  client = MlflowClient()  # Set registered model tag client.set_registered_model_tag(""example-model"", ""task"", ""classification"")  # Delete registered model tag client.delete_registered_model_tag(""example-model"", ""task"")  # Set model version tag client.set_model_version_tag(""example-model"", ""1"", ""validation_status"", ""approved"")  # Delete model version tag client.delete_model_version_tag(""example-model"", ""1"", ""validation_status"")   For more details on alias and tag client APIs, see the mlflow.client API documentation.   Fetching an MLflow Model from the Model Registry  After you have registered an MLflow model, you can fetch that model using mlflow.<model_flavor>.load_model(), or more generally, load_model(). You can use the loaded model for one off predictions or in inference workloads such as batch inference. Fetch a specific model version To fetch a specific model version, just supply that version number as part of the model URI. import mlflow.pyfunc  model_name = ""sk-learn-random-forest-reg-model"" model_version = 1  model = mlflow.pyfunc.load_model(model_uri=f""models:/{model_name}/{model_version}"")  model.predict(data)   Fetch a model version by alias To fetch a model version by alias, specify the model alias in the model URI, and it will fetch the model version currently under it. import mlflow.pyfunc  model_name = ""sk-learn-random-forest-reg-model"" alias = ""champion""  champion_version = mlflow.pyfunc.load_model(f""models:/{model_name}@{alias}"")  champion_version.predict(data)   Note that model alias assignments can be updated independently of your production code. If the champion alias in the snippet above is reassigned to a new model version in the Model Registry, the next execution of this snippet will automatically pick up the new model version. This allows you to decouple model deployments from your inference workloads.   Serving an MLflow Model from Model Registry  After you have registered an MLflow model, you can serve the model as a service on your host. #!/usr/bin/env sh  # Set environment variable for the tracking URL where the Model Registry resides export MLFLOW_TRACKING_URI=http://localhost:5000  # Serve the production model from the model registry mlflow models serve -m ""models:/sk-learn-random-forest-reg-model@champion""     Promoting an MLflow Model across environments  In mature DevOps and MLOps workflows, organizations use separate environments (typically, dev, staging, and prod) with access controls to enable quick development without compromising stability in production. In MLflow, you can use registered models and MLflow Authentication to express access-controlled environments for your MLflow models. For example, you can create registered models corresponding to each combination of environment and business problem (e.g. prod.ml_team.revenue_forecasting, dev.ml_team.revenue_forecasting) and configure permissions accordingly. As you iterate on MLflow models for your business problem, you can promote them through the various environments for continuous integration and deployment. For mature production-grade setups, we recommend setting up automated workflows that train and register models in each environment. To productionize the latest iteration on a business problem, promote your machine learning code across environments via source control and CI/CD systems. For simple model deployment use cases, you can register your trained MLflow Model to a dev environment registered model as the latest model version and then use copy_model_version() to promote it across registered models. from mlflow import MlflowClient  client = MlflowClient() client.copy_model_version(     src_model_uri=""models:/regression-model-staging@candidate"",     dst_name=""regression-model-production"", )   This code snippet copies the model version with the candidate alias in the regression-model-staging model to the regression-model-production model as the latest version. You can also promote model versions in the UI. To do this, navigate to the model version details page and select the Promote model button. This opens a modal where you can choose a registered model to which the current model version will be copied.      Adding or Updating an MLflow Model Descriptions  At any point in a model’s lifecycle development, you can update a model version’s description using update_model_version(). client = MlflowClient() client.update_model_version(     name=""sk-learn-random-forest-reg-model"",     version=1,     description=""This model version is a scikit-learn random forest containing 100 decision trees"", )     Renaming an MLflow Model  As well as adding or updating a description of a specific version of the model, you can rename an existing registered model using rename_registered_model(). client = MlflowClient() client.rename_registered_model(     name=""sk-learn-random-forest-reg-model"",     new_name=""sk-learn-random-forest-reg-model-100"", )     Listing and Searching MLflow Models  You can fetch a list of registered models in the registry with a simple method. from pprint import pprint  client = MlflowClient() for rm in client.search_registered_models():     pprint(dict(rm), indent=4)   This outputs: {   'creation_timestamp': 1582671933216,     'description': None,     'last_updated_timestamp': 1582671960712,     'latest_versions': [<ModelVersion: creation_timestamp=1582671933246, current_stage='Production', description='A random forest model containing 100 decision trees trained in scikit-learn', last_updated_timestamp=1582671960712, name='sk-learn-random-forest-reg-model', run_id='ae2cc01346de45f79a44a320aab1797b', source='./mlruns/0/ae2cc01346de45f79a44a320aab1797b/artifacts/sklearn-model', status='READY', status_message=None, user_id=None, version=1>,                         <ModelVersion: creation_timestamp=1582671960628, current_stage='None', description=None, last_updated_timestamp=1582671960628, name='sk-learn-random-forest-reg-model', run_id='d994f18d09c64c148e62a785052e6723', source='./mlruns/0/d994f18d09c64c148e62a785052e6723/artifacts/sklearn-model', status='READY', status_message=None, user_id=None, version=2>],     'name': 'sk-learn-random-forest-reg-model'}   With hundreds of models, it can be cumbersome to peruse the results returned from this call. A more efficient approach would be to search for a specific model name and list its version details using search_model_versions() method and provide a filter string such as ""name='sk-learn-random-forest-reg-model'"" client = MlflowClient() for mv in client.search_model_versions(""name='sk-learn-random-forest-reg-model'""):     pprint(dict(mv), indent=4)   This outputs: {     ""creation_timestamp"": 1582671933246,     ""current_stage"": ""Production"",     ""description"": ""A random forest model containing 100 decision trees ""     ""trained in scikit-learn"",     ""last_updated_timestamp"": 1582671960712,     ""name"": ""sk-learn-random-forest-reg-model"",     ""run_id"": ""ae2cc01346de45f79a44a320aab1797b"",     ""source"": ""./mlruns/0/ae2cc01346de45f79a44a320aab1797b/artifacts/sklearn-model"",     ""status"": ""READY"",     ""status_message"": None,     ""user_id"": None,     ""version"": 1, }  {     ""creation_timestamp"": 1582671960628,     ""current_stage"": ""None"",     ""description"": None,     ""last_updated_timestamp"": 1582671960628,     ""name"": ""sk-learn-random-forest-reg-model"",     ""run_id"": ""d994f18d09c64c148e62a785052e6723"",     ""source"": ""./mlruns/0/d994f18d09c64c148e62a785052e6723/artifacts/sklearn-model"",     ""status"": ""READY"",     ""status_message"": None,     ""user_id"": None,     ""version"": 2, }     Deleting MLflow Models   Note Deleting registered models or model versions is irrevocable, so use it judiciously.  You can either delete specific versions of a registered model or you can delete a registered model and all its versions. # Delete versions 1,2, and 3 of the model client = MlflowClient() versions = [1, 2, 3] for version in versions:     client.delete_model_version(         name=""sk-learn-random-forest-reg-model"", version=version     )  # Delete a registered model along with all its versions client.delete_registered_model(name=""sk-learn-random-forest-reg-model"")   While the above workflow API demonstrates interactions with the Model Registry, two exceptional cases require attention. One is when you have existing ML models saved from training without the use of MLflow. Serialized and persisted on disk in sklearn’s pickled format, you want to register this model with the Model Registry. The second is when you use an ML framework without a built-in MLflow model flavor support, for instance, vaderSentiment, and want to register the model.   Registering a Model Saved Outside MLflow  Not everyone will start their model training with MLflow. So you may have some models trained before the use of MLflow. Instead of retraining the models, all you want to do is register your saved models with the Model Registry. This code snippet creates a sklearn model, which we assume that you had created and saved in native pickle format.  Note The sklearn library and pickle versions with which the model was saved should be compatible with the current MLflow supported built-in sklearn model flavor.  import numpy as np import pickle  from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score  # source: https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html  # Load the diabetes dataset diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)  # Use only one feature diabetes_X = diabetes_X[:, np.newaxis, 2]  # Split the data into training/testing sets diabetes_X_train = diabetes_X[:-20] diabetes_X_test = diabetes_X[-20:]  # Split the targets into training/testing sets diabetes_y_train = diabetes_y[:-20] diabetes_y_test = diabetes_y[-20:]   def print_predictions(m, y_pred):     # The coefficients     print(""Coefficients: \n"", m.coef_)     # The mean squared error     print(""Mean squared error: %.2f"" % mean_squared_error(diabetes_y_test, y_pred))     # The coefficient of determination: 1 is perfect prediction     print(""Coefficient of determination: %.2f"" % r2_score(diabetes_y_test, y_pred))   # Create linear regression object lr_model = linear_model.LinearRegression()  # Train the model using the training sets lr_model.fit(diabetes_X_train, diabetes_y_train)  # Make predictions using the testing set diabetes_y_pred = lr_model.predict(diabetes_X_test) print_predictions(lr_model, diabetes_y_pred)  # save the model in the native sklearn format filename = ""lr_model.pkl"" pickle.dump(lr_model, open(filename, ""wb""))   Coefficients: [938.23786125] Mean squared error: 2548.07 Coefficient of determination: 0.47   Once saved in pickled format, you can load the sklearn model into memory using pickle API and register the loaded model with the Model Registry. import mlflow from mlflow.models import infer_signature import numpy as np from sklearn import datasets  # load the model into memory loaded_model = pickle.load(open(filename, ""rb""))  # create a signature for the model based on the input and output data diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True) diabetes_X = diabetes_X[:, np.newaxis, 2] signature = infer_signature(diabetes_X, diabetes_y)  # log and register the model using MLflow scikit-learn API mlflow.set_tracking_uri(""sqlite:///mlruns.db"") reg_model_name = ""SklearnLinearRegression"" print(""--"") mlflow.sklearn.log_model(     loaded_model,     ""sk_learn"",     serialization_format=""cloudpickle"",     signature=signature,     registered_model_name=reg_model_name, )   -- Successfully registered model 'SklearnLinearRegression'. 2021/04/02 16:30:57 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: SklearnLinearRegression, version 1 Created version '1' of model 'SklearnLinearRegression'.   Now, using MLflow fluent APIs, you reload the model from the Model Registry and score. # load the model from the Model Registry and score model_uri = f""models:/{reg_model_name}/1"" loaded_model = mlflow.sklearn.load_model(model_uri) print(""--"")  # Make predictions using the testing set diabetes_y_pred = loaded_model.predict(diabetes_X_test) print_predictions(loaded_model, diabetes_y_pred)   -- Coefficients: [938.23786125] Mean squared error: 2548.07 Coefficient of determination: 0.47     Registering an Unsupported Machine Learning Model  In some cases, you might use a machine learning framework without its built-in MLflow Model flavor support. For instance, the vaderSentiment library is a standard Natural Language Processing (NLP) library used for sentiment analysis. Since it lacks a built-in MLflow Model flavor, you cannot log or register the model using MLflow Model fluent APIs. To work around this problem, you can create an instance of a mlflow.pyfunc model flavor and embed your NLP model inside it, allowing you to save, log or register the model. Once registered, load the model from the Model Registry and score using the predict function. The code sections below demonstrate how to create a PythonFuncModel class with a vaderSentiment model embedded in it, save, log, register, and load from the Model Registry and score.  Note To use this example, you will need to pip install vaderSentiment.  from sys import version_info import cloudpickle import pandas as pd  import mlflow.pyfunc from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # # Good and readable paper from the authors of this package # http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf #  INPUT_TEXTS = [     {""text"": ""This is a bad movie. You don't want to see it! :-)""},     {""text"": ""Ricky Gervais is smart, witty, and creative!!!!!! :D""},     {""text"": ""LOL, this guy fell off a chair while sleeping and snoring in a meeting""},     {""text"": ""Men shoots himself while trying to steal a dog, OMG""},     {""text"": ""Yay!! Another good phone interview. I nailed it!!""},     {         ""text"": ""This is INSANE! I can't believe it. How could you do such a horrible thing?""     }, ]  PYTHON_VERSION = f""{version_info.major}.{version_info.minor}.{version_info.micro}""   def score_model(model):     # Use inference to predict output from the customized PyFunc model     for i, text in enumerate(INPUT_TEXTS):         text = INPUT_TEXTS[i][""text""]         m_input = pd.DataFrame([text])         scores = loaded_model.predict(m_input)         print(f""<{text}> -- {str(scores[0])}"")   # Define a class and extend from PythonModel class SocialMediaAnalyserModel(mlflow.pyfunc.PythonModel):     def __init__(self):         super().__init__()         # embed your vader model instance         self._analyser = SentimentIntensityAnalyzer()      # preprocess the input with prediction from the vader sentiment model     def _score(self, txt):         prediction_scores = self._analyser.polarity_scores(txt)         return prediction_scores      def predict(self, context, model_input, params=None):         # Apply the preprocess function from the vader model to score         model_output = model_input.apply(lambda col: self._score(col))         return model_output   model_path = ""vader"" reg_model_name = ""PyFuncVaderSentiments"" vader_model = SocialMediaAnalyserModel()  # Set the tracking URI to use local SQLAlchemy db file and start the run # Log MLflow entities and save the model mlflow.set_tracking_uri(""sqlite:///mlruns.db"")  # Save the conda environment for this model. conda_env = {     ""channels"": [""defaults"", ""conda-forge""],     ""dependencies"": [f""python={PYTHON_VERSION}"", ""pip""],     ""pip"": [         ""mlflow"",         f""cloudpickle=={cloudpickle.__version__}"",         ""vaderSentiment==3.3.2"",     ],     ""name"": ""mlflow-env"", }  # Save the model with mlflow.start_run(run_name=""Vader Sentiment Analysis"") as run:     model_path = f""{model_path}-{run.info.run_uuid}""     mlflow.log_param(""algorithm"", ""VADER"")     mlflow.log_param(""total_sentiments"", len(INPUT_TEXTS))     mlflow.pyfunc.save_model(         path=model_path, python_model=vader_model, conda_env=conda_env     )  # Use the saved model path to log and register into the model registry mlflow.pyfunc.log_model(     artifact_path=model_path,     python_model=vader_model,     registered_model_name=reg_model_name,     conda_env=conda_env, )  # Load the model from the model registry and score model_uri = f""models:/{reg_model_name}/1"" loaded_model = mlflow.pyfunc.load_model(model_uri) score_model(loaded_model)   Successfully registered model 'PyFuncVaderSentiments'. 2021/04/05 10:34:15 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Created version '1' of model 'PyFuncVaderSentiments'.  <This is a bad movie. You don't want to see it! :-)> -- {'neg': 0.307, 'neu': 0.552, 'pos': 0.141, 'compound': -0.4047} <Ricky Gervais is smart, witty, and creative!!!!!! :D> -- {'neg': 0.0, 'neu': 0.316, 'pos': 0.684, 'compound': 0.8957} <LOL, this guy fell off a chair while sleeping and snoring in a meeting> -- {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.5473} <Men shoots himself while trying to steal a dog, OMG> -- {'neg': 0.262, 'neu': 0.738, 'pos': 0.0, 'compound': -0.4939} <Yay!! Another good phone interview. I nailed it!!> -- {'neg': 0.0, 'neu': 0.446, 'pos': 0.554, 'compound': 0.816} <This is INSANE! I can't believe it. How could you do such a horrible thing?> -- {'neg': 0.357, 'neu': 0.643, 'pos': 0.0, 'compound': -0.8034}     Deprecated: Using Model Stages   Warning Model Stages are deprecated and will be removed in a future major release. To learn more about this deprecation, see our migration guide below.  See the sections below on using Model Stages in the MLflow Model Registry. Transitioning an MLflow Model’s Stage Over the course of the model’s lifecycle, a model evolves—from development to staging to production. You can transition a registered model to one of the stages: Staging, Production or Archived. client = MlflowClient() client.transition_model_version_stage(     name=""sk-learn-random-forest-reg-model"", version=3, stage=""Production"" )   The accepted values for <stage> are: Staging|Archived|Production|None. Fetch the latest model version in a specific stage To fetch a model version by stage, simply provide the model stage as part of the model URI, and it will fetch the most recent version of the model in that stage. import mlflow.pyfunc  model_name = ""sk-learn-random-forest-reg-model"" stage = ""Staging""  model = mlflow.pyfunc.load_model(model_uri=f""models:/{model_name}/{stage}"")  model.predict(data)   Archiving an MLflow Model You can move models versions out of a Production stage into an Archived stage. At a later point, if that archived model is not needed, you can delete it. # Archive models version 3 from Production into Archived client = MlflowClient() client.transition_model_version_stage(     name=""sk-learn-random-forest-reg-model"", version=3, stage=""Archived"" )       Migrating from Stages  As of MLflow 2.9.0, Model Stages have been deprecated and will be removed in a future major release. This is the culmination of extensive feedback on the inflexibility of model stages for expressing MLOps workflows, from which we developed and introduced of new tools for managing and deploying models in the MLflow Model Registry. Learn more below.  New model deployment tools  Model stages were used to express the lifecycle of MLflow Models for productionization and deployment. Users transitioned model versions through four fixed stages (from none, to staging, to production, and then to archived) as they proposed, validated, deployed, and deprecated models for their ML use-cases. In doing so, model registry stages provided labeling and aliasing functionality for the model versions, by denoting the status of a model version in the UI and providing named references to model versions in the code (e.g. /Staging in the model URI). Model registry stages were also used to denote the environment that the model is in, though it was not possible to set up access controls for them. To replace and improve upon stages, we elevated model version tags in the UI and introduced model version aliases to provide flexible and powerful ways to label and alias MLflow models in the Model Registry. We also made it possible to set up separate environments for your models and configure access controls for each environment. Model version tags Model version tags can be used to annotate model versions with their status. For example, you could apply a tag of key validation_status and value pending to a model version while it is being validated and then update the tag value to passed when it has passed smoke tests and performance tests. Model version aliases Model version aliases provide a flexible way to create named references for particular model versions, and are useful for identifying which model version(s) are deployed within an environment. For example, setting a champion alias on a model version enables you to fetch the model version by that alias via the get_model_version_by_alias() client API or the model URI models:/<registered model name>@champion. Aliases can be reassigned to new model versions via the UI and client API. Unlike model registry stages, more than one alias can be applied to any given model version, allowing for easier A/B testing and model rollout. Set up separate environments for models In mature DevOps and MLOps workflows, organizations use separate environments (typically, dev, staging, and prod) with access controls to enable quick development without compromising stability in production. With MLflow Authentication, you can use registered models to express access-controlled environments for your MLflow models. For example, you can create registered models corresponding to each combination of environment and business problem (e.g. prod.ml_team.revenue_forecasting, dev.ml_team.revenue_forecasting) and configure permissions accordingly. Automate model retraining against your production registered models, or for simple model deployment use cases, use copy_model_version() to promote model versions across registered models.   Migrating models away from stages  See the information below to learn how to use the new tools for your legacy Model Stage use-cases. Model environments To set up separate environments and permissions for your model versions, create separate registered models:  Given a base name for your model’s use-case, e.g. revenue_forecasting, set up various registered models corresponding to your environments with different prefixes. For example, if you want three separate dev, staging, and production environments, you can set up dev.ml_team.revenue_forecasting, staging.ml_team.revenue_forecasting, and prod.ml_team.revenue_forecasting registered models. Use MLflow Authentication to grant appropriate permissions on these models.  Transition models across environments Once you have registered models set up for each environment, you can build your MLOps workflows on top of them.  For simple model promotion use cases, you can first register your MLflow models under the dev registered model and then promote models across environments using the copy_model_version() client API. For more mature production-grade setups, we recommend promoting your ML code (including model training code, inference code, and ML infrastructure as code) across environments. This eliminates the need to transition models across environments. Dev ML code is experimental and in a dev environment, hence targeting the dev registered model. Before merging developed ML code into your source code repository, your CI stages the code in a staging environment for integration testing (targeting the staging registered model). Post-merge, the ML code is deployed to production for automated retraining (targeting the prod registered model). Such setups enable safe and robust CI/CD of ML systems - including not just model training, but also feature engineering, model monitoring, and automated retraining.  Model aliasing To specify (via named references) which model version to deploy to serve traffic within an environment (e.g. production), use model aliases:  Decide on an equivalent model alias for each model registry stage (e.g., champion for the Production stage, to specify the model intended to serve the majority of traffic) Assign the chosen alias to the latest model version under each stage. You can use the helper function below for this. Update ML workflows to target the alias rather than the stage. For example, the model URI models:/regression_model/Production will be replaced by the model URI models:/prod.ml_team.regression_model@champion in the production code.  from mlflow import MlflowClient  # Initialize an MLflow Client client = MlflowClient()   def assign_alias_to_stage(model_name, stage, alias):     """"""     Assign an alias to the latest version of a registered model within a specified stage.      :param model_name: The name of the registered model.     :param stage: The stage of the model version for which the alias is to be assigned. Can be                 ""Production"", ""Staging"", ""Archived"", or ""None"".     :param alias: The alias to assign to the model version.     :return: None     """"""     latest_mv = client.get_latest_versions(model_name, stages=[stage])[0]     client.set_registered_model_alias(model_name, alias, latest_mv.version)   Model status To represent and communicate the status of your model versions, use model version tags:  Set tags on model versions to indicate the status of the model. For example, to indicate the review status of a model version, you can set a tag with key validation_status and value pending or passed.          Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
recipes.html,"   Documentation  MLflow Recipes       MLflow Recipes  MLflow Recipes (previously known as MLflow Pipelines) is a framework that enables data scientists to quickly develop high-quality models and deploy them to production. Compared to ad-hoc ML workflows, MLflow Recipes offers several major benefits:  Get started quickly: Predefined templates for common ML tasks, such as regression modeling, enable data scientists to get started quickly and focus on building great models, eliminating the large amount of boilerplate code that is traditionally required to curate datasets, engineer features, train & tune models, and package models for production deployment. Iterate faster: The intelligent recipe execution engine accelerates model development by caching results from each step of the process and re-running the minimal set of steps as changes are made. Easily ship to production: The modular, git-integrated recipe structure dramatically simplifies the handoff from development to production by ensuring that all model code, data, and configurations are easily reviewable and deployable by ML engineers.   Quickstarts   Prerequisites  MLflow Recipes is available as an extension of the MLflow Python library. You can install MLflow Recipes as follows:  Local: Install MLflow from PyPI: pip install mlflow. Note that MLflow Recipes requires Make, which may not be preinstalled on some Windows systems. Windows users must install Make before using MLflow Recipes. For more information about installing Make on Windows, see https://gnuwin32.sourceforge.net/install.html. Databricks: Install MLflow Recipes from a Databricks Notebook by running %pip install mlflow, or install MLflow Recipes on a Databricks Cluster by following the PyPI library installation instructions here and specifying the mlflow package string.  Note Databricks Runtime version 11.0 or greater is required in order to install MLflow Recipes on Databricks.      NYC taxi fare prediction example  The NYC taxi fare prediction example uses the MLflow Recipes Regression Template to develop and score models on the NYC Taxi (TLC) Trip Record Dataset. You can run the example locally by installing MLflow Recipes and running the Jupyter example regression notebook. You can run the example on Databricks by cloning the example repository with Databricks Repos and running the Databricks example regression notebook. To build and score models for your own use cases, we recommend using the MLflow Recipes Regression Template. For more information, see the Regression Template reference guide.   Classification problem example  The Classification problem example uses the MLflow Recipes Classification Template to develop and score models on the Wine Quality Dataset. You can run the example locally by installing MLflow Recipes and running the Jupyter example classification notebook. You can run the example on Databricks by cloning the example repository with Databricks Repos and running the Databricks example classification notebook. To build and score models for your own use cases, we recommend using the MLflow Recipes Classification Template. For more information, see the Classification Template reference guide.    Key concepts   Steps: A Step represents an individual ML operation, such as ingesting data, fitting an estimator, evaluating a model against test data, or deploying a model for real-time scoring. Each Step accepts a collection of well-defined inputs and produce well-defined outputs according to user-defined configurations and code.   Recipes: A Recipe is an ordered composition of Steps used to solve an ML problem or perform an MLOps task, such as developing a regression model or performing batch model scoring on production data. MLflow Recipes provides APIs and a CLI for running recipes and inspecting their results.   Templates: A Recipe Template is a git repository with a standardized, modular layout containing all of the customizable code and configurations for a Recipe. Configurations are defined in YAML format for easy review via the recipe.yaml file and Profile YAML files. Each template also defines its requirements, data science notebooks, and tests. MLflow Recipes includes predefined templates for a variety of model development and MLOps tasks.   Profiles: Profiles contain user-specific or environment-specific configurations for a Recipe, such as the particular set of hyperparameters being tuned by a data scientist in development or the MLflow Model Registry URI and credentials used to store production-worthy models. Each profile is represented as a YAML file in the Recipe Template (e.g. local.yaml and databricks.yaml).   Step Cards: Step Cards display the results produced by running a Step, including dataset profiles, model performance & explainability plots, overviews of the best model parameters found during tuning, and more. Step Cards and their corresponding dataset and model information are also logged to MLflow Tracking.    Usage   Model development workflow  The general model development workflow for using MLflow Recipes is as follows:  Clone a Recipe Template git repository corresponding to the ML problem that you wish to solve. Follow the template’s README file for template-specific instructions.   [Local] Clone the MLflow Recipes Regression Template into a local directory.  git clone https://github.com/mlflow/recipes-regression-template    [Databricks] Clone the MLflow Recipes Regression Template git repository using Databricks Repos.      Edit required fields marked by FIXME::REQUIRED comments in recipe.yaml and profiles/*.yaml. The recipe is runnable once all required fields are filled with proper values. You may proceed to step 3 if this is the first time going through this step. Otherwise, continue to edit the YAML config files as well as steps/*.py files, filling out areas marked by FIXME::OPTIONAL as you see fit to customize the recipe steps to your ML problem for better model performance.     Run the recipe by selecting a desired profile. Profiles are used to quickly switch environment specific recipe settings, such as ingest data location. When a recipe run completes, you may inspect the run results. MLflow Recipes creates and displays an interactive Step Card with the results of the last executed step. Each Recipe Template also includes a Databricks Notebook and a Jupyter Notebook for running the recipe and inspecting its results.    Example API and CLI workflows for running the MLflow Recipes Regression Template and inspecting results. Note that recipes must be run from within their corresponding git repositories.  import os from mlflow.recipes import Recipe from mlflow.pyfunc import PyFuncModel  os.chdir(""~/recipes-regression-template"") regression_recipe = Recipe(profile=""local"") # Run the full recipe regression_recipe.run() # Inspect the model training results regression_recipe.inspect(step=""train"") # Load the trained model regression_model_recipe: PyFuncModel = regression_recipe.get_artifact(""model"")    git clone https://github.com/mlflow/recipes-regression-template cd recipes-regression-template # Run the full recipe mlflow recipes run --profile local # Inspect the model training results mlflow recipes inspect --step train --profile local # Inspect the resulting model performance evaluations mlflow recipes inspect --step evaluate --profile local      An example step card produced by running the evaluate step of the MLflow Recipes Regression Template. The step card results indicate that the trained model passed all performance validations and is ready for registration with the MLflow Model Registry.     An example MLflow run view page, showing artifacts logged from the Recipe’s steps.     Example recipe run from the Databricks Notebook included in the MLflow Recipes Regression Template.    Note Data profiling is often best viewed with “quantiles” mode. To switch it on, on the Facet data profile, find Chart to show, click the selector below, and choose Quantiles.    Iterate over step 2 and 3: make changes to an individual step, and test them by running the step and observing the results it produces. Use Recipe.inspect() to visualize the overall Recipe dependency graph and artifacts each step produces. Use Recipe.get_artifact() to further inspect individual step outputs in a notebook. MLflow Recipes intelligently caches results from each Recipe Step, ensuring that steps are only executed if their inputs, code, or configurations have changed, or if such changes have occurred in dependent steps. Once you are satisfied with the results of your changes, commit them to a branch of the Recipe Repository in order to ensure reproducibility, and share or review the changes with your team.     Example Recipe.inspect() output, showing the dependency graph of recipe steps and artifacts each step produces.     Note Before testing changes in a staging or production environment, it is recommended that you commit the changes to a branch of the Recipe Repository to ensure reproducibility.   Note By default, MLflow Recipes caches results from each Recipe Step within the .mlflow subdirectory of the home folder on the local filesystem. The MLFLOW_RECIPES_EXECUTION_DIRECTORY environment variable can be used to specify an alternative location for caching results.       Development environments  We recommend using one of the following environment configurations to develop models with MLflow Recipes:  [Databricks] Edit YAML config and Python files in Databricks Repos. Open separate browser tabs for each file module that you wish to modify. For example, one for the recipe config file recipe.yaml, one for the profile config file profile/databricks.yaml, one for the driver notebook notebooks/databricks.py, and one for the current step (e.g. train) under development steps/train.py. Use notebooks/databricks.py as the driver to run recipe steps and inspect its output. Pin the workspace browser for easy file navigation.    [Local with Jupyter Notebook] Use notebooks/jupyter.ipynb as the driver to run recipe steps and inspect its output. Edit recipe.yaml, steps/*.py and profiles/*.yaml accordingly with an editor of your choice. To run the entire recipe, either run notebooks/jupyter.ipynb or on commandline, invoke mlflow recipes run --profile local (change the current working directory to the project root first).   [Edit locally with IDE (VSCode) and run on Databricks] Edit files on your local machine with VSCode and Jupyter plugin. Use dbx to sync them to Databricks Repos as demonstrated below. On Databricks, use the notebooks/databricks.py notebook as the driver to run recipe steps and inspect their outputs.   Example workflow for efficiently editing a recipe on a local machine and synchronizing changes to Databricks Repos  # Install the Databricks CLI, which is used to remotely access your Databricks Workspace pip install databricks-cli # Configure remote access to your Databricks Workspace databricks configure # Install dbx, which is used to automatically sync changes to and from Databricks Repos pip install dbx # Clone the MLflow Recipes Regression Template git clone https://github.com/mlflow/recipes-regression-template # Enter the MLflow Recipes Regression Template directory and configure dbx within it cd recipes-regression-template dbx configure # Use dbx to enable syncing from the repository directory to Databricks Repos dbx sync repo -d recipes-regression-template # Iteratively make changes to files in the repository directory and observe that they # are automatically synced to Databricks Repos         Recipe Templates  MLflow Recipes currently offers the following predefined templates that can be easily customized to develop and deploy high-quality, production-ready models for your use cases:  MLflow Recipes Regression Template: The MLflow Recipes Regression Template is designed for developing and scoring regression models. For more information, see the Regression Template reference guide. MLflow Recipes Classification Template: The MLflow Recipes Classification Template is designed for developing and scoring classification models. For more information, see the Classification Template reference guide.  Additional recipes for a variety of ML problems and MLOps tasks are under active development.   Detailed reference guide   Template structure  Recipe Templates are git repositories with a standardized, modular layout. The following example provides an overview of the recipe repository structure. It is adapted from the MLflow Recipes Regression Template. ├── recipe.yaml ├── requirements.txt ├── steps │   ├── ingest.py │   ├── split.py │   ├── transform.py │   ├── train.py │   ├── custom_metrics.py ├── profiles │   ├── local.yaml │   ├── databricks.yaml ├── tests │   ├── ingest_test.py │   ├── ... │   ├── train_test.py │   ├── ...   The main components of the Recipe Template layout, which are common across all recipes, are:   recipe.yaml: The main recipe configuration file that declaratively defines the attributes and behavior of each recipe step, such as the input dataset to use for training a model or the performance criteria for promoting a model to production. For reference, see the recipe.yaml configuration file from the MLflow Recipes Regression Template. requirements.txt: A pip requirements file specifying packages that must be installed in order to run the recipe. steps: A directory containing Python code modules used by the recipe steps. For example, the MLflow Recipes Regression Template defines the estimator type and parameters to use when training a model in steps/train.py and defines custom metric computations in steps/custom_metrics.py.   profiles: A directory containing Profile customizations for the configurations defined in recipe.yaml. For example, the MLflow Recipes Regression Template defines a profiles/local.yaml profile that customizes the dataset used for local model development and specifies a local MLflow Tracking store for logging model content. The MLflow Recipes Regression Template also defines a profiles/databricks.yaml profile for development on Databricks. tests: A directory containing Python test code for recipe steps. For example, the MLflow Recipes Regression Template implements tests for the transformer and the estimator defined in the respective steps/transform.py and steps/train.py modules.    Shown below is an example recipe.yaml configuration file adapted from the MLflow Recipes Regression Template. recipe.yaml is the main configuration file for a recipe containing aggregated configurations for all recipe steps; Profile-based substitutions and overrides are supported using Jinja2 templating syntax.  recipe: ""regression/v1"" target_col: ""fare_amount"" primary_metrics: ""root_mean_squared_error"" steps:   ingest: {{INGEST_CONFIG}}   split:     split_ratios: {{SPLIT_RATIOS|default([0.75, 0.125, 0.125])}}   transform:     using: custom     transformer_method: transformer_fn   train:     using: custom     estimator_method: estimator_fn   evaluate:     validation_criteria:       - metric: root_mean_squared_error         threshold: 10       - metric: weighted_mean_squared_error         threshold: 20   register:     allow_non_validated_model: false custom_metrics:   - name: weighted_mean_squared_error     function: weighted_mean_squared_error     greater_is_better: False      Working with profiles  A profile is a collection of customizations for the configurations defined in the recipe’s main recipe.yaml file. Profiles are defined as YAML files within the recipe repository’s profiles directory. When running a recipe or inspecting its results, the desired profile is specified as an API or CLI argument.   Example API and CLI workflows for running recipes with different profile customizations  import os from mlflow.recipes import Recipe  os.chdir(""~/recipes-regression-template"") # Run the regression recipe to train and evaluate the performance of an ElasticNet regressor regression_recipe_local_elasticnet = Recipe(profile=""local-elasticnet"") regression_recipe_local_elasticnet.run() # Run the recipe again to train and evaluate the performance of an SGD regressor regression_recipe_local_sgd = Recipe(profile=""local-sgd"") regression_recipe_local_sgd.run() # After finding the best model type and updating the 'shared-workspace' profile accordingly, # run the recipe again to retrain the best model in a workspace where teammates can view it regression_recipe_shared = Recipe(profile=""shared-workspace"") regression_recipe_shared.run()    git clone https://github.com/mlflow/recipes-regression-template cd recipes-regression-template # Run the regression recipe to train and evaluate the performance of an ElasticNet regressor mlflow recipes run --profile local-elasticnet # Run the recipe again to train and evaluate the performance of an SGD regressor mlflow recipes run --profile local-sgd # After finding the best model type and updating the 'shared-workspace' profile accordingly, # run the recipe again to retrain the best model in a workspace where teammates can view it mlflow recipes run --profile shared-workspace    The following profile customizations are supported:    overrides If the recipe.yaml configuration file defines a Jinja2-templated attribute with a default value, a profile can override the value by mapping the attribute to a different value using YAML dictionary syntax. Note that override values may have arbitrarily nested types (e.g. lists, dictionaries, lists of dictionaries, …).  Example recipe.yaml configuration file defining an overrideable RMSE_THRESHOLD attribute for validating model performance with a default value of 10  steps:   evaluate:     validation_criteria:       - metric: root_mean_squared_error         # The maximum RMSE value on the test dataset that a model can have         # to be eligible for production deployment         threshold: {{RMSE_THRESHOLD|default(10)}}     Example prod.yaml profile that overrides RMSE_THRESHOLD with a custom value to more aggressively validate model quality for production  RMSE_THRESHOLD: 5.2          substitutions If the recipe.yaml configuration file defines a Jinja2-templated attribute without a default value, a profile must map the attribute to a specific value using YAML dictionary syntax. Note that substitute values may have arbitrarily nested types (e.g. lists, dictionaries, lists of dictionaries, …).  Example recipe.yaml configuration file defining a DATASET_INFO variable whose value must be specified by the selected recipe profile  # Specifies the dataset to use for model training ingest: {{INGEST_CONFIG}}     Example dev.yaml profile that provides a value for DATASET_INFO corresponding to a small dataset for development purposes  INGEST_CONFIG:     location: ./data/taxi-small.parquet     format: parquet          additions If the recipe.yaml configuration file does not define a particular attribute, a profile may define it instead. This capability is helpful for providing values of optional configurations that, if unspecified, a recipe would otherwise ignore.  Example local.yaml profile that specifies a sqlite-based MLflow Tracking store for local testing on a laptop  experiment:   tracking_uri: ""sqlite:///metadata/mlflow/mlruns.db""   name: ""sklearn_regression_experiment""   artifact_location: ""./metadata/mlflow/mlartifacts""           Warning If the recipe.yaml configuration file defines an attribute that cannot be overridden or substituted (i.e. because its value is not specified using Jinja2 templating syntax), a profile must not define it. Defining such an attribute in a profile produces an error.           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
plugins.html,"   Documentation  MLflow Plugins       MLflow Plugins  As a framework-agnostic tool for machine learning, the MLflow Python API provides developer APIs for writing plugins that integrate with different ML frameworks and backends. Plugins provide a powerful mechanism for customizing the behavior of the MLflow Python client and integrating third-party tools, allowing you to:  Integrate with third-party storage solutions for experiment data, artifacts, and models Integrate with third-party authentication providers, e.g. read HTTP authentication credentials from a special file Use the MLflow client to communicate with other REST APIs, e.g. your organization’s existing experiment-tracking APIs Automatically capture additional metadata as run tags, e.g. the git repository associated with a run Add new backend to execute MLflow Project entrypoints.  The MLflow Python API supports several types of plugins:  Tracking Store: override tracking backend logic, e.g. to log to a third-party storage solution ArtifactRepository: override artifact logging logic, e.g. to log to a third-party storage solution Run context providers: specify context tags to be set on runs created via the mlflow.start_run() fluent API. Model Registry Store: override model registry backend logic, e.g. to log to a third-party storage solution MLflow Project backend: override the local execution backend to execute a project on your own cluster (Databricks, kubernetes, etc.) MLflow ModelEvaluator: Define custom model evaluator, which can be used in mlflow.evaluate() API.   Table of Contents  Using an MLflow Plugin  Install the Plugin Run Code Using the Plugin Use Plugin for Client Side Authentication   Writing Your Own MLflow Plugins  Defining a Plugin Testing Your Plugin Distributing Your Plugin   Community Plugins  SQL Server Plugin Aliyun(Alibaba Cloud) OSS Plugin XetHub Plugin Deployment Plugins Model Evaluation Plugins Project Backend Plugins Tracking Store Plugins Artifact Repository Plugins      Using an MLflow Plugin  MLflow plugins are Python packages that you can install using PyPI or conda. This example installs a Tracking Store plugin from source and uses it within an example script.  Install the Plugin  To get started, clone MLflow and install this example plugin: git clone https://github.com/mlflow/mlflow cd mlflow pip install -e tests/resources/mlflow-test-plugin     Run Code Using the Plugin  This plugin defines a custom Tracking Store for tracking URIs with the file-plugin scheme. The plugin implementation delegates to MLflow’s built-in file-based run storage. To use the plugin, you can run any code that uses MLflow, setting the tracking URI to one with a file-plugin:// scheme: MLFLOW_TRACKING_URI=file-plugin:$(PWD)/mlruns python examples/quickstart/mlflow_tracking.py   Launch the MLflow UI: cd .. mlflow server --backend-store-uri ./mlflow/mlruns   View results at http://localhost:5000. You should see a newly-created run with a param named “param1” and a metric named “foo”:      Use Plugin for Client Side Authentication  MLflow provides RequestAuthProvider plugin to customize auth header for outgoing http request. To use it, implement the RequestAuthProvider class and override the get_name and get_auth methods. get_name should return the name of your auth provider, while get_auth should return the auth object that will be added to the http request. from mlflow.tracking.request_auth.abstract_request_auth_provider import (     RequestAuthProvider, )   class DummyAuthProvider(RequestAuthProvider):     def get_name(self):         return ""dummy_auth_provider_name""      def get_auth(self):         return DummyAuth()   Once you have the implemented request auth provider class, register it in the entry_points and install the plugin. setup(     entry_points={         ""mlflow.request_auth_provider"": ""dummy-backend=DummyAuthProvider"",     }, )   Then set environment variable MLFLOW_TRACKING_AUTH to enable the injection of custom auth. The value of this environment variable should match the name of the auth provider. export MLFLOW_TRACKING_AUTH=dummy_auth_provider_name      Writing Your Own MLflow Plugins   Defining a Plugin  You define an MLflow plugin as a standalone Python package that can be distributed for installation via PyPI or conda. See https://github.com/mlflow/mlflow/tree/master/tests/resources/mlflow-test-plugin for an example package that implements all available plugin types. The example package contains a setup.py that declares a number of entry points: setup(     name=""mflow-test-plugin"",     # Require MLflow as a dependency of the plugin, so that plugin users can simply install     # the plugin and then immediately use it with MLflow     install_requires=[""mlflow""],     ...,     entry_points={         # Define a Tracking Store plugin for tracking URIs with scheme 'file-plugin'         ""mlflow.tracking_store"": ""file-plugin=mlflow_test_plugin.file_store:PluginFileStore"",         # Define a ArtifactRepository plugin for artifact URIs with scheme 'file-plugin'         ""mlflow.artifact_repository"": ""file-plugin=mlflow_test_plugin.local_artifact:PluginLocalArtifactRepository"",         # Define a RunContextProvider plugin. The entry point name for run context providers         # is not used, and so is set to the string ""unused"" here         ""mlflow.run_context_provider"": ""unused=mlflow_test_plugin.run_context_provider:PluginRunContextProvider"",         # Define a RequestHeaderProvider plugin. The entry point name for request header providers         # is not used, and so is set to the string ""unused"" here         ""mlflow.request_header_provider"": ""unused=mlflow_test_plugin.request_header_provider:PluginRequestHeaderProvider"",         # Define a RequestAuthProvider plugin. The entry point name for request auth providers         # is not used, and so is set to the string ""unused"" here         ""mlflow.request_auth_provider"": ""unused=mlflow_test_plugin.request_auth_provider:PluginRequestAuthProvider"",         # Define a Model Registry Store plugin for tracking URIs with scheme 'file-plugin'         ""mlflow.model_registry_store"": ""file-plugin=mlflow_test_plugin.sqlalchemy_store:PluginRegistrySqlAlchemyStore"",         # Define a MLflow Project Backend plugin called 'dummy-backend'         ""mlflow.project_backend"": ""dummy-backend=mlflow_test_plugin.dummy_backend:PluginDummyProjectBackend"",         # Define a MLflow model deployment plugin for target 'faketarget'         ""mlflow.deployments"": ""faketarget=mlflow_test_plugin.fake_deployment_plugin"",         # Define a MLflow model evaluator with name ""dummy_evaluator""         ""mlflow.model_evaluator"": ""dummy_evaluator=mlflow_test_plugin.dummy_evaluator:DummyEvaluator"",     }, )   Each element of this entry_points dictionary specifies a single plugin. You can choose to implement one or more plugin types in your package, and need not implement them all. The type of plugin defined by each entry point and its corresponding reference implementation in MLflow are described below. You can work from the reference implementations when writing your own plugin:         Description Entry-point group Entry-point name and value Reference Implementation    Plugins for overriding definitions of tracking APIs like mlflow.log_metric, mlflow.start_run for a specific tracking URI scheme. mlflow.tracking_store The entry point value (e.g. mlflow_test_plugin.local_store:PluginFileStore) specifies a custom subclass of mlflow.tracking.store.AbstractStore (e.g., the PluginFileStore class within the mlflow_test_plugin module). The entry point name (e.g. file-plugin) is the tracking URI scheme with which to associate the custom AbstractStore implementation. Users who install the example plugin and set a tracking URI of the form file-plugin://<path> will use the custom AbstractStore implementation defined in PluginFileStore. The full tracking URI is passed to the PluginFileStore constructor.  FileStore  Plugins for defining artifact read/write APIs like mlflow.log_artifact, MlflowClient.download_artifacts for a specified artifact URI scheme (e.g. the scheme used by your in-house blob storage system). mlflow.artifact_repository The entry point value (e.g. mlflow_test_plugin.local_artifact:PluginLocalArtifactRepository) specifies a custom subclass of mlflow.store.artifact.artifact_repo.ArtifactRepository (e.g., the PluginLocalArtifactRepository class within the mlflow_test_plugin module). The entry point name (e.g. file-plugin) is the artifact URI scheme with which to associate the custom ArtifactRepository implementation. Users who install the example plugin and log to a run whose artifact URI is of the form file-plugin://<path> will use the custom ArtifactRepository implementation defined in PluginLocalArtifactRepository. The full artifact URI is passed to the PluginLocalArtifactRepository constructor.  LocalArtifactRepository  Plugins for specifying custom context tags at run creation time, e.g. tags identifying the git repository associated with a run. mlflow.run_context_provider The entry point name is unused. The entry point value (e.g. mlflow_test_plugin.run_context_provider:PluginRunContextProvider) specifies a custom subclass of mlflow.tracking.context.abstract_context.RunContextProvider (e.g., the PluginRunContextProvider class within the mlflow_test_plugin module) to register. GitRunContext, DefaultRunContext  Plugins for specifying custom context request headers to attach to outgoing requests, e.g. headers identifying the client’s environment. mlflow.request_header_provider The entry point name is unused. The entry point value (e.g. mlflow_test_plugin.request_header_provider:PluginRequestHeaderProvider) specifies a custom subclass of mlflow.tracking.request_header.abstract_request_header_provider.RequestHeaderProvider (e.g., the PluginRequestHeaderProvider class within the mlflow_test_plugin module) to register. DatabricksRequestHeaderProvider  Plugins for specifying custom request auth to attach to outgoing requests. mlflow.request_auth_provider The entry point name is unused. The entry point value (e.g. mlflow_test_plugin.request_auth_provider:PluginRequestAuthProvider) specifies a custom subclass of mlflow.tracking.request_auth.abstract_request_auth_provider.RequestAuthProvider (e.g., the PluginRequestAuthProvider class within the mlflow_test_plugin module) to register. N/A (will be added soon)  Plugins for overriding definitions of Model Registry APIs like mlflow.register_model. mlflow.model_registry_store The entry point value (e.g. mlflow_test_plugin.sqlalchemy_store:PluginRegistrySqlAlchemyStore) specifies a custom subclass of mlflow.tracking.model_registry.AbstractStore (e.g., the PluginRegistrySqlAlchemyStore class within the mlflow_test_plugin module) The entry point name (e.g. file-plugin) is the tracking URI scheme with which to associate the custom AbstractStore implementation. Users who install the example plugin and set a tracking URI of the form file-plugin://<path> will use the custom AbstractStore implementation defined in PluginFileStore. The full tracking URI is passed to the PluginFileStore constructor.  SqlAlchemyStore  Plugins for running MLflow projects against custom execution backends (e.g. to run projects against your team’s in-house cluster or job scheduler). mlflow.project.backend The entry point value (e.g. mlflow_test_plugin.dummy_backend:PluginDummyProjectBackend) specifies a custom subclass of mlflow.project.backend.AbstractBackend) N/A (will be added soon)  Plugins for deploying models to custom serving tools. mlflow.deployments The entry point name (e.g. redisai) is the target name. The entry point value (e.g. mlflow_test_plugin.fake_deployment_plugin) specifies a module defining: 1) Exactly one subclass of mlflow.deployments.BaseDeploymentClient (e.g., the PluginDeploymentClient class). MLflow’s mlflow.deployments.get_deploy_client API directly returns an instance of this subclass to the user, so you’re encouraged to write clear user-facing method and class docstrings as part of your plugin implementation. 1) The run_local and target_help functions, with the target parameter excluded, as shown here PluginDeploymentClient.  Plugins for MLflow Model Evaluation mlflow.model_evaluator The entry point name (e.g. dummy_evaluator) is the evaluator name which is used in the evaluators argument of the mlflow.evaluate API. The entry point value (e.g. dummy_evaluator:DummyEvaluator) must refer to a subclass of mlflow.models.evaluation.ModelEvaluator; the subclass must implement 2 methods: 1) can_evaluate: Accepts the keyword-only arguments model_type and evaluator_config. Returns True if the evaluator can evaluate the specified model type with the specified evaluator config. Returns False otherwise. 1) evaluate: Computes and logs metrics and artifacts, returning evaluation results as an instance of mlflow.models.EvaluationResult. Accepts the following arguments: model (a pyfunc model instance), model_type (identical to the model_type argument from mlflow.evaluate()), dataset (an instance of mlflow.models.evaluation.base._EvaluationDataset containing features and labels (optional) for model evaluation), run_id (the ID of the MLflow Run to which to log results), and evaluator_config (a dictionary of additional configurations for the evaluator). DummyEvaluator.  [Experimental] Plugins for custom mlflow server flask app configuration mlflow.server.app. mlflow.app The entry point <app_name>=<object_reference> (e.g. custom_app=mlflow_test_plugin.app:app) specifies a customized flask application. This can be useful for implementing request hooks for authentication/authorization, custom logging and custom flask configurations. The plugin must import mlflow.server.app (e.g. from mlflow.server import app) and may add custom configuration, middleware etc. to the app. The plugin should avoid altering the existing application routes, handlers and environment variables to avoid unexpected behavior. Users who install the example plugin will have a customized flask application. To run the customized flask application, use mlflow server --app-name <app_name>. app.      Testing Your Plugin  We recommend testing your plugin to ensure that it follows the contract expected by MLflow. For example, a Tracking Store plugin should contain tests verifying correctness of its log_metric, log_param, … etc implementations. See also the tests for MLflow’s reference implementations as an example:  Example Tracking Store tests Example ArtifactRepository tests Example RunContextProvider tests Example Model Registry Store tests Example Custom MLflow Evaluator tests Example Custom MLflow server tests    Distributing Your Plugin  Assuming you’ve structured your plugin similarly to the example plugin, you can distribute it via PyPI. Congrats, you’ve now written and distributed your own MLflow plugin!    Community Plugins   SQL Server Plugin  The mlflow-dbstore plugin allows MLflow to use a relational database as an artifact store. As of now, it has only been tested with SQL Server as the artifact store. You can install MLflow with the SQL Server plugin via: pip install mlflow[sqlserver]   and then use MLflow as normal. The SQL Server artifact store support will be provided automatically. The plugin implements all of the MLflow artifact store APIs. To use SQL server as an artifact store, a database URI must be provided, as shown in the example below: db_uri = ""mssql+pyodbc://username:password@host:port/database?driver=ODBC+Driver+17+for+SQL+Server""  client.create_experiment(exp_name, artifact_location=db_uri) mlflow.set_experiment(exp_name)  mlflow.onnx.log_model(onnx, ""model"")   The first time an artifact is logged in the artifact store, the plugin automatically creates an artifacts table in the database specified by the database URI and stores the artifact there as a BLOB. Subsequent logged artifacts are stored in the same table. In the example provided above, the log_model operation creates three entries in the database table to store the ONNX model, the MLmodel file and the conda.yaml file associated with the model.   Aliyun(Alibaba Cloud) OSS Plugin  The aliyunstoreplugin allows MLflow to use Alibaba Cloud OSS storage as an artifact store. pip install mlflow[aliyun-oss]   and then use MLflow as normal. The Alibaba Cloud OSS artifact store support will be provided automatically. The plugin implements all of the MLflow artifact store APIs. It expects Aliyun Storage access credentials in the MLFLOW_OSS_ENDPOINT_URL, MLFLOW_OSS_KEY_ID and MLFLOW_OSS_KEY_SECRET environment variables, so you must set these variables on both your client application and your MLflow tracking server. To use Aliyun OSS as an artifact store, an OSS URI of the form oss://<bucket>/<path> must be provided, as shown in the example below: import mlflow import mlflow.pyfunc   class Mod(mlflow.pyfunc.PythonModel):     def predict(self, ctx, inp, params=None):         return 7   exp_name = ""myexp"" mlflow.create_experiment(exp_name, artifact_location=""oss://mlflow-test/"") mlflow.set_experiment(exp_name) mlflow.pyfunc.log_model(""model_test"", python_model=Mod())   In the example provided above, the log_model operation creates three entries in the OSS storage oss://mlflow-test/$RUN_ID/artifacts/model_test/, the MLmodel file and the conda.yaml file associated with the model.   XetHub Plugin  The xethub plugin allows MLflow to use XetHub storage as an artifact store. pip install mlflow[xethub]   and then use MLflow as normal. The XetHub artifact store support will be provided automatically. The plugin implements all of the MLflow artifact store APIs. It expects XetHub access credentials through xet login CLI command or in the XET_USER_EMAIL, XET_USER_NAME and XET_USER_TOKEN environment variables, so you must authenticate with XetHub for both your client application and your MLflow tracking server. To use XetHub as an artifact store, an XetHub URI of the form xet://<username>/<repo>/<branch> must be provided, as shown in the example below: import mlflow import mlflow.pyfunc   class Mod(mlflow.pyfunc.PythonModel):     def predict(self, ctx, inp, params=None):         return 7   exp_name = ""myexp"" mlflow.create_experiment(     exp_name, artifact_location=""xet://<your_username>/mlflow-test/main"" ) mlflow.set_experiment(exp_name) mlflow.pyfunc.log_model(""model_test"", python_model=Mod())   In the example provided above, the log_model operation creates three entries in the OSS storage xet://mlflow-test/$RUN_ID/artifacts/model_test/, the MLmodel file and the conda.yaml file associated with the model.   Deployment Plugins  The following known plugins provide support for deploying models to custom serving tools using MLflow’s model deployment APIs. See the individual plugin pages for installation instructions, and see the Python API docs and CLI docs for usage instructions and examples.  mlflow-redisai mlflow-torchserve mlflow-algorithmia mlflow-ray-serve mlflow-azureml oci-mlflow Leverages Oracle Cloud Infrastructure (OCI) Model Deployment service for the deployment of MLflow models.    Model Evaluation Plugins  The following known plugins provide support for evaluating models with custom validation tools using MLflow’s mlflow.evaluate() API:  mlflow-giskard: Detect hidden vulnerabilities in ML models, from tabular to LLMs, before moving to production. Anticipate issues such as Performance bias, Unrobustness, Overconfidence, Underconfidence, Ethical bias, Data leakage, Stochasticity, Spurious correlation, and others. Conduct model comparisons using a wide range of tests, either through custom or domain-specific test suites. mlflow-trubrics: validating ML models with Trubrics    Project Backend Plugins  The following known plugins provide support for running MLflow projects against custom execution backends.  mlflow-yarn Running mlflow on Hadoop/YARN oci-mlflow Running mlflow projects on Oracle Cloud Infrastructure (OCI)    Tracking Store Plugins  The following known plugins provide support for running MLflow Tracking Store against custom databases.  mlflow-elasticsearchstore Running MLflow Tracking Store with Elasticsearch  For additional information regarding this plugin, refer to <https://github.com/criteo/mlflow-elasticsearchstore/issues>. The library is available on PyPI here : <https://pypi.org/project/mlflow-elasticsearchstore/>   Artifact Repository Plugins   oci-mlflow Leverages Oracle Cloud Infrastructure (OCI) Object Storage service to store MLflow models artifacts.          Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
auth/index.html,"   Documentation  MLflow Authentication       MLflow Authentication   Note This feature is still experimental and may change in a future release without warning.  MLflow supports basic HTTP authentication to enable access control over experiments and registered models. Once enabled, any visitor will be required to login before they can view any resource from the Tracking Server.  Table of Contents  Overview How It Works  Permissions Permissions Database Admin Users Managing Permissions   Authenticating to MLflow  Using MLflow UI Using Environment Variables Using Credentials File Using REST API   Creating a New User  Using MLflow UI Using REST API Using MLflow AuthServiceClient   Configuration Connecting to a Centralized Database Custom Authentication   MLflow Authentication provides Python and REST API for managing users and permissions.   MLflow Authentication Python API MLflow Authentication REST API    Overview  To enable MLflow authentication, launch the MLflow UI with the following command: mlflow server --app-name basic-auth   Server admin can choose to disable this feature anytime by restarting the server without the app-name flag. Any users and permissions created will be persisted on a SQL database and will be back in service once the feature is re-enabled. Due to the nature of HTTP authentication, it is only supported on a remote Tracking Server, where users send requests to the server via REST APIs.   How It Works   Permissions  The available permissions are:          Permission Can read Can update Can delete Can manage    READ Yes No No No  EDIT Yes Yes No No  MANAGE Yes Yes Yes Yes  NO_PERMISSIONS No No No No    The default permission for all users is READ. It can be changed in the configuration file. Permissions can be granted on individual resources for each user. Supported resources include Experiment and Registered Model. To access an API endpoint, an user must have the required permission. Otherwise, a 403 Forbidden response will be returned. Required Permissions for accessing experiments:         API Endpoint Method Required permission    Create Experiment 2.0/mlflow/experiments/create POST None  Get Experiment 2.0/mlflow/experiments/get GET can_read  Get Experiment By Name 2.0/mlflow/experiments/get-by-name GET can_read  Delete Experiment 2.0/mlflow/experiments/delete POST can_delete  Restore Experiment 2.0/mlflow/experiments/restore POST can_delete  Update Experiment 2.0/mlflow/experiments/update POST can_update  Search Experiments 2.0/mlflow/experiments/search POST None  Search Experiments 2.0/mlflow/experiments/search GET None  Set Experiment Tag 2.0/mlflow/experiments/set-experiment-tag POST can_update  Create Run 2.0/mlflow/runs/create POST can_update  Get Run 2.0/mlflow/runs/get GET can_read  Update Run 2.0/mlflow/runs/update POST can_update  Delete Run 2.0/mlflow/runs/delete POST can_delete  Restore Run 2.0/mlflow/runs/restore POST can_delete  Search Runs 2.0/mlflow/runs/search POST None  Set Tag 2.0/mlflow/runs/set-tag POST can_update  Delete Tag 2.0/mlflow/runs/delete-tag POST can_update  Log Metric 2.0/mlflow/runs/log-metric POST can_update  Log Param 2.0/mlflow/runs/log-parameter POST can_update  Log Batch 2.0/mlflow/runs/log-batch POST can_update  Log Model 2.0/mlflow/runs/log-model POST can_update  List Artifacts 2.0/mlflow/artifacts/list GET can_read  Get Metric History 2.0/mlflow/metrics/get-history GET can_read    Required Permissions for accessing registered models:         API Endpoint Method Required permission    Create Registered Model 2.0/mlflow/registered-models/create POST None  Rename Registered Model 2.0/mlflow/registered-models/rename POST can_update  Update Registered Model 2.0/mlflow/registered-models/update PATCH can_update  Delete Registered Model 2.0/mlflow/registered-models/delete DELETE can_delete  Get Registered Model 2.0/mlflow/registered-models/get GET can_read  Search Registered Models 2.0/mlflow/registered-models/search GET None  Get Latest Versions 2.0/mlflow/registered-models/get-latest-versions POST can_read  Get Latest Versions 2.0/mlflow/registered-models/get-latest-versions GET can_read  Set Registered Model Tag 2.0/mlflow/registered-models/set-tag POST can_update  Delete Registered Model Tag 2.0/mlflow/registered-models/delete-tag DELETE can_update  Set Registered Model Alias 2.0/mlflow/registered-models/alias POST can_update  Delete Registered Model Alias 2.0/mlflow/registered-models/alias DELETE can_delete  Get Model Version By Alias 2.0/mlflow/registered-models/alias GET can_read  Create Model Version 2.0/mlflow/model-versions/create POST can_update  Update Model Version 2.0/mlflow/model-versions/update PATCH can_update  Transition Model Version Stage 2.0/mlflow/model-versions/transition-stage POST can_update  Delete Model Version 2.0/mlflow/model-versions/delete DELETE can_delete  Get Model Version 2.0/mlflow/model-versions/get GET can_read  Search Model Versions 2.0/mlflow/model-versions/search GET None  Get Model Version Download Uri 2.0/mlflow/model-versions/get-download-uri GET can_read  Set Model Version Tag 2.0/mlflow/model-versions/set-tag POST can_update  Delete Model Version Tag 2.0/mlflow/model-versions/delete-tag DELETE can_delete    MLflow Authentication introduces several new API endpoints to manage users and permissions.         API Endpoint Method Required permission    Create User 2.0/mlflow/users/create POST None  Get User 2.0/mlflow/users/get GET Only readable by that user  Update User Password 2.0/mlflow/users/update-password PATCH Only updatable by that user  Update User Admin 2.0/mlflow/users/update-admin PATCH Only admin  Delete User 2.0/mlflow/users/delete DELETE Only admin  Create Experiment Permission 2.0/mlflow/experiments/permissions/create POST can_manage  Get Experiment Permission 2.0/mlflow/experiments/permissions/get GET can_manage  Update Experiment Permission 2.0/mlflow/experiments/permissions/update PATCH can_manage  Delete Experiment Permission 2.0/mlflow/experiments/permissions/delete DELETE can_manage  Create Registered Model Permission 2.0/mlflow/registered-models/permissions/create POST can_manage  Get Registered Model Permission 2.0/mlflow/registered-models/permissions/get GET can_manage  Update Registered Model Permission 2.0/mlflow/registered-models/permissions/update PATCH can_manage  Delete Registered Model Permission 2.0/mlflow/registered-models/permissions/delete DELETE can_manage    Some APIs will also have their behaviour modified. For example, the creator of an experiment will automatically be granted MANAGE permission on that experiment, so that the creator can grant or revoke other users’ access to that experiment.         API Endpoint Method Effect    Create Experiment 2.0/mlflow/experiments/create POST Automatically grants MANAGE permission to the creator.  Create Registered Model 2.0/mlflow/registered-models/create POST Automatically grants MANAGE permission to the creator.  Search Experiments 2.0/mlflow/experiments/search POST Only returns experiments which the user has READ permission on.  Search Experiments 2.0/mlflow/experiments/search GET Only returns experiments which the user has READ permission on.  Search Runs 2.0/mlflow/runs/search POST Only returns experiments which the user has READ permission on.  Search Registered Models 2.0/mlflow/registered-models/search GET Only returns registered models which the user has READ permission on.  Search Model Versions 2.0/mlflow/model-versions/search GET Only returns registered models which the user has READ permission on.      Permissions Database  All users and permissions are stored in a database in basic_auth.db, relative to the directory where MLflow server is launched. The location can be changed in the configuration file. To run migrations, use the following command: python -m mlflow.server.auth db upgrade --url <database_url>     Admin Users  Admin users have unrestricted access to all MLflow resources, including creating or deleting users, updating password and admin status of other users, granting or revoking permissions from other users, and managing permissions for all MLflow resources, even if NO_PERMISSIONS is explicitly set to that admin account. MLflow has a built-in admin user that will be created the first time that the MLflow authentication feature is enabled.  Note It is recommended that you update the default admin password as soon as possible after creation.  The default admin user credentials are as follows:       Username Password    admin password    Multiple admin users can exist by promoting other users to admin, using the 2.0/mlflow/users/update-admin endpoint.  Example  # authenticate as built-in admin user export MLFLOW_TRACKING_USERNAME=admin export MLFLOW_TRACKING_PASSWORD=password    from mlflow.server import get_app_client  tracking_uri = ""http://localhost:5000/""  auth_client = get_app_client(""basic-auth"", tracking_uri=tracking_uri) auth_client.create_user(username=""user1"", password=""pw1"") auth_client.update_user_admin(username=""user1"", is_admin=True)     Managing Permissions  MLflow provides REST APIs and a client class AuthServiceClient to manage users and permissions. To instantiate AuthServiceClient, it is recommended that you use mlflow.server.get_app_client().  Example  export MLFLOW_TRACKING_USERNAME=admin export MLFLOW_TRACKING_PASSWORD=password    from mlflow import MlflowClient from mlflow.server import get_app_client  tracking_uri = ""http://localhost:5000/""  auth_client = get_app_client(""basic-auth"", tracking_uri=tracking_uri) auth_client.create_user(username=""user1"", password=""pw1"") auth_client.create_user(username=""user2"", password=""pw2"")  client = MlflowClient(tracking_uri=tracking_uri) experiment_id = client.create_experiment(name=""experiment"")  auth_client.create_experiment_permission(     experiment_id=experiment_id, username=""user2"", permission=""MANAGE"" )      Authenticating to MLflow   Using MLflow UI  When a user first visits the MLflow UI on a browser, they will be prompted to login. There is no limit to how many login attempts can be made. Currently, MLflow UI does not display any information about the current user. Once a user is logged in, the only way to log out is to close the browser.      Using Environment Variables  MLflow provides two environment variables for authentication: MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD. To use basic authentication, you must set both environment variables. export MLFLOW_TRACKING_USERNAME=username export MLFLOW_TRACKING_PASSWORD=password   import mlflow  mlflow.set_tracking_uri(""https://<mlflow_tracking_uri>/"") with mlflow.start_run():     ...     Using Credentials File  You can save your credentials in a file to remove the need for setting environment variables every time. The credentials should be saved in ~/.mlflow/credentials using INI format. Note that the password will be stored unencrypted on disk, and is protected only by filesystem permissions. If the environment variables MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD are configured, they override any credentials provided in the credentials file.  Credentials file format  [mlflow] mlflow_tracking_username = username mlflow_tracking_password = password      Using REST API  A user can authenticate using the HTTP Authorization request header. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication for more information. In Python, you can use the requests library: import requests  response = requests.get(     ""https://<mlflow_tracking_uri>/"",     auth=(""username"", ""password""), )      Creating a New User   Important To create a new user, you are required to authenticate with admin privileges.   Using MLflow UI  MLflow UI provides a simple page for creating new users at <tracking_uri>/signup.      Using REST API  Alternatively, you can send POST requests to the Tracking Server endpoint 2.0/users/create. In Python, you can use the requests library: import requests  response = requests.post(     ""https://<mlflow_tracking_uri>/api/2.0/mlflow/users/create"",     json={         ""username"": ""username"",         ""password"": ""password"",     }, )     Using MLflow AuthServiceClient  MLflow AuthServiceClient provides a function to create new users easily. import mlflow  auth_client = mlflow.server.get_app_client(     ""basic-auth"", tracking_uri=""https://<mlflow_tracking_uri>/"" ) auth_client.create_user(username=""username"", password=""password"")      Configuration  Authentication configuration is located at mlflow/server/auth/basic_auth.ini:       Variable Description    default_permission Default permission on all resources  database_uri Database location to store permission and user data  admin_username Default admin username if the admin is not already created  admin_password Default admin password if the admin is not already created  authorization_function Function to authenticate requests    Alternatively, assign the environment variable MLFLOW_AUTH_CONFIG_PATH to point to your custom configuration file. The authorization_function setting supports pluggable authentication methods if you want to use another authentication method than HTTP basic auth. The value specifies module_name:function_name. The function has the following signature:  def authenticate_request() -> Union[Authorization, Response]:     ...    The function should return a werkzeug.datastructures.Authorization object if the request is authenticated, or a Response object (typically 401: Unauthorized) if the request is not authenticated. For an example of how to implement a custom authentication method, see tests/server/auth/jwt_auth.py. NOTE: This example is not intended for production use.   Connecting to a Centralized Database  By default, MLflow Authentication uses a local SQLite database to store user and permission data. In the case of a multi-node deployment, it is recommended to use a centralized database to store this data. To connect to a centralized database, you can set the database_uri configuration variable to the database URL.  Example: /path/to/my_auth_config.ini  [mlflow] database_uri = postgresql://username:password@hostname:port/database    Then, start the MLflow server with the MLFLOW_AUTH_CONFIG_PATH environment variable set to the path of your configuration file. MLFLOW_AUTH_CONFIG_PATH=/path/to/my_auth_config.ini mlflow server --app-name basic-auth   The database must be created before starting the MLflow server. The database schema will be created automatically when the server starts.   Custom Authentication  MLflow authentication is designed to be extensible. If your organization desires more advanced authentication logic (e.g., token-based authentication), it is possible to install a third party plugin or to create your own plugin. Your plugin should be an installable Python package. It should include an app factory that extends the MLflow app and, optionally, implement a client to manage permissions. The app factory function name will be passed to the --app argument in Flask CLI. See https://flask.palletsprojects.com/en/latest/cli/#application-discovery for more information.  Example: my_auth/__init__.py  from flask import Flask from mlflow.server import app   def create_app(app: Flask = app):     app.add_url_rule(...)     return app   class MyAuthClient:     ...    Then, the plugin should be installed in your Python environment: pip install my_auth   Then, register your plugin in mlflow/setup.py: setup(     ...,     entry_points=""""""         ...          [mlflow.app]         my-auth=my_auth:create_app          [mlflow.app.client]         my-auth=my_auth:MyAuthClient     """""", )   Then, you can start the MLflow server: mlflow server --app-name my-auth          Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
cli.html,"   Documentation  Command-Line Interface       Command-Line Interface  The MLflow command-line interface (CLI) provides a simple interface to various functionality in MLflow. You can use the CLI to run projects, start the tracking UI, create and list experiments, download run artifacts, serve MLflow Python Function and scikit-learn models, serve MLflow Python Function and scikit-learn models, and serve models on Microsoft Azure Machine Learning and Amazon SageMaker. Each individual command has a detailed help screen accessible via mlflow command_name --help.  Table of Contents  mlflow  artifacts db deployments doctor experiments gateway gc models recipes run runs sagemaker server      mlflow  mlflow [OPTIONS] COMMAND [ARGS]...   Options   --version  Show the version and exit.   artifacts  Upload, list, and download artifacts from an MLflow artifact repository. To manage artifacts for a run associated with a tracking server, set the MLFLOW_TRACKING_URI environment variable to the URL of the desired server. mlflow artifacts [OPTIONS] COMMAND [ARGS]...    download  Download an artifact file or directory to a local directory. The output is the name of the file or directory on the local filesystem. Either --artifact-uri or --run-id must be provided. mlflow artifacts download [OPTIONS]   Options   -r, --run-id <run_id>  Run ID from which to download    -a, --artifact-path <artifact_path>  For use with Run ID: if specified, a path relative to the run’s root directory to download    -u, --artifact-uri <artifact_uri>  URI pointing to the artifact file or artifacts directory; use as an alternative to specifying –run_id and –artifact-path    -d, --dst-path <dst_path>  Path of the local filesystem destination directory to which to download the specified artifacts. If the directory does not exist, it is created. If unspecified the artifacts are downloaded to a new uniquely-named directory on the local filesystem, unless the artifacts already exist on the local filesystem, in which case their local path is returned directly    list  Return all the artifacts directly under run’s root artifact directory, or a sub-directory. The output is a JSON-formatted list. mlflow artifacts list [OPTIONS]   Options   -r, --run-id <run_id>  Required Run ID to be listed    -a, --artifact-path <artifact_path>  If specified, a path relative to the run’s root directory to list.    log-artifact  Log a local file as an artifact of a run, optionally within a run-specific artifact path. Run artifacts can be organized into directories, so you can place the artifact in a directory this way. mlflow artifacts log-artifact [OPTIONS]   Options   -l, --local-file <local_file>  Required Local path to artifact to log    -r, --run-id <run_id>  Required Run ID into which we should log the artifact.    -a, --artifact-path <artifact_path>  If specified, we will log the artifact into this subdirectory of the run’s artifact directory.    log-artifacts  Log the files within a local directory as an artifact of a run, optionally within a run-specific artifact path. Run artifacts can be organized into directories, so you can place the artifact in a directory this way. mlflow artifacts log-artifacts [OPTIONS]   Options   -l, --local-dir <local_dir>  Required Directory of local artifacts to log    -r, --run-id <run_id>  Required Run ID into which we should log the artifact.    -a, --artifact-path <artifact_path>  If specified, we will log the artifact into this subdirectory of the run’s artifact directory.     db  Commands for managing an MLflow tracking database. mlflow db [OPTIONS] COMMAND [ARGS]...    upgrade  Upgrade the schema of an MLflow tracking database to the latest supported version. IMPORTANT: Schema migrations can be slow and are not guaranteed to be transactional - always take a backup of your database before running migrations. The migrations README, which is located at https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md, describes large migrations and includes information about how to estimate their performance and recover from failures. mlflow db upgrade [OPTIONS] URL   Arguments   URL  Required argument     deployments  Deploy MLflow models to custom targets. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions in https://mlflow.org/docs/latest/plugins.html#community-plugins You can also write your own plugin for deployment to a custom target. For instructions on writing and distributing a plugin, see https://mlflow.org/docs/latest/plugins.html#writing-your-own-mlflow-plugins. mlflow deployments [OPTIONS] COMMAND [ARGS]...    create  Deploy the model at model_uri to the specified target. Additional plugin-specific arguments may also be passed to this command, via -C key=value mlflow deployments create [OPTIONS]   Options   --endpoint <endpoint>  Name of the endpoint    -C, --config <NAME=VALUE>  Extra target-specific config for the model deployment, of the form -C name=value. See documentation/help for your deployment target for a list of supported config options.    --name <name>  Required Name of the deployment    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -f, --flavor <flavor>  Which flavor to be deployed. This will be auto inferred if it’s not given    create-endpoint  Create an endpoint with the specified name at the specified target. Additional plugin-specific arguments may also be passed to this command, via -C key=value mlflow deployments create-endpoint [OPTIONS]   Options   -C, --config <NAME=VALUE>  Extra target-specific config for the endpoint, of the form -C name=value. See documentation/help for your deployment target for a list of supported config options.    --endpoint <endpoint>  Required Name of the endpoint    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    delete  Delete the deployment with name given at –name from the specified target. mlflow deployments delete [OPTIONS]   Options   --endpoint <endpoint>  Name of the endpoint    -C, --config <NAME=VALUE>  Extra target-specific config for the model deployment, of the form -C name=value. See documentation/help for your deployment target for a list of supported config options.    --name <name>  Required Name of the deployment    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    delete-endpoint  Delete the specified endpoint at the specified target mlflow deployments delete-endpoint [OPTIONS]   Options   --endpoint <endpoint>  Required Name of the endpoint    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    explain  Generate explanations of model predictions on the specified input for the deployed model for the given input(s). Explanation output formats vary by deployment target, and can include details like feature importance for understanding/debugging predictions. Run mlflow deployments help or consult the documentation for your plugin for details on explanation format. For information about the input data formats accepted by this function, see the following documentation: https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools mlflow deployments explain [OPTIONS]   Options   --name <name>  Name of the deployment. Exactly one of –name or –endpoint must be specified.    --endpoint <endpoint>  Name of the endpoint. Exactly one of –name or –endpoint must be specified.    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    -I, --input-path <input_path>  Required Path to input json file for prediction    -O, --output-path <output_path>  File to output results to as a JSON file. If not provided, prints output to stdout.    get  Print a detailed description of the deployment with name given at --name in the specified target. mlflow deployments get [OPTIONS]   Options   --endpoint <endpoint>  Name of the endpoint    --name <name>  Required Name of the deployment    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    get-endpoint  Get details for the specified endpoint at the specified target mlflow deployments get-endpoint [OPTIONS]   Options   --endpoint <endpoint>  Required Name of the endpoint    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    help  Display additional help for a specific deployment target, e.g. info on target-specific config options and the target’s URI format. mlflow deployments help [OPTIONS]   Options   -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    list  List the names of all model deployments in the specified target. These names can be used with the delete, update, and get commands. mlflow deployments list [OPTIONS]   Options   --endpoint <endpoint>  Name of the endpoint    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    list-endpoints  List all endpoints at the specified target mlflow deployments list-endpoints [OPTIONS]   Options   -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    predict  Predict the results for the deployed model for the given input(s) mlflow deployments predict [OPTIONS]   Options   --name <name>  Name of the deployment. Exactly one of –name or –endpoint must be specified.    --endpoint <endpoint>  Name of the endpoint. Exactly one of –name or –endpoint must be specified.    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    -I, --input-path <input_path>  Required Path to input json file for prediction    -O, --output-path <output_path>  File to output results to as a JSON file. If not provided, prints output to stdout.    run-local  Deploy the model locally. This has very similar signature to create API mlflow deployments run-local [OPTIONS]   Options   -C, --config <NAME=VALUE>  Extra target-specific config for the model deployment, of the form -C name=value. See documentation/help for your deployment target for a list of supported config options.    --name <name>  Required Name of the deployment    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -f, --flavor <flavor>  Which flavor to be deployed. This will be auto inferred if it’s not given    start-server  Start the MLflow Deployments server mlflow deployments start-server [OPTIONS]   Options   --config-path <config_path>  Required The path to the deployments configuration file.    --host <host>  The network address to listen on (default: 127.0.0.1).    --port <port>  The port to listen on (default: 5000).    --workers <workers>  The number of workers.  Environment variables   MLFLOW_DEPLOYMENTS_CONFIG  Provide a default for --config-path     update  Update the deployment with ID deployment_id in the specified target. You can update the URI of the model and/or the flavor of the deployed model (in which case the model URI must also be specified). Additional plugin-specific arguments may also be passed to this command, via -C key=value. mlflow deployments update [OPTIONS]   Options   --endpoint <endpoint>  Name of the endpoint    -C, --config <NAME=VALUE>  Extra target-specific config for the model deployment, of the form -C name=value. See documentation/help for your deployment target for a list of supported config options.    --name <name>  Required Name of the deployment    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins    -m, --model-uri <URI>  URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -f, --flavor <flavor>  Which flavor to be deployed. This will be auto inferred if it’s not given    update-endpoint  Update the specified endpoint at the specified target. Additional plugin-specific arguments may also be passed to this command, via -C key=value mlflow deployments update-endpoint [OPTIONS]   Options   -C, --config <NAME=VALUE>  Extra target-specific config for the endpoint, of the form -C name=value. See documentation/help for your deployment target for a list of supported config options.    --endpoint <endpoint>  Required Name of the endpoint    -t, --target <target>  Required Deployment target URI. Run mlflow deployments help –target-name <target-name> for more details on the supported URI format and config options for a given target. Support is currently installed for deployment to: databricks, http, https, openai, sagemaker See all supported deployment targets and installation instructions at https://mlflow.org/docs/latest/plugins.html#community-plugins     doctor  Prints out useful information for debugging issues with MLflow. mlflow doctor [OPTIONS]   Options   --mask-envs  If set (the default behavior without setting this flag is not to obfuscate information), mask the MLflow environment variable values (e.g. “MLFLOW_ENV_VAR”: “***”) in the output to prevent leaking sensitive information.    experiments  Manage experiments. To manage experiments associated with a tracking server, set the MLFLOW_TRACKING_URI environment variable to the URL of the desired server. mlflow experiments [OPTIONS] COMMAND [ARGS]...    create  Create an experiment. All artifacts generated by runs related to this experiment will be stored under artifact location, organized under specific run_id sub-directories. Implementation of experiment and metadata store is dependent on backend storage. FileStore creates a folder for each experiment ID and stores metadata in meta.yaml. Runs are stored as subfolders. mlflow experiments create [OPTIONS]   Options   -n, --experiment-name <experiment_name>  Required    -l, --artifact-location <artifact_location>  Base location for runs to store artifact results. Artifacts will be stored at $artifact_location/$run_id/artifacts. See https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded for more info on the properties of artifact location. If no location is provided, the tracking server will pick a default.    csv  Generate CSV with all runs for an experiment mlflow experiments csv [OPTIONS]   Options   -x, --experiment-id <experiment_id>  Required    -o, --filename <filename>     delete  Mark an active experiment for deletion. This also applies to experiment’s metadata, runs and associated data, and artifacts if they are store in default location. Use list command to view artifact location. Command will throw an error if experiment is not found or already marked for deletion. Experiments marked for deletion can be restored using restore command, unless they are permanently deleted. Specific implementation of deletion is dependent on backend stores. FileStore moves experiments marked for deletion under a .trash folder under the main folder used to instantiate FileStore. Experiments marked for deletion can be permanently deleted by clearing the .trash folder. It is recommended to use a cron job or an alternate workflow mechanism to clear .trash folder. mlflow experiments delete [OPTIONS]   Options   -x, --experiment-id <experiment_id>  Required    rename  Renames an active experiment. Returns an error if the experiment is inactive. mlflow experiments rename [OPTIONS]   Options   -x, --experiment-id <experiment_id>  Required    --new-name <new_name>  Required    restore  Restore a deleted experiment. This also applies to experiment’s metadata, runs and associated data. The command throws an error if the experiment is already active, cannot be found, or permanently deleted. mlflow experiments restore [OPTIONS]   Options   -x, --experiment-id <experiment_id>  Required    search  Search for experiments in the configured tracking server. mlflow experiments search [OPTIONS]   Options   -v, --view <view>  Select view type for experiments. Valid view types are ‘active_only’ (default), ‘deleted_only’, and ‘all’.     gateway  Manage the MLflow Gateway service mlflow gateway [OPTIONS] COMMAND [ARGS]...    start  Start the MLflow Gateway service mlflow gateway start [OPTIONS]   Options   --config-path <config_path>  Required The path to the gateway configuration file.    --host <host>  The network address to listen on (default: 127.0.0.1).    --port <port>  The port to listen on (default: 5000).    --workers <workers>  The number of workers.  Environment variables   MLFLOW_GATEWAY_CONFIG  Provide a default for --config-path      gc  Permanently delete runs in the deleted lifecycle stage from the specified backend store. This command deletes all artifacts and metadata associated with the specified runs. If the provided artifact URL is invalid, the artifact deletion will be bypassed, and the gc process will continue. mlflow gc [OPTIONS]   Options   --older-than <older_than>  Optional. Remove run(s) older than the specified time limit. Specify a string in #d#h#m#s format. Float values are also supported. For example: –older-than 1d2h3m4s, –older-than 1.2d3h4m5s    --backend-store-uri <PATH>  URI of the backend store from which to delete runs. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. ‘sqlite:///path/to/file.db’) or local filesystem URIs (e.g. ‘file:///absolute/path/to/directory’). By default, data will be deleted from the ./mlruns directory.    --run-ids <run_ids>  Optional comma separated list of runs to be permanently deleted. If run ids are not specified, data is removed for all runs in the deleted lifecycle stage.    --experiment-ids <experiment_ids>  Optional comma separated list of experiments to be permanently deleted including all of their associated runs. If experiment ids are not specified, data is removed for all experiments in the deleted lifecycle stage.    models  Deploy MLflow models locally. To deploy a model associated with a run on a tracking server, set the MLFLOW_TRACKING_URI environment variable to the URL of the desired server. mlflow models [OPTIONS] COMMAND [ARGS]...    build-docker  Builds a Docker image whose default entrypoint serves an MLflow model at port 8080, using the python_function flavor. The container serves the model referenced by --model-uri, if specified when build-docker is called. If --model-uri is not specified when build_docker is called, an MLflow Model directory must be mounted as a volume into the /opt/ml/model directory in the container. Building a Docker image with --model-uri: # Build a Docker image named 'my-image-name' that serves the model from run 'some-run-uuid' # at run-relative artifact path 'my-model' mlflow models build-docker --model-uri ""runs:/some-run-uuid/my-model"" --name ""my-image-name"" # Serve the model docker run -p 5001:8080 ""my-image-name""   Building a Docker image without --model-uri: # Build a generic Docker image named 'my-image-name' mlflow models build-docker --name ""my-image-name"" # Mount the model stored in '/local/path/to/artifacts/model' and serve it docker run --rm -p 5001:8080 -v /local/path/to/artifacts/model:/opt/ml/model ""my-image-name""    Important Since MLflow 2.10.1, the Docker image built with --model-uri does not install Java for improved performance, unless the model flavor is one of [""johnsnowlabs"", ""h2o"", ""mleap"", ""spark""]. If you need to install Java for other flavors, e.g. custom Python model that uses SparkML, please specify the --install-java flag to enforce Java installation.   Warning The image built without --model-uri doesn’t support serving models with RFunc / Java MLeap model server.  NB: by default, the container will start nginx and gunicorn processes. If you don’t need the nginx process to be started (for instance if you deploy your container to Google Cloud Run), you can disable it via the DISABLE_NGINX environment variable: docker run -p 5001:8080 -e DISABLE_NGINX=true ""my-image-name""   See https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html for more information on the ‘python_function’ flavor. mlflow models build-docker [OPTIONS]   Options   -m, --model-uri <URI>  [Optional] URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -n, --name <name>  Name to use for built image    --env-manager <env_manager>  If specified, create an environment for MLmodel using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, default to virtualenv.    --mlflow-home <PATH>  Path to local clone of MLflow project. Use for development only.    --install-java  Install Java in the image. Default is False in order to reduce both the image size and the build time. Model flavors requiring Java will enable this setting automatically, such as the Spark flavor.    --install-mlflow  If specified and there is a conda or virtualenv environment to be activated mlflow will be installed into the environment after it has been activated. The version of installed mlflow will be the same as the one used to invoke this command.    --enable-mlserver  Enable serving with MLServer through the v2 inference protocol. You can use environment variables to configure MLServer. (See https://mlserver.readthedocs.io/en/latest/reference/settings.html)    generate-dockerfile  Generates a directory with Dockerfile whose default entrypoint serves an MLflow model at port 8080 using the python_function flavor. The generated Dockerfile is written to the specified output directory, along with the model (if specified). This Dockerfile defines an image that is equivalent to the one produced by mlflow models build-docker. mlflow models generate-dockerfile [OPTIONS]   Options   -m, --model-uri <URI>  [Optional] URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -d, --output-directory <output_directory>  Output directory where the generated Dockerfile is stored.    --env-manager <env_manager>  If specified, create an environment for MLmodel using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, default to virtualenv.    --mlflow-home <PATH>  Path to local clone of MLflow project. Use for development only.    --install-java  Install Java in the image. Default is False in order to reduce both the image size and the build time. Model flavors requiring Java will enable this setting automatically, such as the Spark flavor.    --install-mlflow  If specified and there is a conda or virtualenv environment to be activated mlflow will be installed into the environment after it has been activated. The version of installed mlflow will be the same as the one used to invoke this command.    --enable-mlserver  Enable serving with MLServer through the v2 inference protocol. You can use environment variables to configure MLServer. (See https://mlserver.readthedocs.io/en/latest/reference/settings.html)    predict  Generate predictions in json format using a saved MLflow model. For information about the input data formats accepted by this function, see the following documentation: https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools. mlflow models predict [OPTIONS]   Options   -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -i, --input-path <input_path>  CSV containing pandas DataFrame to predict against.    -o, --output-path <output_path>  File to output results to as json file. If not provided, output to stdout.    -t, --content-type <content_type>  Content type of the input file. Can be one of {‘json’, ‘csv’}.    --env-manager <env_manager>  If specified, create an environment for MLmodel using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, default to virtualenv.    --install-mlflow  If specified and there is a conda or virtualenv environment to be activated mlflow will be installed into the environment after it has been activated. The version of installed mlflow will be the same as the one used to invoke this command.    -r, --pip-requirements-override <pip_requirements_override>  Specify packages and versions to override the dependencies defined in the model. Must be a comma-separated string like x==y,z==a.    prepare-env  Performs any preparation necessary to predict or serve the model, for example downloading dependencies or initializing a conda environment. After preparation, calling predict or serve should be fast. mlflow models prepare-env [OPTIONS]   Options   -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    --env-manager <env_manager>  If specified, create an environment for MLmodel using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, default to virtualenv.    --install-mlflow  If specified and there is a conda or virtualenv environment to be activated mlflow will be installed into the environment after it has been activated. The version of installed mlflow will be the same as the one used to invoke this command.    serve  Serve a model saved with MLflow by launching a webserver on the specified host and port. The command supports models with the python_function or crate (R Function) flavor. For information about the input data formats accepted by the webserver, see the following documentation: https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools.  Warning Models built using MLflow 1.x will require adjustments to the endpoint request payload if executed in an environment that has MLflow 2.x installed. In 1.x, a request payload was in the format: {'columns': [str], 'data': [[...]]}. 2.x models require payloads that are defined by the structural-defining keys of either dataframe_split, instances, inputs or dataframe_records. See the examples below for demonstrations of the changes to the invocation API endpoint in 2.0.   Note Requests made in pandas DataFrame structures can be made in either split or records oriented formats. See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html for detailed information on orientation formats for converting a pandas DataFrame to json.  Example: $ mlflow models serve -m runs:/my-run-id/model-path &  # records orientation input format for serializing a pandas DataFrame $ curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{     ""dataframe_records"": [{""a"":1, ""b"":2}, {""a"":3, ""b"":4}, {""a"":5, ""b"":6}] }'  # split orientation input format for serializing a pandas DataFrame $ curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{     ""dataframe_split"": {""columns"": [""a"", ""b""],                         ""index"": [0, 1, 2],                         ""data"": [[1, 2], [3, 4], [5, 6]]} }'  # inputs format for List submission of array, tensor, or DataFrame data $ curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{     ""inputs"": [[1, 2], [3, 4], [5, 6]] }'  # instances format for submission of Tensor data curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{     ""instances"": [         {""a"": ""t1"", ""b"": [1, 2, 3]},         {""a"": ""t2"", ""b"": [4, 5, 6]},         {""a"": ""t3"", ""b"": [7, 8, 9]}     ] }'   mlflow models serve [OPTIONS]   Options   -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -p, --port <port>  The port to listen on (default: 5000).    -h, --host <HOST>  The network address to listen on (default: 127.0.0.1). Use 0.0.0.0 to bind to all addresses if you want to access the tracking server from other machines.    -t, --timeout <timeout>  Timeout in seconds to serve a request (default: 60).    -w, --workers <workers>  Number of gunicorn worker processes to handle requests (default: 1).    --env-manager <env_manager>  If specified, create an environment for MLmodel using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, default to virtualenv.    --no-conda  If specified, use local environment.    --install-mlflow  If specified and there is a conda or virtualenv environment to be activated mlflow will be installed into the environment after it has been activated. The version of installed mlflow will be the same as the one used to invoke this command.    --enable-mlserver  Enable serving with MLServer through the v2 inference protocol. You can use environment variables to configure MLServer. (See https://mlserver.readthedocs.io/en/latest/reference/settings.html)  Environment variables   MLFLOW_PORT  Provide a default for --port     MLFLOW_HOST  Provide a default for --host     MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT  Provide a default for --timeout     MLFLOW_WORKERS  Provide a default for --workers     update-pip-requirements  Add or remove requirements from a model’s conda.yaml and requirements.txt files. If using a remote tracking server, please make sure to set the MLFLOW_TRACKING_URI environment variable to the URL of the desired server. REQUIREMENT_STRINGS is a list of pip requirements specifiers. See below for examples. Sample usage: # Add requirements using the model's ""runs:/"" URI  mlflow models update-pip-requirements -m runs:/<run_id>/<model_path> \     add ""pandas==1.0.0"" ""scikit-learn"" ""mlflow >= 2.8, != 2.9.0""  # Remove requirements from a local model  mlflow models update-pip-requirements -m /path/to/local/model \     remove ""torchvision"" ""pydantic""   Note that model registry URIs (i.e. URIs in the form models:/) are not supported, as artifacts in the model registry are intended to be read-only. Editing requirements is read-only artifact repositories is also not supported. If adding requirements, the function will overwrite any existing requirements that overlap, or else append the new requirements to the existing list. If removing requirements, the function will ignore any version specifiers, and remove all the specified package names. Any requirements that are not found in the existing files will be ignored. mlflow models update-pip-requirements [OPTIONS] {add|remove}                                       [REQUIREMENT_STRINGS]...   Options   -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores  Arguments   OPERATION  Required argument    REQUIREMENT_STRINGS  Optional argument(s)     recipes  Run MLflow Recipes and inspect recipe results. mlflow recipes [OPTIONS] COMMAND [ARGS]...    clean  Remove all recipe outputs from the cache, or remove the cached outputs of a particular recipe step if specified. After cached outputs are cleaned for a particular step, the step will be re-executed in its entirety the next time it is run. mlflow recipes clean [OPTIONS]   Options   -s, --step <step>  The name of the recipe step for which to remove cached outputs.    -p, --profile <profile>  Required The name of the recipe profile to use. Profiles customize the configuration of one or more recipe steps, and recipe executions with different profiles often produce different results.  Environment variables   MLFLOW_RECIPES_PROFILE  Provide a default for --profile     get-artifact  Get the location of an artifact output from the recipe. mlflow recipes get-artifact [OPTIONS]   Options   -a, --artifact <artifact>  Required The name of the artifact to retrieve.    -p, --profile <profile>  Required The name of the recipe profile to use. Profiles customize the configuration of one or more recipe steps, and recipe executions with different profiles often produce different results.  Environment variables   MLFLOW_RECIPES_PROFILE  Provide a default for --profile     inspect  Display a visual overview of the recipe graph, or display a summary of results from a particular recipe step if specified. If the specified step has not been executed, nothing is displayed. mlflow recipes inspect [OPTIONS]   Options   -s, --step <step>  The name of the recipe step to inspect.    -p, --profile <profile>  Required The name of the recipe profile to use. Profiles customize the configuration of one or more recipe steps, and recipe executions with different profiles often produce different results.  Environment variables   MLFLOW_RECIPES_PROFILE  Provide a default for --profile     run  Run the full recipe, or run a particular recipe step if specified, producing outputs and displaying a summary of results upon completion. mlflow recipes run [OPTIONS]   Options   -s, --step <step>  The name of the recipe step to run.    -p, --profile <profile>  Required The name of the recipe profile to use. Profiles customize the configuration of one or more recipe steps, and recipe executions with different profiles often produce different results.  Environment variables   MLFLOW_RECIPES_PROFILE  Provide a default for --profile      run  Run an MLflow project from the given URI. For local runs, the run will block until it completes. Otherwise, the project will run asynchronously. If running locally (the default), the URI can be either a Git repository URI or a local path. If running on Databricks, the URI must be a Git repository. By default, Git projects run in a new working directory with the given parameters, while local projects run from the project’s root directory. mlflow run [OPTIONS] URI   Options   -e, --entry-point <NAME>  Entry point within project. [default: main]. If the entry point is not found, attempts to run the project file with the specified name as a script, using ‘python’ to run .py files and the default shell (specified by environment variable $SHELL) to run .sh files    -v, --version <VERSION>  Version of the project to run, as a Git commit reference for Git projects.    -P, --param-list <NAME=VALUE>  A parameter for the run, of the form -P name=value. Provided parameters that are not in the list of parameters for an entry point will be passed to the corresponding entry point as command-line arguments in the form –name value    -A, --docker-args <NAME=VALUE>  A docker run argument or flag, of the form -A name=value (e.g. -A gpus=all) or -A name (e.g. -A t). The argument will then be passed as docker run –name value or docker run –name respectively.    --experiment-name <experiment_name>  Name of the experiment under which to launch the run. If not specified, ‘experiment-id’ option will be used to launch run.    --experiment-id <experiment_id>  ID of the experiment under which to launch the run.    -b, --backend <BACKEND>  Execution backend to use for run. Supported values: ‘local’, ‘databricks’, kubernetes (experimental). Defaults to ‘local’. If running against Databricks, will run against a Databricks workspace determined as follows: if a Databricks tracking URI of the form ‘databricks://profile’ has been set (e.g. by setting the MLFLOW_TRACKING_URI environment variable), will run against the workspace specified by <profile>. Otherwise, runs against the workspace specified by the default Databricks CLI profile. See https://github.com/databricks/databricks-cli for more info on configuring a Databricks CLI profile.    -c, --backend-config <FILE>  Path to JSON file (must end in ‘.json’) or JSON string which will be passed as config to the backend. The exact content which should be provided is different for each execution backend and is documented at https://www.mlflow.org/docs/latest/projects.html.    --env-manager <env_manager>  If specified, create an environment for MLproject using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, the appropriate environment manager is automatically selected based on the project configuration. For example, if MLproject.yaml contains a python_env key, virtualenv is used.    --storage-dir <storage_dir>  Only valid when backend is local. MLflow downloads artifacts from distributed URIs passed to parameters of type ‘path’ to subdirectories of storage_dir.    --run-id <RUN_ID>  If specified, the given run ID will be used instead of creating a new run. Note: this argument is used internally by the MLflow project APIs and should not be specified.    --run-name <RUN_NAME>  The name to give the MLflow Run associated with the project execution. If not specified, the MLflow Run name is left unset.    --build-image  Only valid for Docker projects. If specified, build a new Docker image that’s based on the image specified by the image field in the MLproject file, and contains files in the project directory.  Default False    Arguments   URI  Required argument  Environment variables   MLFLOW_EXPERIMENT_NAME  Provide a default for --experiment-name     MLFLOW_EXPERIMENT_ID  Provide a default for --experiment-id     MLFLOW_TMP_DIR  Provide a default for --storage-dir     runs  Manage runs. To manage runs of experiments associated with a tracking server, set the MLFLOW_TRACKING_URI environment variable to the URL of the desired server. mlflow runs [OPTIONS] COMMAND [ARGS]...    delete  Mark a run for deletion. Return an error if the run does not exist or is already marked. You can restore a marked run with restore_run, or permanently delete a run in the backend store. mlflow runs delete [OPTIONS]   Options   --run-id <run_id>  Required    describe  All of run details will print to the stdout as JSON format. mlflow runs describe [OPTIONS]   Options   --run-id <run_id>  Required    list  List all runs of the specified experiment in the configured tracking server. mlflow runs list [OPTIONS]   Options   --experiment-id <experiment_id>  Required Specify the experiment ID for list of runs.    -v, --view <view>  Select view type for list experiments. Valid view types are ‘active_only’ (default), ‘deleted_only’, and ‘all’.  Environment variables   MLFLOW_EXPERIMENT_ID  Provide a default for --experiment-id     restore  Restore a deleted run. Returns an error if the run is active or has been permanently deleted. mlflow runs restore [OPTIONS]   Options   --run-id <run_id>  Required     sagemaker  Serve models on SageMaker. To serve a model associated with a run on a tracking server, set the MLFLOW_TRACKING_URI environment variable to the URL of the desired server. mlflow sagemaker [OPTIONS] COMMAND [ARGS]...    build-and-push-container  Build new MLflow Sagemaker image, assign it a name, and push to ECR. This function builds an MLflow Docker image. The image is built locally and it requires Docker to run. The image is pushed to ECR under current active AWS account and to current active AWS region. mlflow sagemaker build-and-push-container [OPTIONS]   Options   --build, --no-build  Build the container if set.    --push, --no-push  Push the container to AWS ECR if set.    -c, --container <container>  image name    --env-manager <env_manager>  If specified, create an environment for MLmodel using the specified environment manager. The following values are supported:  - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda  If unspecified, default to virtualenv.    --mlflow-home <PATH>  Path to local clone of MLflow project. Use for development only.    deploy-transform-job  Deploy model on Sagemaker as a batch transform job. Current active AWS account needs to have correct permissions setup. By default, unless the --async flag is specified, this command will block until either the batch transform job completes (definitively succeeds or fails) or the specified timeout elapses. mlflow sagemaker deploy-transform-job [OPTIONS]   Options   -n, --job-name <job_name>  Required Transform job name    -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    --input-data-type <input_data_type>  Required Input data type for the transform job    -u, --input-uri <input_uri>  Required S3 key name prefix or manifest of the input data    --content-type <content_type>  Required The multipurpose internet mail extension (MIME) type of the data    -o, --output-path <output_path>  Required The S3 path to store the output results of the Sagemaker transform job    --compression-type <compression_type>  The compression type of the transform data    -s, --split-type <split_type>  The method to split the transform job’s data files into smaller batches    -a, --accept <accept>  The multipurpose internet mail extension (MIME) type of the output data    --assemble-with <assemble_with>  The method to assemble the results of the transform job as a single S3 object    --input-filter <input_filter>  A JSONPath expression used to select a portion of the input data for the transform job    --output-filter <output_filter>  A JSONPath expression used to select a portion of the output data from the transform job    -j, --join-resource <join_resource>  The source of the data to join with the transformed data    -e, --execution-role-arn <execution_role_arn>  SageMaker execution role    -b, --bucket <bucket>  S3 bucket to store model artifacts    -i, --image-url <image_url>  ECR URL for the Docker image    --region-name <region_name>  Name of the AWS region in which to deploy the transform job    -t, --instance-type <instance_type>  The type of SageMaker ML instance on which to perform the batch transform job. For a list of supported instance types, see https://aws.amazon.com/sagemaker/pricing/instance-types/.    -c, --instance-count <instance_count>  The number of SageMaker ML instances on which to perform the batch transform job    -v, --vpc-config <vpc_config>  Path to a file containing a JSON-formatted VPC configuration. This configuration will be used when creating the new SageMaker model associated with this application. For more information, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_VpcConfig.html    -f, --flavor <flavor>  The name of the flavor to use for deployment. Must be one of the following: [‘python_function’, ‘mleap’]. If unspecified, a flavor will be automatically selected from the model’s available flavors.    --archive  If specified, any SageMaker resources that become inactive after the finished batch transform job are preserved. These resources may include the associated SageMaker models and model artifacts. Otherwise, if –archive is unspecified, these resources are deleted. –archive must be specified when deploying asynchronously with –async.    --async  If specified, this command will return immediately after starting the deployment process. It will not wait for the deployment process to complete. The caller is responsible for monitoring the deployment process via native SageMaker APIs or the AWS console.    --timeout <timeout>  If the command is executed synchronously, the deployment process will return after the specified number of seconds if no definitive result (success or failure) is achieved. Once the function returns, the caller is responsible for monitoring the health and status of the pending deployment via native SageMaker APIs or the AWS console. If the command is executed asynchronously using the –async flag, this value is ignored.    push-model  Push an MLflow model to Sagemaker model registry. Current active AWS account needs to have correct permissions setup. mlflow sagemaker push-model [OPTIONS]   Options   -n, --model-name <model_name>  Required Sagemaker model name    -m, --model-uri <URI>  Required URI to the model. A local path, a ‘runs:/’ URI, or a remote storage URI (e.g., an ‘s3://’ URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores    -e, --execution-role-arn <execution_role_arn>  SageMaker execution role    -b, --bucket <bucket>  S3 bucket to store model artifacts    -i, --image-url <image_url>  ECR URL for the Docker image    --region-name <region_name>  Name of the AWS region in which to push the Sagemaker model    -v, --vpc-config <vpc_config>  Path to a file containing a JSON-formatted VPC configuration. This configuration will be used when creating the new SageMaker model. For more information, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_VpcConfig.html    -f, --flavor <flavor>  The name of the flavor to use for deployment. Must be one of the following: [‘python_function’, ‘mleap’]. If unspecified, a flavor will be automatically selected from the model’s available flavors.    terminate-transform-job  Terminate the specified Sagemaker batch transform job. Unless --archive is specified, all SageMaker resources associated with the batch transform job are deleted as well. By default, unless the --async flag is specified, this command will block until either the termination process completes (definitively succeeds or fails) or the specified timeout elapses. mlflow sagemaker terminate-transform-job [OPTIONS]   Options   -n, --job-name <job_name>  Required Transform job name    -r, --region-name <region_name>  Name of the AWS region in which the transform job is deployed    --archive  If specified, resources associated with the application are preserved. These resources may include unused SageMaker models and model artifacts. Otherwise, if –archive is unspecified, these resources are deleted. –archive must be specified when deleting asynchronously with –async.    --async  If specified, this command will return immediately after starting the termination process. It will not wait for the termination process to complete. The caller is responsible for monitoring the termination process via native SageMaker APIs or the AWS console.    --timeout <timeout>  If the command is executed synchronously, the termination process will return after the specified number of seconds if no definitive result (success or failure) is achieved. Once the function returns, the caller is responsible for monitoring the health and status of the pending termination via native SageMaker APIs or the AWS console. If the command is executed asynchronously using the –async flag, this value is ignored.     server  Run the MLflow tracking server. The server listens on http://localhost:5000 by default and only accepts connections from the local machine. To let the server accept connections from other machines, you will need to pass --host 0.0.0.0 to listen on all network interfaces (or a specific interface address). mlflow server [OPTIONS]   Options   --backend-store-uri <PATH>  URI to which to persist experiment and run data. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. ‘sqlite:///path/to/file.db’) or local filesystem URIs (e.g. ‘file:///absolute/path/to/directory’). By default, data will be logged to the ./mlruns directory.    --registry-store-uri <URI>  URI to which to persist registered models. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. ‘sqlite:///path/to/file.db’). If not specified, backend-store-uri is used.    --default-artifact-root <URI>  Directory in which to store artifacts for any new experiments created. For tracking server backends that rely on SQL, this option is required in order to store artifacts. Note that this flag does not impact already-created experiments with any previous configuration of an MLflow server instance. By default, data will be logged to the mlflow-artifacts:/ uri proxy if the –serve-artifacts option is enabled. Otherwise, the default location will be ./mlruns.    --serve-artifacts, --no-serve-artifacts  Enables serving of artifact uploads, downloads, and list requests by routing these requests to the storage location that is specified by ‘–artifacts-destination’ directly through a proxy. The default location that these requests are served from is a local ‘./mlartifacts’ directory which can be overridden via the ‘–artifacts-destination’ argument. To disable artifact serving, specify –no-serve-artifacts. Default: True    --artifacts-only  If specified, configures the mlflow server to be used only for proxied artifact serving. With this mode enabled, functionality of the mlflow tracking service (e.g. run creation, metric logging, and parameter logging) is disabled. The server will only expose endpoints for uploading, downloading, and listing artifacts. Default: False    --artifacts-destination <URI>  The base artifact location from which to resolve artifact upload/download/list requests (e.g. ‘s3://my-bucket’). Defaults to a local ‘./mlartifacts’ directory. This option only applies when the tracking server is configured to stream artifacts and the experiment’s artifact root location is http or mlflow-artifacts URI.    -h, --host <HOST>  The network address to listen on (default: 127.0.0.1). Use 0.0.0.0 to bind to all addresses if you want to access the tracking server from other machines.    -p, --port <port>  The port to listen on (default: 5000).    -w, --workers <workers>  Number of gunicorn worker processes to handle requests (default: 1).    --static-prefix <static_prefix>  A prefix which will be prepended to the path of all static paths.    --gunicorn-opts <gunicorn_opts>  Additional command line options forwarded to gunicorn processes.    --waitress-opts <waitress_opts>  Additional command line options for waitress-serve.    --expose-prometheus <expose_prometheus>  Path to the directory where metrics will be stored. If the directory doesn’t exist, it will be created. Activate prometheus exporter to expose metrics on /metrics endpoint.    --app-name <app_name>  Application name to be used for the tracking server. If not specified, ‘mlflow.server:app’ will be used.  Options basic-auth | basic-auth      --dev  If enabled, run the server with debug logging and auto-reload. Should only be used for development purposes. Cannot be used with ‘–gunicorn-opts’. Unsupported on Windows.  Default False    Environment variables   MLFLOW_BACKEND_STORE_URI  Provide a default for --backend-store-uri     MLFLOW_REGISTRY_STORE_URI  Provide a default for --registry-store-uri     MLFLOW_DEFAULT_ARTIFACT_ROOT  Provide a default for --default-artifact-root     MLFLOW_SERVE_ARTIFACTS  Provide a default for --serve-artifacts     MLFLOW_ARTIFACTS_ONLY  Provide a default for --artifacts-only     MLFLOW_ARTIFACTS_DESTINATION  Provide a default for --artifacts-destination     MLFLOW_HOST  Provide a default for --host     MLFLOW_PORT  Provide a default for --port     MLFLOW_WORKERS  Provide a default for --workers     MLFLOW_STATIC_PREFIX  Provide a default for --static-prefix     MLFLOW_GUNICORN_OPTS  Provide a default for --gunicorn-opts     MLFLOW_EXPOSE_PROMETHEUS  Provide a default for --expose-prometheus           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
search-runs.html,"   Documentation  Search Runs       Search Runs  This guide will walk you through how to search your MLflow runs through the MLflow UI and Python API. This resource will be valuable if you’re interested in querying specific runs based on their metrics, params, tags, dataset information, or run metadata. In short, you can leverage SQL-like syntax to filter your runs based on a variety of conditions. Note that the OR keyword is not supported and there are a few other differences from SQL mentioned below, but despite these limitations, the run search functionality is quite powerful.  Search Runs on MLflow UI  The MLflow UI provides a powerful search interface that allows you to filter runs. Below we’ll…  Create example MLflow runs Look at a simple querying example Deep dive into query syntax Provide a variety of example queries   Create Example MLflow Runs  First, let’s create some example MLflow runs. This documentation is based on experiments created with the below script. If you don’t want to interactively explore this on your machine, skip this section. Before running the script, let’s simply start the MLflow UI on a local host. mlflow ui   Let’s visit http://localhost:5000/ in our web browser. After doing so, you’ll notice that we don’t have any experiments or models. Let’s resolve this by creating a few MLflow runs via the script below. Note that when you run this script, you’ll want to do so from the same directory that you ran the mlflow ui command. import mlflow import numpy as np  mlflow.set_experiment(""search-run-guide"")  accuracy = np.arange(0, 1, 0.1) loss = np.arange(1, 0, -0.1) log_scale_loss = np.log(loss) f1_score = np.arange(0, 1, 0.1)  batch_size = [2] * 5 + [4] * 5 learning_rate = [0.001, 0.01] * 5 model = [""GPT-2"", ""GPT-3"", ""GPT-3.5"", ""GPT-4""] + [None] * 6  task = [""classification"", ""regression"", ""causal lm""] + [None] * 7 environment = [""notebook""] * 5 + [None] * 5  dataset_name = [""custom""] * 5 + [""also custom""] * 5 dataset_digest = [""s8ds293b"", ""jks834s2""] + [None] * 8 dataset_context = [""train""] * 5 + [""test""] * 5  for i in range(10):     with mlflow.start_run():         mlflow.log_metrics(             {                 ""loss"": loss[i],                 ""accuracy"": accuracy[i],                 ""log-scale-loss"": log_scale_loss[i],                 ""f1 score"": f1_score[i],             }         )          mlflow.log_params(             {                 ""batch_size"": batch_size[i],                 ""learning rate"": learning_rate[i],                 ""model"": model[i],             }         )          mlflow.set_tags(             {                 ""task"": task[i],                 ""environment"": environment[i],             }         )          dataset = mlflow.data.from_numpy(             features=np.random.uniform(size=[20, 28, 28, 3]),             targets=np.random.randint(0, 10, size=[20]),             name=dataset_name[i],             digest=dataset_digest[i],         )         mlflow.log_input(dataset, context=dataset_context[i])   The code above creates 10 MLflow runs with different metrics, params, tags and dataset information. After successful execution, if you return to the MLflow UI in your browser, you should find all of these runs under the experiment “search-run-guide”, as shown by the following screenshot:    In real-world production deployments of MLflow, it’s common to have thousands or even hundreds of thousands of runs. In such cases, it’s important to be able to filter and search for runs based on specific criteria.   Search Query Example  In order to filter your MLflow runs, you will need to write search queries, which are pseudo-SQL conditions expressed in a distinct syntax. To showcase this functionality, let’s look at the below code examples. import mlflow  all_runs = mlflow.search_runs(search_all_experiments=True) print(all_runs)    Output                               run_id  ... tags.mlflow.user 0  5984a3488161440f92de9847e846b342  ...     michael.berk 1  41160f238a5841998dda263794b26067  ...     michael.berk 2  babe221a676b4fa4b204f8240f2c4f14  ...     michael.berk 3  45eb4f02c5a1461aa6098fa550233be6  ...     michael.berk 4  1c7c459486c44b23bb016028aee1f153  ...     michael.berk 5  4453f59f1ab04491bb9582d8cba5f437  ...     michael.berk 6  22db81f070f6413588641c8c343cdd72  ...     michael.berk 7  c3680e37d0fa44eb9c9fb7828f6b5481  ...     michael.berk 8  67973142b9c0470d8d764ada07c5a988  ...     michael.berk 9  59853d5f17f946218f63de1dc82de07b  ...     michael.berk  [10 rows x 19 columns]    Second, let’s try filtering the runs for our really bad models: metrics.loss > 0.8. import mlflow  bad_runs = mlflow.search_runs(     filter_string=""metrics.loss > 0.8"", search_all_experiments=True ) print(bad_runs)    Output                               run_id  ... tags.mlflow.source.name 0  67973142b9c0470d8d764ada07c5a988  ...               delete.py 1  59853d5f17f946218f63de1dc82de07b  ...               delete.py  [2 rows x 19 columns]    You’ll notice that we now are displaying 2 runs instead of 10. Pretty easy, right? Now let’s go over the search query syntax in more detail.   Search Query Syntax Deep Dive  As noted above, MLflow search syntax is similar to SQL with a few notable exceptions.  The SQL OR keyword is not supported. For fields that contain special characters or start with numbers, you need to wrap them in double quotes. - Bad:  metrics.cross-entropy-loss < 0.5 + Good: metrics.""cross-entropy-loss"" < 0.5  - Bad:  params.1st_iteration_timestamp = ""2022-01-01"" + Good: params.""1st_iteration_timestamp"" = ""2022-01-01""    For the SQL IN keyword, you must surround the values of your list with single quotes. - Bad:  attributes.run_id IN (""5984a3488161440f92de9847e846b342"", ""babe221a676b4fa4b204f8240f2c4f14"") + Good: attributes.run_id IN ('5984a3488161440f92de9847e846b342', 'babe221a676b4fa4b204f8240f2c4f14')    For the SQL IN keyword, you can only search the following fields:  datasets.name datasets.digest datasets.context attributes.run_id   Non-None conditions for numeric fields are not supported e.g. metrics.accuracy != ""None"" will fail.  Other than the that, the syntax should be intuitive to anyone who has used SQL. To assemble a single search condition, you must assemble an inequality using the following components…  An MLflow field: a metric, param, tag, dataset or run metadata. A comparator: an inequality operator.    For numerics, MLflow supports =, !=, >, >=, <, and <=. Examples include: metrics.accuracy > 0.72 metrics.loss <= 0.15 metrics.accuracy != 0.15    For strings, MLflow supports =, !=, LIKE (case-sensitive) and ILIKE (case-insensitive). Examples include: params.model = ""GPT-3"" params.model LIKE ""GPT%"" params.model ILIKE ""gpt%""    For sets, MLflow supports IN. Examples include: datasets.name IN ('custom', 'also custom', 'another custom name') datasets.digest IN ('s8ds293b', 'jks834s2') attributes.run_id IN ('5984a3488161440f92de9847e846b342')       A reference value: a numeric value, string, or set of strings.  Let’s look at some more examples.   Example Queries  In this section we will go over how to search by different categories of MLflow fields. For each category we provide a few sample queries. If you have executed the run creation script we provided, these queries should fetch certain runs but sometimes require modification for run-specific information, such as start_time.  1 - Searching By Metrics  Metrics are quantitative measures typically used to evaluate the model’s performance during or after training. Metrics can include values like accuracy, precision, recall, F1 score, etc., and can change over time as the model trains. They are logged manually via mlflow.log_metric or mlflow.log_metrics or automatically via autologging. To search for runs by filtering on metrics, you must include the metrics prefix in the left side of the inequality. Note that they are stored as numbers, so you must use numeric comparators. metrics.accuracy > 0.72 metrics.""accuracy"" > 0.72 metrics.loss <= 0.15 metrics.""log-scale-loss"" <= 0 metrics.""f1 score"" >= 0.5 metrics.accuracy > 0.72 AND metrics.loss <= 0.15     2 - Searching By Params  Params are strings that typically represent the configuration aspects of the model. Parameters can include values like learning rate, batch size, and number of epochs. They are logged manually via mlflow.log_param or mlflow.log_params or automatically via autologging. To search for runs by filtering on params, you must include the params prefix in the left side of the inequality. Note that they are stored as strings, so you must use string comparators, such as = and !=. params.batch_size = ""2"" params.model LIKE ""GPT%"" params.model ILIKE ""gPt%"" params.model LIKE ""GPT%"" AND params.batch_size = ""2""     3 - Searching By Tags  Tags are metadata that typically provide additional context about the run. Tags can include values like user name, team, etc. They are logged manually via mlflow.set_tag or mlflow.set_tags. In addition, system tags, such as mlflow.user, are automatically logged. To search for runs by filtering on tags, you must include the tags or mlflow prefixes in the left side of the inequality. Note that tags are stored as strings, so you must use string comparators, such as = and !=. tags.""environment"" = ""notebook"" tags.environment = ""notebook"" tags.task = ""Classification"" tags.task ILIKE ""classif%""     4 - Searching By Dataset Information  Datasets represent data used in model training or evaluation, including features, targets, predictions, and metadata such as the dataset’s name, digest (hash) schema, profile, and source. They are logged via mlflow.log_input or automatically via autologging. To search for runs by filtering on dataset information, you must filter on one of the below fields  datasets.name, which is the dataset’s name. datasets.digest, which is a unique identifier for the dataset. datasets.context, which represents if the dataset is used for train, evaluation or test.  Note that dataset information is stored as strings, so you must use string comparators, such as = and !=. Also note that datasets support set comparators, such as IN. datasets.name LIKE ""custom"" datasets.digest IN ('s8ds293b', 'jks834s2') datasets.context = ""train""     5 - Searching By Run’s Metadata  Run metadata are a variety of user-specified and system-generated attributes that provide additional context about the run. To search for runs by filtering on the metadata of runs, you must include the attributes prefix in the left side of the inequality. Note that run metadata can be either a string or a numeric depending on the attribute, so you must use the appropriate comparator. For a complete list of attributes, see mlflow.entities.RunInfo, however note that not all fields in the RunInfo object are searchable. To search for runs by filtering on tags, you must include the tags or mlflow prefixes in the left side of the inequality. Note that tags are stored as strings, so you must use string comparators, such as = and !=.  Examples for Strings  attributes.status = 'ACTIVE' attributes.user_id LIKE 'user1' attributes.run_name = 'my-run' attributes.run_id = 'a1b2c3d4' attributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')     Examples for Numerics  attributes.start_time >= 1664067852747 attributes.end_time < 1664067852747 attributes.created > 1664067852747      6 - Searching over a Set  You can search for runs by filtering on a set of acceptable values via the IN keyword. As noted above, this is only supported for the following fields:  datasets.{any_attribute} attributes.run_id  datasets.name IN ('custom', 'also custom') datasets.digest IN ('s8ds293b', 'jks834s2') attributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')     7 - Chained Queries  You can chain multiple queries together using the AND keyword. For example, to search for runs with a variety of conditions, you can use the following queries: metrics.accuracy > 0.72 AND metrics.loss <= 0.15 metrics.accuracy > 0.72 AND metrics.batch_size != 0 metrics.accuracy > 0.72 AND metrics.batch_size != 0 AND attributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')   You can also apply multiple conditions on the same field, for example searching for all loss metrics BETWEEEN 0.1 and 0.15, inclusive: metrics.loss <= 0.15 AND metrics.loss >= 0.1   Finally, before moving on it’s important to revisit that that you cannot use the OR keyword in your queries.   8 - Non-None Queries  To search for runs where a field (only type string is supported) is not null, use the field != ""None"" syntax. For example, to search for runs where the batch_size is not null, you can use the following query: params.batch_size != ""None""       Programmatically Searching Runs  When scaling out to large production systems, typically you’ll want to interact with your runs outside the MLflow UI. This can be done programmatically using the MLflow client APIs.  Python  mlflow.client.MlflowClient.search_runs() or mlflow.search_runs() take the same arguments as the above UI examples and more! They return all the runs that match the specified filters. Your best resource is the dosctrings for each of these functions, but here are some useful examples.  1 - Complex Filter  Python provides powerful ways to build these queries programmatically. Some tips:  For complex filters, specifically those with both single and double quotes, use multi-line strings or \” to escape the quotes. When working with lists, use the .join() method to concatenate the list elements with a delimiter. It’s often most concise to use the fluent APIs, so below we demo only with the fluent API.  import mlflow  run_ids = [""22db81f070f6413588641c8c343cdd72"", ""c3680e37d0fa44eb9c9fb7828f6b5481""] run_id_condition = ""'"" + ""','"".join(run_ids) + ""'""  complex_filter = f"""""" attributes.run_id IN ({run_id_condition})   AND metrics.loss > 0.3   AND metrics.""f1 score"" < 0.5   AND params.model LIKE ""GPT%"" """"""  runs_with_complex_filter = mlflow.search_runs(     experiment_names=[""search-run-guide""],     filter_string=complex_filter, ) print(runs_with_complex_filter)   The output will be a pandas DataFrame with the runs that match the specified filters, as shown below.                               run_id  ... tags.mlflow.runName 0  22db81f070f6413588641c8c343cdd72  ...   orderly-quail-568 1  c3680e37d0fa44eb9c9fb7828f6b5481  ...    melodic-lynx-301  [2 rows x 19 columns]     2 - run_view_type  The run_view_type parameter exposes additional filtering options, as noted in the mlflow.entities.ViewType enum. For example, if you want to filter only active runs, which is a dropdown in the UI, simply pass run_view_type=ViewType.ACTIVE_ONLY. import mlflow from mlflow.entities import ViewType  active_runs = mlflow.search_runs(     experiment_names=[""search-run-guide""],     run_view_type=ViewType.ACTIVE_ONLY,     order_by=[""metrics.accuracy DESC""], )     2 - Ordering  Another useful feature that is available in the search API is allowing for ordering of the returned search results. You can specify a list of columns of interest along with DESC or ASC in the order_by kwarg. Note that the DESC or ASC value is optional, so when the value is not provided, the default is ASC. Also note that the default ordering when the order_by parameter is omitted is to sort by start_time DESC, then run_id ASC. import mlflow from mlflow.entities import ViewType  active_runs_ordered_by_accuracy = mlflow.search_runs(     experiment_names=[""search-run-guide""],     run_view_type=ViewType.ACTIVE_ONLY,     order_by=[""metrics.accuracy DESC""], )   A common use case is getting the top n results, for example, the top 5 runs by accuracy. When combined with the max_results parameter, you can get the top n that match your query. import mlflow from mlflow.entities import ViewType  highest_accuracy_run = mlflow.search_runs(     experiment_names=[""search-run-guide""],     run_view_type=ViewType.ACTIVE_ONLY,     max_results=1,     order_by=[""metrics.accuracy DESC""], )[0]     3 - Searching All Experiments  Now you might be wondering how to search all experiments. It’s as simple as specifying search_all_experiments=True and omitting the experiment_ids parameter. import mlflow from mlflow.entities import ViewType  model_of_interest = ""GPT-4"" gpt_4_runs_global = mlflow.search_runs(     filter_string=f""params.model = '{model_of_interest}'"",     run_view_type=ViewType.ALL,     search_all_experiments=True, )   Finally, there are additioanl useful features in the mlflow.client.MlflowClient.search_runs() or mlflow.search_runs() methods, so be sure to check out the documentation for more details.    R  The R API is similar to the Python API. library(mlflow) mlflow_search_runs(   filter = ""metrics.rmse < 0.9 and tags.production = 'true'"",   experiment_ids = as.character(1:2),   order_by = ""params.lr DESC"" )     Java  The Java API is similar to Python API. List<Long> experimentIds = Arrays.asList(""1"", ""2"", ""4"", ""8""); List<RunInfo> searchResult = client.searchRuns(experimentIds, ""metrics.accuracy_score < 99.90"");           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
search-experiments.html,"   Documentation  Search Experiments       Search Experiments  mlflow.search_experiments() and MlflowClient.search_experiments() support the same filter string syntax as mlflow.search_runs() and MlflowClient.search_runs(), but the supported identifiers and comparators are different.  Table of Contents  Syntax  Identifier Comparator Examples      Syntax  See Search Runs Syntax for more information.  Identifier  The following identifiers are supported:  attributes.name: Experiment name attributes.creation_time: Experiment creation time attributes.last_update_time: Experiment last update time   Note attributes can be omitted. name is equivalent to attributes.name.    tags.<tag key>: Tag    Comparator  Comparators for string attributes and tags:  =: Equal !=: Not equal LIKE: Case-sensitive pattern match ILIKE: Case-insensitive pattern match  Comparators for numeric attributes:  =: Equal !=: Not equal <: Less than <=: Less than or equal to >: Greater than >=: Greater than or equal to    Examples  # Matches experiments with name equal to 'x' ""attributes.name = 'x'""  # or ""name = 'x'""  # Matches experiments with name starting with 'x' ""attributes.name LIKE 'x%'""  # Matches experiments with 'group' tag value not equal to 'x' ""tags.group != 'x'""  # Matches experiments with 'group' tag value containing 'x' or 'X' ""tags.group ILIKE '%x%'""  # Matches experiments with name starting with 'x' and 'group' tag value equal to 'y' ""attributes.name LIKE 'x%' AND tags.group = 'y'""           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
python_api/index.html,"   Documentation  Python API       Python API  The MLflow Python API is organized into the following modules. The most common functions are exposed in the mlflow module, so we recommend starting there.   mlflow mlflow.artifacts mlflow.catboost mlflow.client mlflow.config mlflow.data mlflow.deployments mlflow.diviner mlflow.entities mlflow.environment_variables mlflow.fastai mlflow.gateway mlflow.gluon mlflow.h2o mlflow.johnsnowlabs mlflow.keras mlflow.langchain mlflow.lightgbm mlflow.metrics mlflow.mleap mlflow.models mlflow.onnx mlflow.paddle mlflow.pmdarima mlflow.projects mlflow.prophet mlflow.pyfunc mlflow.pyspark.ml mlflow.pytorch mlflow.recipes mlflow.sagemaker mlflow.sentence_transformers mlflow.server mlflow.shap mlflow.sklearn mlflow.spacy mlflow.spark mlflow.statsmodels mlflow.system_metrics mlflow.tensorflow mlflow.transformers mlflow.types mlflow.utils mlflow.xgboost mlflow.openai   See also the index of all functions and classes.  Log Levels  MLflow Python APIs log information during execution using the Python Logging API. You can configure the log level for MLflow logs using the following code snippet. Learn more about Python log levels at the Python language logging guide. import logging  logger = logging.getLogger(""mlflow"")  # Set log level to debugging logger.setLevel(logging.DEBUG)          Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
R-api.html,"   Documentation  R API       R API  The MLflow R API allows you to use MLflow Tracking, Projects and Models.  Prerequisites  To use the MLflow R API, you must install the MLflow Python package. pip install mlflow   Installing with an Available Conda Environment example: conda create -n mlflow-env python conda activate mlflow-env pip install mlflow   The above provided commands create a new Conda environment named mlflow-env, specifying the default Python version. It then activates this environment, making it the active working environment. Finally, it installs the MLflow package using pip, ensuring that MLflow is isolated within this environment, allowing for independent Python and package management for MLflow-related tasks. Optionally, you can set the MLFLOW_PYTHON_BIN and MLFLOW_BIN environment variables to specify the Python and MLflow binaries to use. By default, the R client automatically finds them using Sys.which('python') and Sys.which('mlflow'). export MLFLOW_PYTHON_BIN=/path/to/bin/python export MLFLOW_BIN=/path/to/bin/mlflow   You can use the R API to start the user interface, create experiment and search experiments, save models, run projects and serve models among many other functions available in the R API.   build_context_tags_from_databricks_job_info  Get information from a Databricks job execution context Parses the data from a job execution context when running on Databricks in a non-interactive mode. This function extracts relevant data that MLflow needs in order to properly utilize the MLflow APIs from this context. build_context_tags_from_databricks_job_info(job_info)    Arguments        Argument Description    job_info The job-related metadata from a running Databricks job      Value  A list of tags to be set by the run context when creating MLflow runs in the current Databricks Job environment    build_context_tags_from_databricks_notebook_info  Get information from Databricks Notebook environment Retrieves the notebook id, path, url, name, version, and type from the Databricks Notebook execution environment and sets them to a list to be used for setting the configured environment for executing an MLflow run in R from Databricks. build_context_tags_from_databricks_notebook_info(notebook_info)    Arguments        Argument Description    notebook_info The configuration data from the Databricks Notebook environment      Value  A list of tags to be set by the run context when creating MLflow runs in the current Databricks Notebook environment    mlflow_client  Initialize an MLflow Client Initializes and returns an MLflow client that communicates with the tracking server or store at the specified URI. mlflow_client(tracking_uri = NULL)    Arguments        Argument Description    tracking_uri The tracking URI. If not provided, defaults to the service set by mlflow_set_tracking_uri().       mlflow_create_experiment  Create Experiment Creates an MLflow experiment and returns its id. mlflow_create_experiment(   name,   artifact_location = NULL,   client = NULL,   tags = NULL )    Arguments        Argument Description    name The name of the experiment to create.  artifact_location Location where all artifacts for this experiment are stored. If not provided, the remote server will select an appropriate default.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.  tags Experiment tags to set on the experiment upon experiment creation.       mlflow_create_model_version  Create a model version Create a model version mlflow_create_model_version(   name,   source,   run_id = NULL,   tags = NULL,   run_link = NULL,   description = NULL,   client = NULL )    Arguments        Argument Description    name Register model under this name.  source URI indicating the location of the model artifacts.  run_id MLflow run ID for correlation, if source was generated by an experiment run in MLflow Tracking.  tags Additional metadata.  run_link MLflow run link - This is the exact link of the run that generated this model version.  description Description for model version.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_create_registered_model  Create registered model Creates a new registered model in the model registry mlflow_create_registered_model(   name,   tags = NULL,   description = NULL,   client = NULL )    Arguments        Argument Description    name The name of the model to create.  tags Additional metadata for the registered model (Optional).  description Description for the registered model (Optional).  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_delete_experiment  Delete Experiment Marks an experiment and associated runs, params, metrics, etc. for deletion. If the experiment uses FileStore, artifacts associated with experiment are also deleted. mlflow_delete_experiment(experiment_id, client = NULL)    Arguments        Argument Description    experiment_id ID of the associated experiment. This field is required.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_delete_model_version  Delete a model version Delete a model version mlflow_delete_model_version(name, version, client = NULL)    Arguments        Argument Description    name Name of the registered model.  version Model version number.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_delete_registered_model  Delete registered model Deletes an existing registered model by name mlflow_delete_registered_model(name, client = NULL)    Arguments        Argument Description    name The name of the model to delete  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_delete_run  Delete a Run Deletes the run with the specified ID. mlflow_delete_run(run_id, client = NULL)    Arguments        Argument Description    run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_delete_tag  Delete Tag Deletes a tag on a run. This is irreversible. Tags are run metadata that can be updated during a run and after a run completes. mlflow_delete_tag(key, run_id = NULL, client = NULL)    Arguments        Argument Description    key Name of the tag. Maximum size is 255 bytes. This field is required.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_download_artifacts  Download Artifacts Download an artifact file or directory from a run to a local directory if applicable, and return a local path for it. mlflow_download_artifacts(path, run_id = NULL, client = NULL)    Arguments        Argument Description    path Relative source path to the desired artifact.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_end_run  End a Run Terminates a run. Attempts to end the current active run if run_id is not specified. mlflow_end_run(   status = c(""FINISHED"", ""FAILED"", ""KILLED""),   end_time = NULL,   run_id = NULL,   client = NULL )    Arguments        Argument Description    status Updated status of the run. Defaults to FINISHED. Can also be set to “FAILED” or “KILLED”.  end_time Unix timestamp of when the run ended in milliseconds.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_experiment  Get Experiment Gets metadata for an experiment and a list of runs for the experiment. Attempts to obtain the active experiment if both experiment_id and name are unspecified. mlflow_get_experiment(experiment_id = NULL, name = NULL, client = NULL)    Arguments        Argument Description    experiment_id ID of the experiment.  name The experiment name. Only one of name or experiment_id should be specified.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_latest_versions  Get latest model versions Retrieves a list of the latest model versions for a given model. mlflow_get_latest_versions(name, stages = list(), client = NULL)    Arguments        Argument Description    name Name of the model.  stages A list of desired stages. If the input list is NULL, return latest versions for ALL_STAGES.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_metric_history  Get Metric History Get a list of all values for the specified metric for a given run. mlflow_get_metric_history(metric_key, run_id = NULL, client = NULL)    Arguments        Argument Description    metric_key Name of the metric.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_model_version  Get a model version Get a model version mlflow_get_model_version(name, version, client = NULL)    Arguments        Argument Description    name Name of the registered model.  version Model version number.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_registered_model  Get a registered model Retrieves a registered model from the Model Registry. mlflow_get_registered_model(name, client = NULL)    Arguments        Argument Description    name The name of the model to retrieve.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_run  Get Run Gets metadata, params, tags, and metrics for a run. Returns a single value for each metric key: the most recently logged metric value at the largest step. mlflow_get_run(run_id = NULL, client = NULL)    Arguments        Argument Description    run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_get_tracking_uri  Get Remote Tracking URI Gets the remote tracking URI. mlflow_get_tracking_uri()     mlflow_id  Get Run or Experiment ID Extracts the ID of the run or experiment. mlflow_id(object) list(list(""mlflow_id""), list(""mlflow_run""))(object) list(list(""mlflow_id""), list(""mlflow_experiment""))(object)    Arguments        Argument Description    object An mlflow_run or mlflow_experiment object.       mlflow_list_artifacts  List Artifacts Gets a list of artifacts. mlflow_list_artifacts(path = NULL, run_id = NULL, client = NULL)    Arguments        Argument Description    path The run’s relative artifact path to list from. If not specified, it is set to the root artifact path  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_load_flavor  Load MLflow Model Flavor Loads an MLflow model using a specific flavor. This method is called internally by mlflow_load_model , but is exposed for package authors to extend the supported MLflow models. See https://mlflow.org/docs/latest/models.html#storage-format for more info on MLflow model flavors. mlflow_load_flavor(flavor, model_path)    Arguments        Argument Description    flavor An MLflow flavor object loaded by mlflo w_load_model , with class loaded from the flavor field in an MLmodel file.  model_path The path to the MLflow model wrapped in the correct class.       mlflow_load_model  Load MLflow Model Loads an MLflow model. MLflow models can have multiple model flavors. Not all flavors / models can be loaded in R. This method by default searches for a flavor supported by R/MLflow. mlflow_load_model(model_uri, flavor = NULL, client = mlflow_client())    Arguments        Argument Description    model_uri The location, in URI format, of the MLflow model.  flavor Optional flavor specification (string). Can be used to load a particular flavor in case there are multiple flavors available.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.      Details  The URI scheme must be supported by MLflow - i.e. there has to be an MLflow artifact repository corresponding to the scheme of the URI. The content is expected to point to a directory containing MLmodel. The following are examples of valid model uris:  file:///absolute/path/to/local/model file:relative/path/to/local/model s3://my_bucket/path/to/model runs:/<mlflow_run_id>/run-relative/path/to/model models:/<model_name>/<model_version> models:/<model_name>/<stage>  For more information about supported URI schemes, see the Artifacts Documentation at https://www.mlflow.org/docs/latest/tracking.html#artifact-stores.    mlflow_log_artifact  Log Artifact Logs a specific file or directory as an artifact for a run. mlflow_log_artifact(path, artifact_path = NULL, run_id = NULL, client = NULL)    Arguments        Argument Description    path The file or directory to log as an artifact.  artifact_path Destination path within the run’s artifact URI.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.      Details  When logging to Amazon S3, ensure that you have the s3:PutObject, s3:GetObject, s3:ListBucket, and s3:GetBucketLocation permissions on your bucket. Additionally, at least the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables must be set to the corresponding key and secrets provided by Amazon IAM.    mlflow_log_batch  Log Batch Log a batch of metrics, params, and/or tags for a run. The server will respond with an error (non-200 status code) if any data failed to be persisted. In case of error (due to internal server error or an invalid request), partial data may be written. mlflow_log_batch(   metrics = NULL,   params = NULL,   tags = NULL,   run_id = NULL,   client = NULL )    Arguments        Argument Description    metrics A dataframe of metrics to log, containing the following columns: “key”, “value”, “step”, “timestamp”. This dataframe cannot contain any missing (‘NA’) entries.  params A dataframe of params to log, containing the following columns: “key”, “value”. This dataframe cannot contain any missing (‘NA’) entries.  tags A dataframe of tags to log, containing the following columns: “key”, “value”. This dataframe cannot contain any missing (‘NA’) entries.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_log_metric  Log Metric Logs a metric for a run. Metrics key-value pair that records a single float measure. During a single execution of a run, a particular metric can be logged several times. The MLflow Backend keeps track of historical metric values along two axes: timestamp and step. mlflow_log_metric(   key,   value,   timestamp = NULL,   step = NULL,   run_id = NULL,   client = NULL )    Arguments        Argument Description    key Name of the metric.  value Float value for the metric being logged.  timestamp Timestamp at which to log the metric. Timestamp is rounded to the nearest integer. If unspecified, the number of milliseconds since the Unix epoch is used.  step Step at which to log the metric. Step is rounded to the nearest integer. If unspecified, the default value of zero is used.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_log_model  Log Model Logs a model for this run. Similar to mlflow_save_model() but stores model as an artifact within the active run. mlflow_log_model(model, artifact_path, ...)    Arguments        Argument Description    model The model that will perform a prediction.  artifact_path Destination path where this MLflow compatible model will be saved.  ... Optional additional arguments passed to mlflow_save_model() when persisting the model. For example, conda_env = /path/to/conda.yaml may be passed to specify a conda dependencies file for flavors (e.g. keras) that support conda environments.       mlflow_log_param  Log Parameter Logs a parameter for a run. Examples are params and hyperparams used for ML training, or constant dates and values used in an ETL pipeline. A param is a STRING key-value pair. For a run, a single parameter is allowed to be logged only once. mlflow_log_param(key, value, run_id = NULL, client = NULL)    Arguments        Argument Description    key Name of the parameter.  value String value of the parameter.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_param  Read Command-Line Parameter Reads a command-line parameter passed to an MLflow project MLflow allows you to define named, typed input parameters to your R scripts via the mlflow_param API. This is useful for experimentation, e.g. tracking multiple invocations of the same script with different parameters. mlflow_param(name, default = NULL, type = NULL, description = NULL)    Arguments        Argument Description    name The name of the parameter.  default The default value of the parameter.  type Type of this parameter. Required if default is not set. If specified, must be one of “numeric”, “integer”, or “string”.  description Optional description for the parameter.      Examples  # This parametrized script trains a GBM model on the Iris dataset and can be run as an MLflow # project. You can run this script (assuming it's saved at /some/directory/params_example.R) # with custom parameters via: # mlflow_run(entry_point = ""params_example.R"", uri = ""/some/directory"", #   parameters = list(num_trees = 200, learning_rate = 0.1)) install.packages(""gbm"") library(mlflow) library(gbm) # define and read input parameters num_trees <- mlflow_param(name = ""num_trees"", default = 200, type = ""integer"") lr <- mlflow_param(name = ""learning_rate"", default = 0.1, type = ""numeric"") # use params to fit a model ir.adaboost <- gbm(Species ~., data=iris, n.trees=num_trees, shrinkage=lr)      mlflow_predict  Generate Prediction with MLflow Model Performs prediction over a model loaded using mlflow_load_model() , to be used by package authors to extend the supported MLflow models. mlflow_predict(model, data, ...)    Arguments        Argument Description    model The loaded MLflow model flavor.  data A data frame to perform scoring.  ... Optional additional arguments passed to underlying predict methods.       mlflow_register_external_observer  Register an external MLflow observer Registers an external MLflow observer that will receive a register_tracking_event(event_name, data) callback on any model tracking event such as “create_run”, “delete_run”, or “log_metric”. Each observer should have a register_tracking_event(event_name, data) callback accepting a character vector event_name specifying the name of the tracking event, and data containing a list of attributes of the event. The callback should be non-blocking, and ideally should complete instantaneously. Any exception thrown from the callback will be ignored. mlflow_register_external_observer(observer)    Arguments        Argument Description    observer The observer object (see example)      Examples  library(mlflow)  observer <- structure(list()) observer$register_tracking_event <- function(event_name, data) { print(event_name) print(data) } mlflow_register_external_observer(observer)      mlflow_rename_experiment  Rename Experiment Renames an experiment. mlflow_rename_experiment(new_name, experiment_id = NULL, client = NULL)    Arguments        Argument Description    new_name The experiment’s name will be changed to this. The new name must be unique.  experiment_id ID of the associated experiment. This field is required.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_rename_registered_model  Rename a registered model Renames a model in the Model Registry. mlflow_rename_registered_model(name, new_name, client = NULL)    Arguments        Argument Description    name The current name of the model.  new_name The new name for the model.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_restore_experiment  Restore Experiment Restores an experiment marked for deletion. This also restores associated metadata, runs, metrics, and params. If experiment uses FileStore, underlying artifacts associated with experiment are also restored. mlflow_restore_experiment(experiment_id, client = NULL)    Arguments        Argument Description    experiment_id ID of the associated experiment. This field is required.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.      Details  Throws RESOURCE_DOES_NOT_EXIST if the experiment was never created or was permanently deleted.    mlflow_restore_run  Restore a Run Restores the run with the specified ID. mlflow_restore_run(run_id, client = NULL)    Arguments        Argument Description    run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_rfunc_serve  Serve an RFunc MLflow Model Serves an RFunc MLflow model as a local REST API server. This interface provides similar functionality to mlflow models serve cli command, however, it can only be used to deploy models that include RFunc flavor. The deployed server supports standard mlflow models interface with /ping and /invocation endpoints. In addition, R function models also support deprecated /predict endpoint for generating predictions. The /predict endpoint will be removed in a future version of mlflow. mlflow_rfunc_serve(   model_uri,   host = ""127.0.0.1"",   port = 8090,   daemonized = FALSE,   browse = !daemonized,   ... )    Arguments        Argument Description    model_uri The location, in URI format, of the MLflow model.  host Address to use to serve model, as a string.  port Port to use to serve model, as numeric.  daemonized Makes httpuv server daemonized so R interactive sessions are not blocked to handle requests. To terminate a daemonized server, call httpuv::stopDaemonizedServer() with the handle returned from this call.  browse Launch browser with serving landing page?  ... Optional arguments passed to mlflow_predict().      Details  The URI scheme must be supported by MLflow - i.e. there has to be an MLflow artifact repository corresponding to the scheme of the URI. The content is expected to point to a directory containing MLmodel. The following are examples of valid model uris:  file:///absolute/path/to/local/model file:relative/path/to/local/model s3://my_bucket/path/to/model runs:/<mlflow_run_id>/run-relative/path/to/model models:/<model_name>/<model_version> models:/<model_name>/<stage>  For more information about supported URI schemes, see the Artifacts Documentation at https://www.mlflow.org/docs/latest/tracking.html#artifact-stores.   Examples  library(mlflow)  # save simple model with constant prediction mlflow_save_model(function(df) 1, ""mlflow_constant"")  # serve an existing model over a web interface mlflow_rfunc_serve(""mlflow_constant"")  # request prediction from server httr::POST(""http://127.0.0.1:8090/predict/"")      mlflow_run  Run an MLflow Project Wrapper for the mlflow run CLI command. See https://www.mlflow.org/docs/latest/cli.html#mlflow-run for more info. mlflow_run(   uri = ""."",   entry_point = NULL,   version = NULL,   parameters = NULL,   experiment_id = NULL,   experiment_name = NULL,   backend = NULL,   backend_config = NULL,   env_manager = NULL,   storage_dir = NULL )    Arguments        Argument Description    uri A directory containing modeling scripts, defaults to the current directory.  entry_point Entry point within project, defaults to main if not specified.  version Version of the project to run, as a Git commit reference for Git projects.  parameters A list of parameters.  experiment_id ID of the experiment under which to launch the run.  experiment_name Name of the experiment under which to launch the run.  backend Execution backend to use for run.  backend_config Path to JSON file which will be passed to the backend. For the Databricks backend, it should describe the cluster to use when launching a run on Databricks.  env_manager If specified, create an environment for the project using the specified environment manager. Available options are ‘local’, ‘virtualenv’, and ‘conda’.  storage_dir Valid only when backend is local. MLflow downloads artifacts from distributed URIs passed to parameters of type path to subdirectories of storage_dir.      Value  The run associated with this run.   Examples  # This parametrized script trains a GBM model on the Iris dataset and can be run as an MLflow # project. You can run this script (assuming it's saved at /some/directory/params_example.R) # with custom parameters via: # mlflow_run(entry_point = ""params_example.R"", uri = ""/some/directory"", #   parameters = list(num_trees = 200, learning_rate = 0.1)) install.packages(""gbm"") library(mlflow) library(gbm) # define and read input parameters num_trees <- mlflow_param(name = ""num_trees"", default = 200, type = ""integer"") lr <- mlflow_param(name = ""learning_rate"", default = 0.1, type = ""numeric"") # use params to fit a model ir.adaboost <- gbm(Species ~., data=iris, n.trees=num_trees, shrinkage=lr)      mlflow_save_model.crate  Save Model for MLflow Saves model in MLflow format that can later be used for prediction and serving. This method is generic to allow package authors to save custom model types. list(list(""mlflow_save_model""), list(""crate""))(model, path, model_spec = list(), ...) mlflow_save_model(model, path, model_spec = list(), ...) list(list(""mlflow_save_model""), list(""H2OModel""))(model, path, model_spec = list(), conda_env = NULL, ...) list(list(""mlflow_save_model""), list(""keras.engine.training.Model""))(model, path, model_spec = list(), conda_env = NULL, ...) list(list(""mlflow_save_model""), list(""xgb.Booster""))(model, path, model_spec = list(), conda_env = NULL, ...)    Arguments        Argument Description    model The model that will perform a prediction.  path Destination path where this MLflow compatible model will be saved.  model_spec MLflow model config this model flavor is being added to.  ... Optional additional arguments.  conda_env Path to Conda dependencies file.       mlflow_search_experiments  Search Experiments Search for experiments that satisfy specified criteria. mlflow_search_experiments(   filter = NULL,   experiment_view_type = c(""ACTIVE_ONLY"", ""DELETED_ONLY"", ""ALL""),   max_results = 1000,   order_by = list(),   page_token = NULL,   client = NULL )    Arguments        Argument Description    filter A filter expression used to identify specific experiments. The syntax is a subset of SQL which allows only ANDing together binary operations. Examples: “attribute.name = ‘MyExperiment’”, “tags.problem_type = ‘iris_regression’”  experiment_view_type Experiment view type. Only experiments matching this view type are returned.  max_results Maximum number of experiments to retrieve.  order_by List of properties to order by. Example: “attribute.name”.  page_token Pagination token to go to the next page based on a previous query.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_search_registered_models  List registered models Retrieves a list of registered models. mlflow_search_registered_models(   filter = NULL,   max_results = 100,   order_by = list(),   page_token = NULL,   client = NULL )    Arguments        Argument Description    filter A filter expression used to identify specific registered models. The syntax is a subset of SQL which allows only ANDing together binary operations. Example: “name = ‘my_model_name’ and tag.key = ‘value1’”  max_results Maximum number of registered models to retrieve.  order_by List of registered model properties to order by. Example: “name”.  page_token Pagination token to go to the next page based on a previous query.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_search_runs  Search Runs Search for runs that satisfy expressions. Search expressions can use Metric and Param keys. mlflow_search_runs(   filter = NULL,   run_view_type = c(""ACTIVE_ONLY"", ""DELETED_ONLY"", ""ALL""),   experiment_ids = NULL,   order_by = list(),   client = NULL )    Arguments        Argument Description    filter A filter expression over params, metrics, and tags, allowing returning a subset of runs. The syntax is a subset of SQL which allows only ANDing together binary operations between a param/metric/tag and a constant.  run_view_type Run view type.  experiment_ids List of string experiment IDs (or a single string experiment ID) to search over. Attempts to use active experiment if not specified.  order_by List of properties to order by. Example: “metrics.acc DESC”.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_server  Run MLflow Tracking Server Wrapper for mlflow server. mlflow_server(   file_store = ""mlruns"",   default_artifact_root = NULL,   host = ""127.0.0.1"",   port = 5000,   workers = NULL,   static_prefix = NULL,   serve_artifacts = FALSE )    Arguments        Argument Description    file_store The root of the backing file store for experiment and run data.  default_artifact_root Local or S3 URI to store artifacts in, for newly created experiments.  host The network address to listen on (default: 127.0.0.1).  port The port to listen on (default: 5000).  workers Number of gunicorn worker processes to handle requests (default: 4).  static_prefix A prefix which will be prepended to the path of all static paths.  serve_artifacts A flag specifying whether or not to enable artifact serving (default: FALSE).       mlflow_set_experiment_tag  Set Experiment Tag Sets a tag on an experiment with the specified ID. Tags are experiment metadata that can be updated. mlflow_set_experiment_tag(key, value, experiment_id = NULL, client = NULL)    Arguments        Argument Description    key Name of the tag. All storage backends are guaranteed to support key values up to 250 bytes in size. This field is required.  value String value of the tag being logged. All storage backends are guaranteed to support key values up to 5000 bytes in size. This field is required.  experiment_id ID of the experiment.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_set_experiment  Set Experiment Sets an experiment as the active experiment. Either the name or ID of the experiment can be provided. If the a name is provided but the experiment does not exist, this function creates an experiment with provided name. Returns the ID of the active experiment. mlflow_set_experiment(   experiment_name = NULL,   experiment_id = NULL,   artifact_location = NULL )    Arguments        Argument Description    experiment_name Name of experiment to be activated.  experiment_id ID of experiment to be activated.  artifact_location Location where all artifacts for this experiment are stored. If not provided, the remote server will select an appropriate default.       mlflow_set_model_version_tag  Set Model version tag Set a tag for the model version. When stage is set, tag will be set for latest model version of the stage. Setting both version and stage parameter will result in error. mlflow_set_model_version_tag(   name,   version = NULL,   key = NULL,   value = NULL,   stage = NULL,   client = NULL )    Arguments        Argument Description    name Registered model name.  version Registered model version.  key Tag key to log. key is required.  value Tag value to log. value is required.  stage Registered model stage.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_set_tag  Set Tag Sets a tag on a run. Tags are run metadata that can be updated during a run and after a run completes. mlflow_set_tag(key, value, run_id = NULL, client = NULL)    Arguments        Argument Description    key Name of the tag. Maximum size is 255 bytes. This field is required.  value String value of the tag being logged. Maximum size is 500 bytes. This field is required.  run_id Run ID.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_set_tracking_uri  Set Remote Tracking URI Specifies the URI to the remote MLflow server that will be used to track experiments. mlflow_set_tracking_uri(uri)    Arguments        Argument Description    uri The URI to the remote MLflow server.       mlflow_source  Source a Script with MLflow Params This function should not be used interactively. It is designed to be called via Rscript from the terminal or through the MLflow CLI. mlflow_source(uri)    Arguments        Argument Description    uri Path to an R script, can be a quoted or unquoted string.       mlflow_start_run  Start Run Starts a new run. If client is not provided, this function infers contextual information such as source name and version, and also registers the created run as the active run. If client is provided, no inference is done, and additional arguments such as start_time can be provided. mlflow_start_run(   run_id = NULL,   experiment_id = NULL,   start_time = NULL,   tags = NULL,   client = NULL,   nested = FALSE )    Arguments        Argument Description    run_id If specified, get the run with the specified UUID and log metrics and params under that run. The run’s end time is unset and its status is set to running, but the run’s other attributes remain unchanged.  experiment_id Used only when run_id is unspecified. ID of the experiment under which to create the current run. If unspecified, the run is created under a new experiment with a randomly generated name.  start_time Unix timestamp of when the run started in milliseconds. Only used when client is specified.  tags Additional metadata for run in key-value pairs. Only used when client is specified.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.  nested Controls whether the run to be started is nested in a parent run. TRUE creates a nest run.      Examples  with(mlflow_start_run(), { mlflow_log_metric(""test"", 10) })      mlflow_transition_model_version_stage  Transition ModelVersion Stage Transition a model version to a different stage. mlflow_transition_model_version_stage(   name,   version,   stage,   archive_existing_versions = FALSE,   client = NULL )    Arguments        Argument Description    name Name of the registered model.  version Model version number.  stage Transition model_version to this stage.  archive_existing_versions (Optional)  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_ui  Run MLflow User Interface Launches the MLflow user interface. mlflow_ui(client, ...)    Arguments        Argument Description    client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.  ... Optional arguments passed to mlflow_server() when x is a path to a file store.      Examples  library(mlflow)  # launch mlflow ui locally mlflow_ui()  # launch mlflow ui for existing mlflow server mlflow_set_tracking_uri(""http://tracking-server:5000"") mlflow_ui()      mlflow_update_model_version  Update model version Updates a model version mlflow_update_model_version(name, version, description, client = NULL)    Arguments        Argument Description    name Name of the registered model.  version Model version number.  description Description of this model version.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.       mlflow_update_registered_model  Update a registered model Updates a model in the Model Registry. mlflow_update_registered_model(name, description, client = NULL)    Arguments        Argument Description    name The name of the registered model.  description The updated description for this registered model.  client (Optional) An MLflow client object returned from mlflow_client . If specified, MLflow will use the tracking server associated with the passed-in client. If unspecified (the common case), MLflow will use the tracking server associated with the current tracking URI.            Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
rest-api.html,"   Documentation  REST API       REST API  The MLflow REST API allows you to create, list, and get experiments and runs, and log parameters, metrics, and artifacts. The API is hosted under the /api route on the MLflow tracking server. For example, to search for experiments on a tracking server hosted at http://localhost:5000, make a POST request to http://localhost:5000/api/2.0/mlflow/experiments/search.  Table of Contents  Create Experiment Search Experiments Get Experiment Get Experiment By Name Delete Experiment Restore Experiment Update Experiment Create Run Delete Run Restore Run Get Run Log Metric Log Batch Log Model Log Inputs Set Experiment Tag Set Tag Delete Tag Log Param Get Metric History Search Runs List Artifacts Update Run Create RegisteredModel Get RegisteredModel Rename RegisteredModel Update RegisteredModel Delete RegisteredModel Get Latest ModelVersions Create ModelVersion Get ModelVersion Update ModelVersion Delete ModelVersion Search ModelVersions Get Download URI For ModelVersion Artifacts Transition ModelVersion Stage Search RegisteredModels Set Registered Model Tag Set Model Version Tag Delete Registered Model Tag Delete Model Version Tag Delete Registered Model Alias Get Model Version by Alias Set Registered Model Alias Data Structures     Create Experiment        Endpoint HTTP Method    2.0/mlflow/experiments/create POST    Create an experiment with a name. Returns the ID of the newly created experiment. Validates that another experiment with the same name does not already exist and fails if another experiment with the same name already exists. Throws RESOURCE_ALREADY_EXISTS if a experiment with the given name exists.  Request Structure         Field Name Type Description    name STRING Experiment name. This field is required.  artifact_location STRING Location where all artifacts for the experiment are stored. If not provided, the remote server will select an appropriate default.  tags An array of ExperimentTag A collection of tags to set on the experiment. Maximum tag size and number of tags per request depends on the storage backend. All storage backends are guaranteed to support tag keys up to 250 bytes in size and tag values up to 5000 bytes in size. All storage backends are also guaranteed to support up to 20 tags per request.      Response Structure         Field Name Type Description    experiment_id STRING Unique identifier for the experiment.        Search Experiments        Endpoint HTTP Method    2.0/mlflow/experiments/search POST     Request Structure         Field Name Type Description    max_results INT64 Maximum number of experiments desired. Servers may select a desired default max_results value. All servers are guaranteed to support a max_results threshold of at least 1,000 but may support more. Callers of this endpoint are encouraged to pass max_results explicitly and leverage page_token to iterate through experiments.  page_token STRING Token indicating the page of experiments to fetch  filter STRING A filter expression over experiment attributes and tags that allows returning a subset of experiments. The syntax is a subset of SQL that supports ANDing together binary operations between an attribute or tag, and a constant. Example: name LIKE 'test-%' AND tags.key = 'value' You can select columns with special characters (hyphen, space, period, etc.) by using double quotes or backticks. Example: tags.""extra-key"" = 'value' or tags.`extra-key` = 'value' Supported operators are =, !=, LIKE, and ILIKE.   order_by An array of STRING List of columns for ordering search results, which can include experiment name and id with an optional “DESC” or “ASC” annotation, where “ASC” is the default. Tiebreaks are done by experiment id DESC.  view_type ViewType Qualifier for type of experiments to be returned. If unspecified, return only active experiments.      Response Structure         Field Name Type Description    experiments An array of Experiment Experiments that match the search criteria  next_page_token STRING Token that can be used to retrieve the next page of experiments. An empty token means that no more experiments are available for retrieval.        Get Experiment        Endpoint HTTP Method    2.0/mlflow/experiments/get GET    Get metadata for an experiment. This method works on deleted experiments.  Request Structure         Field Name Type Description    experiment_id STRING ID of the associated experiment. This field is required.      Response Structure         Field Name Type Description    experiment Experiment Experiment details.        Get Experiment By Name        Endpoint HTTP Method    2.0/mlflow/experiments/get-by-name GET    Get metadata for an experiment. This endpoint will return deleted experiments, but prefers the active experiment if an active and deleted experiment share the same name. If multiple deleted experiments share the same name, the API will return one of them. Throws RESOURCE_DOES_NOT_EXIST if no experiment with the specified name exists.  Request Structure         Field Name Type Description    experiment_name STRING Name of the associated experiment. This field is required.      Response Structure         Field Name Type Description    experiment Experiment Experiment details.        Delete Experiment        Endpoint HTTP Method    2.0/mlflow/experiments/delete POST    Mark an experiment and associated metadata, runs, metrics, params, and tags for deletion. If the experiment uses FileStore, artifacts associated with experiment are also deleted.  Request Structure         Field Name Type Description    experiment_id STRING ID of the associated experiment. This field is required.        Restore Experiment        Endpoint HTTP Method    2.0/mlflow/experiments/restore POST    Restore an experiment marked for deletion. This also restores associated metadata, runs, metrics, params, and tags. If experiment uses FileStore, underlying artifacts associated with experiment are also restored. Throws RESOURCE_DOES_NOT_EXIST if experiment was never created or was permanently deleted.  Request Structure         Field Name Type Description    experiment_id STRING ID of the associated experiment. This field is required.        Update Experiment        Endpoint HTTP Method    2.0/mlflow/experiments/update POST    Update experiment metadata.  Request Structure         Field Name Type Description    experiment_id STRING ID of the associated experiment. This field is required.  new_name STRING If provided, the experiment’s name is changed to the new name. The new name must be unique.        Create Run        Endpoint HTTP Method    2.0/mlflow/runs/create POST    Create a new run within an experiment. A run is usually a single execution of a machine learning or data ETL pipeline. MLflow uses runs to track Param, Metric, and RunTag associated with a single execution.  Request Structure         Field Name Type Description    experiment_id STRING ID of the associated experiment.  user_id STRING ID of the user executing the run. This field is deprecated as of MLflow 1.0, and will be removed in a future MLflow release. Use ‘mlflow.user’ tag instead.  run_name STRING Name of the run.  start_time INT64 Unix timestamp in milliseconds of when the run started.  tags An array of RunTag Additional metadata for run.      Response Structure         Field Name Type Description    run Run The newly created run.        Delete Run        Endpoint HTTP Method    2.0/mlflow/runs/delete POST    Mark a run for deletion.  Request Structure         Field Name Type Description    run_id STRING ID of the run to delete. This field is required.        Restore Run        Endpoint HTTP Method    2.0/mlflow/runs/restore POST    Restore a deleted run.  Request Structure         Field Name Type Description    run_id STRING ID of the run to restore. This field is required.        Get Run        Endpoint HTTP Method    2.0/mlflow/runs/get GET    Get metadata, metrics, params, and tags for a run. In the case where multiple metrics with the same key are logged for a run, return only the value with the latest timestamp. If there are multiple values with the latest timestamp, return the maximum of these values.  Request Structure         Field Name Type Description    run_id STRING ID of the run to fetch. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run to fetch. This field will be removed in a future MLflow version.      Response Structure         Field Name Type Description    run Run Run metadata (name, start time, etc) and data (metrics, params, and tags).        Log Metric        Endpoint HTTP Method    2.0/mlflow/runs/log-metric POST    Log a metric for a run. A metric is a key-value pair (string key, float value) with an associated timestamp. Examples include the various metrics that represent ML model accuracy. A metric can be logged multiple times.  Request Structure         Field Name Type Description    run_id STRING ID of the run under which to log the metric. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run under which to log the metric. This field will be removed in a future MLflow version.  key STRING Name of the metric. This field is required.  value DOUBLE Double value of the metric being logged. This field is required.  timestamp INT64 Unix timestamp in milliseconds at the time metric was logged. This field is required.  step INT64 Step at which to log the metric        Log Batch        Endpoint HTTP Method    2.0/mlflow/runs/log-batch POST    Log a batch of metrics, params, and tags for a run. If any data failed to be persisted, the server will respond with an error (non-200 status code). In case of error (due to internal server error or an invalid request), partial data may be written. You can write metrics, params, and tags in interleaving fashion, but within a given entity type are guaranteed to follow the order specified in the request body. That is, for an API request like {    ""run_id"": ""2a14ed5c6a87499199e0106c3501eab8"",    ""metrics"": [      {""key"": ""mae"", ""value"": 2.5, ""timestamp"": 1552550804},      {""key"": ""rmse"", ""value"": 2.7, ""timestamp"": 1552550804},    ],    ""params"": [      {""key"": ""model_class"", ""value"": ""LogisticRegression""},    ] }   the server is guaranteed to write metric “rmse” after “mae”, though it may write param “model_class” before both metrics, after “mae”, or after both metrics. The overwrite behavior for metrics, params, and tags is as follows:  Metrics: metric values are never overwritten. Logging a metric (key, value, timestamp) appends to the set of values for the metric with the provided key. Tags: tag values can be overwritten by successive writes to the same tag key. That is, if multiple tag values with the same key are provided in the same API request, the last-provided tag value is written. Logging the same tag (key, value) is permitted - that is, logging a tag is idempotent. Params: once written, param values cannot be changed (attempting to overwrite a param value will result in an error). However, logging the same param (key, value) is permitted - that is, logging a param is idempotent.   Request Limits  A single JSON-serialized API request may be up to 1 MB in size and contain:  No more than 1000 metrics, params, and tags in total Up to 1000 metrics Up to 100 params Up to 100 tags  For example, a valid request might contain 900 metrics, 50 params, and 50 tags, but logging 900 metrics, 50 params, and 51 tags is invalid. The following limits also apply to metric, param, and tag keys and values:  Metric, param, and tag keys can be up to 250 characters in length Param and tag values can be up to 250 characters in length    Request Structure         Field Name Type Description    run_id STRING ID of the run to log under  metrics An array of Metric Metrics to log. A single request can contain up to 1000 metrics, and up to 1000 metrics, params, and tags in total.  params An array of Param Params to log. A single request can contain up to 100 params, and up to 1000 metrics, params, and tags in total.  tags An array of RunTag Tags to log. A single request can contain up to 100 tags, and up to 1000 metrics, params, and tags in total.        Log Model        Endpoint HTTP Method    2.0/mlflow/runs/log-model POST     Note Experimental: This API may change or be removed in a future release without warning.   Request Structure         Field Name Type Description    run_id STRING ID of the run to log under  model_json STRING MLmodel file in json format.        Log Inputs        Endpoint HTTP Method    2.0/mlflow/runs/log-inputs POST     Request Structure         Field Name Type Description    run_id STRING ID of the run to log under This field is required.  datasets An array of DatasetInput Dataset inputs        Set Experiment Tag        Endpoint HTTP Method    2.0/mlflow/experiments/set-experiment-tag POST    Set a tag on an experiment. Experiment tags are metadata that can be updated.  Request Structure         Field Name Type Description    experiment_id STRING ID of the experiment under which to log the tag. Must be provided. This field is required.  key STRING Name of the tag. Maximum size depends on storage backend. All storage backends are guaranteed to support key values up to 250 bytes in size. This field is required.  value STRING String value of the tag being logged. Maximum size depends on storage backend. All storage backends are guaranteed to support key values up to 5000 bytes in size. This field is required.        Set Tag        Endpoint HTTP Method    2.0/mlflow/runs/set-tag POST    Set a tag on a run. Tags are run metadata that can be updated during a run and after a run completes.  Request Structure         Field Name Type Description    run_id STRING ID of the run under which to log the tag. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run under which to log the tag. This field will be removed in a future MLflow version.  key STRING Name of the tag. Maximum size depends on storage backend. All storage backends are guaranteed to support key values up to 250 bytes in size. This field is required.  value STRING String value of the tag being logged. Maximum size depends on storage backend. All storage backends are guaranteed to support key values up to 5000 bytes in size. This field is required.        Delete Tag        Endpoint HTTP Method    2.0/mlflow/runs/delete-tag POST    Delete a tag on a run. Tags are run metadata that can be updated during a run and after a run completes.  Request Structure         Field Name Type Description    run_id STRING ID of the run that the tag was logged under. Must be provided. This field is required.  key STRING Name of the tag. Maximum size is 255 bytes. Must be provided. This field is required.        Log Param        Endpoint HTTP Method    2.0/mlflow/runs/log-parameter POST    Log a param used for a run. A param is a key-value pair (string key, string value). Examples include hyperparameters used for ML model training and constant dates and values used in an ETL pipeline. A param can be logged only once for a run.  Request Structure         Field Name Type Description    run_id STRING ID of the run under which to log the param. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run under which to log the param. This field will be removed in a future MLflow version.  key STRING Name of the param. Maximum size is 255 bytes. This field is required.  value STRING String value of the param being logged. Maximum size is 6000 bytes. This field is required.        Get Metric History        Endpoint HTTP Method    2.0/mlflow/metrics/get-history GET    Get a list of all values for the specified metric for a given run.  Request Structure         Field Name Type Description    run_id STRING ID of the run from which to fetch metric values. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run from which to fetch metric values. This field will be removed in a future MLflow version.  metric_key STRING Name of the metric. This field is required.  page_token STRING Token indicating the page of metric history to fetch  max_results INT32 Maximum number of logged instances of a metric for a run to return per call. Backend servers may restrict the value of max_results depending on performance requirements. Requests that do not specify this value will behave as non-paginated queries where all metric history values for a given metric within a run are returned in a single response.      Response Structure         Field Name Type Description    metrics An array of Metric All logged values for this metric.  next_page_token STRING Token that can be used to issue a query for the next page of metric history values. A missing token indicates that no additional metrics are available to fetch.        Search Runs        Endpoint HTTP Method    2.0/mlflow/runs/search POST    Search for runs that satisfy expressions. Search expressions can use Metric and Param keys.  Request Structure         Field Name Type Description    experiment_ids An array of STRING List of experiment IDs to search over.  filter STRING A filter expression over params, metrics, and tags, that allows returning a subset of runs. The syntax is a subset of SQL that supports ANDing together binary operations between a param, metric, or tag and a constant. Example: metrics.rmse < 1 and params.model_class = 'LogisticRegression' You can select columns with special characters (hyphen, space, period, etc.) by using double quotes: metrics.""model class"" = 'LinearRegression' and tags.""user-name"" = 'Tomas' Supported operators are =, !=, >, >=, <, and <=.   run_view_type ViewType Whether to display only active, only deleted, or all runs. Defaults to only active runs.  max_results INT32 Maximum number of runs desired. If unspecified, defaults to 1000. All servers are guaranteed to support a max_results threshold of at least 50,000 but may support more. Callers of this endpoint are encouraged to pass max_results explicitly and leverage page_token to iterate through experiments.  order_by An array of STRING List of columns to be ordered by, including attributes, params, metrics, and tags with an optional “DESC” or “ASC” annotation, where “ASC” is the default. Example: [“params.input DESC”, “metrics.alpha ASC”, “metrics.rmse”] Tiebreaks are done by start_time DESC followed by run_id for runs with the same start time (and this is the default ordering criterion if order_by is not provided).  page_token STRING       Response Structure         Field Name Type Description    runs An array of Run Runs that match the search criteria.  next_page_token STRING         List Artifacts        Endpoint HTTP Method    2.0/mlflow/artifacts/list GET    List artifacts for a run. Takes an optional artifact_path prefix which if specified, the response contains only artifacts with the specified prefix.  Request Structure         Field Name Type Description    run_id STRING ID of the run whose artifacts to list. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run whose artifacts to list. This field will be removed in a future MLflow version.  path STRING Filter artifacts matching this path (a relative path from the root artifact directory).  page_token STRING Token indicating the page of artifact results to fetch      Response Structure         Field Name Type Description    root_uri STRING Root artifact directory for the run.  files An array of FileInfo File location and metadata for artifacts.  next_page_token STRING Token that can be used to retrieve the next page of artifact results        Update Run        Endpoint HTTP Method    2.0/mlflow/runs/update POST    Update run metadata.  Request Structure         Field Name Type Description    run_id STRING ID of the run to update. Must be provided.  run_uuid STRING [Deprecated, use run_id instead] ID of the run to update.. This field will be removed in a future MLflow version.  status RunStatus Updated status of the run.  end_time INT64 Unix timestamp in milliseconds of when the run ended.  run_name STRING Updated name of the run.      Response Structure         Field Name Type Description    run_info RunInfo Updated metadata of the run.        Create RegisteredModel        Endpoint HTTP Method    2.0/mlflow/registered-models/create POST    Throws RESOURCE_ALREADY_EXISTS if a registered model with the given name exists.  Request Structure         Field Name Type Description    name STRING Register models under this name This field is required.  tags An array of RegisteredModelTag Additional metadata for registered model.  description STRING Optional description for registered model.      Response Structure         Field Name Type Description    registered_model RegisteredModel         Get RegisteredModel        Endpoint HTTP Method    2.0/mlflow/registered-models/get GET     Request Structure         Field Name Type Description    name STRING Registered model unique name identifier. This field is required.      Response Structure         Field Name Type Description    registered_model RegisteredModel         Rename RegisteredModel        Endpoint HTTP Method    2.0/mlflow/registered-models/rename POST     Request Structure         Field Name Type Description    name STRING Registered model unique name identifier. This field is required.  new_name STRING If provided, updates the name for this registered_model.      Response Structure         Field Name Type Description    registered_model RegisteredModel         Update RegisteredModel        Endpoint HTTP Method    2.0/mlflow/registered-models/update PATCH     Request Structure         Field Name Type Description    name STRING Registered model unique name identifier. This field is required.  description STRING If provided, updates the description for this registered_model.      Response Structure         Field Name Type Description    registered_model RegisteredModel         Delete RegisteredModel        Endpoint HTTP Method    2.0/mlflow/registered-models/delete DELETE     Request Structure         Field Name Type Description    name STRING Registered model unique name identifier. This field is required.        Get Latest ModelVersions        Endpoint HTTP Method    2.0/mlflow/registered-models/get-latest-versions POST     Request Structure         Field Name Type Description    name STRING Registered model unique name identifier. This field is required.  stages An array of STRING List of stages.      Response Structure         Field Name Type Description    model_versions An array of ModelVersion Latest version models for each requests stage. Only return models with current READY status. If no stages provided, returns the latest version for each stage, including ""None"".        Create ModelVersion        Endpoint HTTP Method    2.0/mlflow/model-versions/create POST     Request Structure         Field Name Type Description    name STRING Register model under this name This field is required.  source STRING URI indicating the location of the model artifacts. This field is required.  run_id STRING MLflow run ID for correlation, if source was generated by an experiment run in MLflow tracking server  tags An array of ModelVersionTag Additional metadata for model version.  run_link STRING MLflow run link - this is the exact link of the run that generated this model version, potentially hosted at another instance of MLflow.  description STRING Optional description for model version.      Response Structure         Field Name Type Description    model_version ModelVersion Return new version number generated for this model in registry.        Get ModelVersion        Endpoint HTTP Method    2.0/mlflow/model-versions/get GET     Request Structure         Field Name Type Description    name STRING Name of the registered model This field is required.  version STRING Model version number This field is required.      Response Structure         Field Name Type Description    model_version ModelVersion         Update ModelVersion        Endpoint HTTP Method    2.0/mlflow/model-versions/update PATCH     Request Structure         Field Name Type Description    name STRING Name of the registered model This field is required.  version STRING Model version number This field is required.  description STRING If provided, updates the description for this registered_model.      Response Structure         Field Name Type Description    model_version ModelVersion Return new version number generated for this model in registry.        Delete ModelVersion        Endpoint HTTP Method    2.0/mlflow/model-versions/delete DELETE     Request Structure         Field Name Type Description    name STRING Name of the registered model This field is required.  version STRING Model version number This field is required.        Search ModelVersions        Endpoint HTTP Method    2.0/mlflow/model-versions/search GET     Request Structure         Field Name Type Description    filter STRING String filter condition, like “name=’my-model-name’”. Must be a single boolean condition, with string values wrapped in single quotes.  max_results INT64 Maximum number of models desired. Max threshold is 200K. Backends may choose a lower default value and maximum threshold.  order_by An array of STRING List of columns to be ordered by including model name, version, stage with an optional “DESC” or “ASC” annotation, where “ASC” is the default. Tiebreaks are done by latest stage transition timestamp, followed by name ASC, followed by version DESC.  page_token STRING Pagination token to go to next page based on previous search query.      Response Structure         Field Name Type Description    model_versions An array of ModelVersion Models that match the search criteria  next_page_token STRING Pagination token to request next page of models for the same search query.        Get Download URI For ModelVersion Artifacts        Endpoint HTTP Method    2.0/mlflow/model-versions/get-download-uri GET     Request Structure         Field Name Type Description    name STRING Name of the registered model This field is required.  version STRING Model version number This field is required.      Response Structure         Field Name Type Description    artifact_uri STRING URI corresponding to where artifacts for this model version are stored.        Transition ModelVersion Stage        Endpoint HTTP Method    2.0/mlflow/model-versions/transition-stage POST     Request Structure         Field Name Type Description    name STRING Name of the registered model This field is required.  version STRING Model version number This field is required.  stage STRING Transition model_version to new stage. This field is required.  archive_existing_versions BOOL When transitioning a model version to a particular stage, this flag dictates whether all existing model versions in that stage should be atomically moved to the “archived” stage. This ensures that at-most-one model version exists in the target stage. This field is required when transitioning a model versions’s stage This field is required.      Response Structure         Field Name Type Description    model_version ModelVersion Updated model version        Search RegisteredModels        Endpoint HTTP Method    2.0/mlflow/registered-models/search GET     Request Structure         Field Name Type Description    filter STRING String filter condition, like “name LIKE ‘my-model-name’”. Interpreted in the backend automatically as “name LIKE ‘%my-model-name%’”. Single boolean condition, with string values wrapped in single quotes.  max_results INT64 Maximum number of models desired. Default is 100. Max threshold is 1000.  order_by An array of STRING List of columns for ordering search results, which can include model name and last updated timestamp with an optional “DESC” or “ASC” annotation, where “ASC” is the default. Tiebreaks are done by model name ASC.  page_token STRING Pagination token to go to the next page based on a previous search query.      Response Structure         Field Name Type Description    registered_models An array of RegisteredModel Registered Models that match the search criteria.  next_page_token STRING Pagination token to request the next page of models.        Set Registered Model Tag        Endpoint HTTP Method    2.0/mlflow/registered-models/set-tag POST     Request Structure         Field Name Type Description    name STRING Unique name of the model. This field is required.  key STRING Name of the tag. Maximum size depends on storage backend. If a tag with this name already exists, its preexisting value will be replaced by the specified value. All storage backends are guaranteed to support key values up to 250 bytes in size. This field is required.  value STRING String value of the tag being logged. Maximum size depends on storage backend. This field is required.        Set Model Version Tag        Endpoint HTTP Method    2.0/mlflow/model-versions/set-tag POST     Request Structure         Field Name Type Description    name STRING Unique name of the model. This field is required.  version STRING Model version number. This field is required.  key STRING Name of the tag. Maximum size depends on storage backend. If a tag with this name already exists, its preexisting value will be replaced by the specified value. All storage backends are guaranteed to support key values up to 250 bytes in size. This field is required.  value STRING String value of the tag being logged. Maximum size depends on storage backend. This field is required.        Delete Registered Model Tag        Endpoint HTTP Method    2.0/mlflow/registered-models/delete-tag DELETE     Request Structure         Field Name Type Description    name STRING Name of the registered model that the tag was logged under. This field is required.  key STRING Name of the tag. The name must be an exact match; wild-card deletion is not supported. Maximum size is 250 bytes. This field is required.        Delete Model Version Tag        Endpoint HTTP Method    2.0/mlflow/model-versions/delete-tag DELETE     Request Structure         Field Name Type Description    name STRING Name of the registered model that the tag was logged under. This field is required.  version STRING Model version number that the tag was logged under. This field is required.  key STRING Name of the tag. The name must be an exact match; wild-card deletion is not supported. Maximum size is 250 bytes. This field is required.        Delete Registered Model Alias        Endpoint HTTP Method    2.0/mlflow/registered-models/alias DELETE     Request Structure         Field Name Type Description    name STRING Name of the registered model. This field is required.  alias STRING Name of the alias. The name must be an exact match; wild-card deletion is not supported. Maximum size is 256 bytes. This field is required.        Get Model Version by Alias        Endpoint HTTP Method    2.0/mlflow/registered-models/alias GET     Request Structure         Field Name Type Description    name STRING Name of the registered model. This field is required.  alias STRING Name of the alias. Maximum size is 256 bytes. This field is required.      Response Structure         Field Name Type Description    model_version ModelVersion         Set Registered Model Alias        Endpoint HTTP Method    2.0/mlflow/registered-models/alias POST     Request Structure         Field Name Type Description    name STRING Name of the registered model. This field is required.  alias STRING Name of the alias. Maximum size depends on storage backend. If an alias with this name already exists, its preexisting value will be replaced by the specified version. All storage backends are guaranteed to support alias name values up to 256 bytes in size. This field is required.  version STRING Model version number. This field is required.       Data Structures   Dataset  Dataset. Represents a reference to data used for training, testing, or evaluation during the model development process.        Field Name Type Description    name STRING The name of the dataset. E.g. ?my.uc.table@2? ?nyc-taxi-dataset?, ?fantastic-elk-3? This field is required.  digest STRING Dataset digest, e.g. an md5 hash of the dataset that uniquely identifies it within datasets of the same name. This field is required.  source_type STRING Source information for the dataset. Note that the source may not exactly reproduce the dataset if it was transformed / modified before use with MLflow. This field is required.  source STRING The type of the dataset source, e.g. ?databricks-uc-table?, ?DBFS?, ?S3?, … This field is required.  schema STRING The schema of the dataset. E.g., MLflow ColSpec JSON for a dataframe, MLflow TensorSpec JSON for an ndarray, or another schema format.  profile STRING The profile of the dataset. Summary statistics for the dataset, such as the number of rows in a table, the mean / std / mode of each column in a table, or the number of elements in an array.      DatasetInput  DatasetInput. Represents a dataset and input tags.        Field Name Type Description    tags An array of InputTag A list of tags for the dataset input, e.g. a ?context? tag with value ?training?  dataset Dataset The dataset being used as a Run input. This field is required.      Experiment  Experiment        Field Name Type Description    experiment_id STRING Unique identifier for the experiment.  name STRING Human readable name that identifies the experiment.  artifact_location STRING Location where artifacts for the experiment are stored.  lifecycle_stage STRING Current life cycle stage of the experiment: “active” or “deleted”. Deleted experiments are not returned by APIs.  last_update_time INT64 Last update time  creation_time INT64 Creation time  tags An array of ExperimentTag Tags: Additional metadata key-value pairs.      ExperimentTag  Tag for an experiment.        Field Name Type Description    key STRING The tag key.  value STRING The tag value.      FileInfo         Field Name Type Description    path STRING Path relative to the root artifact directory run.  is_dir BOOL Whether the path is a directory.  file_size INT64 Size in bytes. Unset for directories.      InputTag  Tag for an input.        Field Name Type Description    key STRING The tag key. This field is required.  value STRING The tag value. This field is required.      Metric  Metric associated with a run, represented as a key-value pair.        Field Name Type Description    key STRING Key identifying this metric.  value DOUBLE Value associated with this metric.  timestamp INT64 The timestamp at which this metric was recorded.  step INT64 Step at which to log the metric.      ModelVersion         Field Name Type Description    name STRING Unique name of the model  version STRING Model’s version number.  creation_timestamp INT64 Timestamp recorded when this model_version was created.  last_updated_timestamp INT64 Timestamp recorded when metadata for this model_version was last updated.  user_id STRING User that created this model_version.  current_stage STRING Current stage for this model_version.  description STRING Description of this model_version.  source STRING URI indicating the location of the source model artifacts, used when creating model_version  run_id STRING MLflow run ID used when creating model_version, if source was generated by an experiment run stored in MLflow tracking server.  status ModelVersionStatus Current status of model_version  status_message STRING Details on current status, if it is pending or failed.  tags An array of ModelVersionTag Tags: Additional metadata key-value pairs for this model_version.  run_link STRING Run Link: Direct link to the run that generated this version. This field is set at model version creation time only for model versions whose source run is from a tracking server that is different from the registry server.  aliases An array of STRING Aliases pointing to this model_version.      ModelVersionTag  Tag for a model version.        Field Name Type Description    key STRING The tag key.  value STRING The tag value.      Param  Param associated with a run.        Field Name Type Description    key STRING Key identifying this param.  value STRING Value associated with this param.      RegisteredModel         Field Name Type Description    name STRING Unique name for the model.  creation_timestamp INT64 Timestamp recorded when this registered_model was created.  last_updated_timestamp INT64 Timestamp recorded when metadata for this registered_model was last updated.  user_id STRING User that created this registered_model NOTE: this field is not currently returned.  description STRING Description of this registered_model.  latest_versions An array of ModelVersion Collection of latest model versions for each stage. Only contains models with current READY status.  tags An array of RegisteredModelTag Tags: Additional metadata key-value pairs for this registered_model.  aliases An array of RegisteredModelAlias Aliases pointing to model versions associated with this registered_model.      RegisteredModelAlias  Alias for a registered model        Field Name Type Description    alias STRING The name of the alias.  version STRING The model version number that the alias points to.      RegisteredModelTag  Tag for a registered model        Field Name Type Description    key STRING The tag key.  value STRING The tag value.      Run  A single run.        Field Name Type Description    info RunInfo Run metadata.  data RunData Run data.  inputs RunInputs Run inputs.      RunData  Run data (metrics, params, and tags).        Field Name Type Description    metrics An array of Metric Run metrics.  params An array of Param Run parameters.  tags An array of RunTag Additional metadata key-value pairs.      RunInfo  Metadata of a single run.        Field Name Type Description    run_id STRING Unique identifier for the run.  run_uuid STRING [Deprecated, use run_id instead] Unique identifier for the run. This field will be removed in a future MLflow version.  run_name STRING The name of the run.  experiment_id STRING The experiment ID.  user_id STRING User who initiated the run. This field is deprecated as of MLflow 1.0, and will be removed in a future MLflow release. Use ‘mlflow.user’ tag instead.  status RunStatus Current status of the run.  start_time INT64 Unix timestamp of when the run started in milliseconds.  end_time INT64 Unix timestamp of when the run ended in milliseconds.  artifact_uri STRING URI of the directory where artifacts should be uploaded. This can be a local path (starting with “/”), or a distributed file system (DFS) path, like s3://bucket/directory or dbfs:/my/directory. If not set, the local ./mlruns directory is  chosen.  lifecycle_stage STRING Current life cycle stage of the experiment : OneOf(“active”, “deleted”)      RunInputs  Run inputs.        Field Name Type Description    dataset_inputs An array of DatasetInput Dataset inputs to the Run.      RunTag  Tag for a run.        Field Name Type Description    key STRING The tag key.  value STRING The tag value.      ModelVersionStatus        Name Description    PENDING_REGISTRATION Request to register a new model version is pending as server performs background tasks.  FAILED_REGISTRATION Request to register a new model version has failed.  READY Model version is ready for use.      RunStatus  Status of a run.       Name Description    RUNNING Run has been initiated.  SCHEDULED Run is scheduled to run at a later time.  FINISHED Run has completed.  FAILED Run execution failed.  KILLED Run killed by user.      ViewType  View type for ListExperiments query.       Name Description    ACTIVE_ONLY Default. Return only active experiments.  DELETED_ONLY Return only deleted experiments.  ALL Get all experiments.            Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
docker.html,"   Documentation  Official MLflow Docker Image       Official MLflow Docker Image  The official MLflow Docker image is available on GitHub Container Registry at https://ghcr.io/mlflow/mlflow. export CR_PAT=YOUR_TOKEN echo $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin # Pull the latest version docker pull ghcr.io/mlflow/mlflow # Pull 2.0.1 docker pull ghcr.io/mlflow/mlflow:v2.0.1         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
community-model-flavors.html,"   Documentation  Community Model Flavors       Community Model Flavors  Other useful MLflow flavors are developed and maintained by the MLflow community, enabling you to use MLflow Models with an even broader ecosystem of machine learning libraries. For more information, check out the description of each community-developed flavor below.   MLflow VizMod BigML (bigmlflow) Sktime MLflavors    MLflow VizMod  The mlflow-vizmod project allows data scientists to be more productive with their visualizations. We treat visualizations as models - just like ML models - thus being able to use the same infrastructure as MLflow to track, create projects, register, and deploy visualizations. Installation: pip install mlflow-vizmod   Example: from sklearn.datasets import load_iris import altair as alt import mlflow_vismod  df_iris = load_iris(as_frame=True)  viz_iris = (     alt.Chart(df_iris)     .mark_circle(size=60)     .encode(x=""x"", y=""y"", color=""z:N"")     .properties(height=375, width=575)     .interactive() )  mlflow_vismod.log_model(     model=viz_iris,     artifact_path=""viz"",     style=""vegalite"",     input_example=df_iris.head(5), )     BigML (bigmlflow)  The bigmlflow library implements the bigml model flavor. It enables using BigML supervised models and offers the save_model(), log_model() and load_model() methods.  Installing bigmlflow  BigMLFlow can be installed from PyPI as follows: pip install bigmlflow     BigMLFlow usage  The bigmlflow module defines the flavor that implements the save_model() and log_model() methods. They can be used to save BigML models and their related information in MLflow Model format. import json import mlflow import bigmlflow  MODEL_FILE = ""logistic_regression.json"" with mlflow.start_run():     with open(MODEL_FILE) as handler:         model = json.load(handler)         bigmlflow.log_model(             model, artifact_path=""model"", registered_model_name=""my_model""         )   These methods also add the python_function flavor to the MLflow Models that they produce, allowing the models to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with DataFrame inputs. # saving the model save_model(model, path=model_path) # retrieving model pyfunc_model = pyfunc.load_model(model_path) pyfunc_predictions = pyfunc_model.predict(dataframe)   You can also use the bigmlflow.load_model() method to load MLflow Models with the bigmlflow model flavor as a BigML SupervisedModel. For more information, see the BigMLFlow documentation and BigML’s blog.    Sktime  The sktime custom model flavor enables logging of sktime models in MLflow format via the save_model() and log_model() methods. These methods also add the python_function flavor to the MLflow Models that they produce, allowing the model to be interpreted as generic Python functions for inference via mlflow.pyfunc.load_model(). This loaded PyFunc model can only be scored with a DataFrame input. You can also use the load_model() method to load MLflow Models with the sktime model flavor in native sktime formats.  Installing Sktime  Install sktime with mlflow dependency: pip install sktime[mlflow]     Usage example  Refer to the sktime mlflow documentation for details on the interface for utilizing sktime models loaded as a pyfunc type and an example notebook for extended code usage examples. import pandas as pd  from sktime.datasets import load_airline from sktime.forecasting.arima import AutoARIMA from sktime.utils import mlflow_sktime  airline = load_airline() model_path = ""model""   auto_arima_model = AutoARIMA(sp=12, d=0, max_p=2, max_q=2, suppress_warnings=True).fit(     airline, fh=[1, 2, 3] )  mlflow_sktime.save_model(     sktime_model=auto_arima_model,     path=model_path, )  loaded_model = mlflow_sktime.load_model(     model_uri=model_path, ) loaded_pyfunc = mlflow_sktime.pyfunc.load_model(     model_uri=model_path, )  print(loaded_model.predict()) print(loaded_pyfunc.predict(pd.DataFrame()))      MLflavors  The MLflavors package adds MLflow support for some popular machine learning frameworks currently not considered for inclusion as MLflow built-in flavors. Similar to the built-in flavors, you can use this package to save your model as an MLflow artifact, load your model from MLflow for batch inference, and deploy your model to a serving endpoint using MLflow deployment tools. The following open-source libraries are currently supported:         Framework Tutorials Category  Orbit MLflow-Orbit Time Series Forecasting  Sktime MLflow-Sktime Time Series Forecasting  StatsForecast MLflow-StatsForecast Time Series Forecasting  PyOD MLflow-PyOD Anomaly Detection  SDV MLflow-SDV Synthetic Data Generation     The interface design for the supported frameworks is similar to many of the existing built-in flavors. Particularly, the interface for utilizing the custom model loaded as a pyfunc flavor for generating predictions uses a single-row Pandas DataFrame configuration argument to expose the parameters of the flavor’s inference API.  Documentation  Usage examples for all flavors and the API reference can be found in the package documenation.   Installation  Installing from PyPI: $ pip install mlflavors     Quickstart  This example trains a PyOD KNN outlier detection model using a synthetic dataset. A new MLflow experiment is created to log the evaluation metrics and the trained model as an artifact and anomaly scores are computed loading the trained model in native flavor and pyfunc flavor. Finally, the model is served for real-time inference using a local endpoint.  Saving the model as an MLflow artifact  import json  import mlflow import pandas as pd from pyod.models.knn import KNN from pyod.utils.data import generate_data from sklearn.metrics import roc_auc_score  import mlflavors  ARTIFACT_PATH = ""model""  with mlflow.start_run() as run:     contamination = 0.1  # percentage of outliers     n_train = 200  # number of training points     n_test = 100  # number of testing points      X_train, X_test, _, y_test = generate_data(         n_train=n_train, n_test=n_test, contamination=contamination     )      # Train kNN detector     clf = KNN()     clf.fit(X_train)      # Evaluate model     y_test_scores = clf.decision_function(X_test)      metrics = {         ""roc"": roc_auc_score(y_test, y_test_scores),     }      print(f""Metrics: \n{json.dumps(metrics, indent=2)}"")      # Log metrics     mlflow.log_metrics(metrics)      # Log model using pickle serialization (default).     mlflavors.pyod.log_model(         pyod_model=clf,         artifact_path=ARTIFACT_PATH,         serialization_format=""pickle"",     )     model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)  # Print the run id wich is used below for serving the model to a local REST API endpoint print(f""\nMLflow run id:\n{run.info.run_id}"")     Loading the model from MLflow  Make a prediction loading the model from MLflow in native format: loaded_model = mlflavors.pyod.load_model(model_uri=model_uri) print(loaded_model.decision_function(X_test))   Make a prediction loading the model from MLflow in pyfunc format: loaded_pyfunc = mlflavors.pyod.pyfunc.load_model(model_uri=model_uri)  # Create configuration DataFrame predict_conf = pd.DataFrame(     [         {             ""X"": X_test,             ""predict_method"": ""decision_function"",         }     ] )  print(loaded_pyfunc.predict(predict_conf)[0])     Serving the model using an endpoint  To serve the model using a local REST API endpoint run the command below where you substitute the run id printed above: mlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1   Similarly, you could serve the model using an endpoint in the cloud (e.g. Azure ML, AWS SageMaker, etc.) using MLflow deployment tools. Open a new terminal and run the below model scoring script to request a prediction from the served model: import pandas as pd import requests from pyod.utils.data import generate_data  contamination = 0.1  # percentage of outliers n_train = 200  # number of training points n_test = 100  # number of testing points  _, X_test, _, _ = generate_data(     n_train=n_train, n_test=n_test, contamination=contamination )  # Define local host and endpoint url host = ""127.0.0.1"" url = f""http://{host}:5000/invocations""  # Convert to list for JSON serialization X_test_list = X_test.tolist()  # Create configuration DataFrame predict_conf = pd.DataFrame(     [         {             ""X"": X_test_list,             ""predict_method"": ""decision_function"",         }     ] )  # Create dictionary with pandas DataFrame in the split orientation json_data = {""dataframe_split"": predict_conf.to_dict(orient=""split"")}  # Score model response = requests.post(url, json=json_data) print(response.json())            Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
tutorials-and-examples/index.html,"   Documentation  Tutorials and Examples       Tutorials and Examples  Below, you can find a number of tutorials and examples for various MLflow use cases.  Hyperparameter Tuning Orchestrating Multistep Workflows Using the MLflow REST API Directly Reproducibly run & share ML code  Packaging Training Code in a Docker Environment Python Package Anti-Tampering   Write & Use MLflow Plugins Instrument ML training code with MLflow  Gluon H2O Keras Prophet PyTorch XGBoost LightGBM Statsmodels Glmnet (R) SpaCy Fastai SHAP Prophet Pmdarima Diviner Transformers LangChain OpenAI Tensorflow SynapseML scikit-learn  Diabetes example Elastic Net example Logistic Regression example   RAPIDS  Random Forest Classifier            Previous               © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#mlflow-a-tool-for-managing-the-machine-learning-lifecycle,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
getting-started/index.html,"   Documentation  Getting Started with MLflow       Getting Started with MLflow  For those new to MLflow or seeking a refresher on its core functionalities, the quickstart tutorials here are the perfect starting point. They will guide you step-by-step through fundamental concepts, focusing purely on a task that will maximize your understanding of how to use MLflow to solve a particular task.  Guidance on Running Tutorials  If you are new to MLflow and have never interfaced with the MLflow Tracking Server, we highly encourage you to head on over to quickly read the guide below. It will help you get started as quickly as possible with tutorial content throughout the documentation.                          Tracking Server Options                                   Learn about your options for running an MLflow Tracking Server for executing any of the guides and tutorials in the MLflow documentation                         Getting Started Guides   MLflow Tracking  MLflow Tracking is one of the primary service components of MLflow. In these guides, you will gain an understanding of what MLflow Tracking can do to enhance your MLOps related activities while building ML models.    In these introductory guides to MLflow Tracking, you will learn how to leverage MLflow to:  Log training statistics (loss, accuracy, etc.) and hyperparameters for a model Log (save) a model for later retrieval Register a model using the MLflow Model Registry to enable deployment Load the model and use it for inference  In the process of learning these key concepts, you will be exposed to the MLflow Tracking APIs, the MLflow Tracking UI, and learn how to add metadata associated with a model training event to an MLflow run.                          MLflow Tracking Quickstart Guide                                   Learn the basics of MLflow Tracking in a fast-paced guide with a focus on seeing your first model in the MLflow UI                                           In-depth Tutorial for MLflow Tracking                                       Learn the nuances of interfacing with the MLflow Tracking Server in an in-depth tutorial                     If you would like to get started immediately by downloading and running a notebook yourself:  Download the Tracking Quickstart Notebook    Autologging Basics  A great way to get started with MLflow is to use the autologging feature. Autologging automatically logs your model, metrics, examples, signature, and parameters with only a single line of code for many of the most popular ML libraries in the Python ecosystem.    In this brief tutorial, you’ll learn how to leverage MLflow’s autologging feature to simplify your model logging activities.                          MLflow Autologging Quickstart                                   Get started with logging to MLflow with the high-level autologging API in a fast-paced guide                       Run Comparison Basics  This quickstart tutorial focuses on the MLflow UI’s run comparison feature and provides a step-by-step walkthrough of registering the best model found from a hyperparameter tuning execution. After locally serving the registered model, a brief example of preparing a model for remote deployment by containerizing the model using Docker is covered.                             MLflow Run Comparison Quickstart                                   Get started with using the MLflow UI to compare runs and register a model for deployment                         Tracking Server Quickstart  This quickstart tutorial walks through different types of MLflow Tracking Servers and how to use them to log your MLflow experiments.                          5 Minute Tracking Server Overview                                    Learn how to log MLflow experiments with different tracking servers                    Model Registry Quickstart  This quickstart tutorial walks through registering a model in the MLflow model registry and how to retrieve registered models.                          5 Minute Model Registry Overview                                    Learn how to log MLflow models to the model registry                     Further Learning - What’s Next?  Now that you have the essentials under your belt, below are some recommended collections of tutorial and guide content that will help to broaden your understanding of MLflow and its APIs.  Tracking - Learn more abou the MLflow tracking APIs by reading the tracking guide. LLMs - Discover how you can leverage cutting-edge advanced LLMs to power your ML applications by reading the LLMs guide. MLflow Deployment - Follow the comprehensive guide on model deployment to learn how to deploy your MLflow models to a variety of deployment targets. Model Registry - Learn about the MLflow Model Registry and how it can help you manage the lifecycle of your ML models. Deep Learning Library Integrations - From PyTorch to TensorFlow and more, learn about the integrated deep learning capabilities in MLflow by reading the deep learning guide. Traditional ML - Learn about the traditional ML capabilities in MLflow and how they can help you manage your traditional ML workflows.           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#id1,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#getting-started-guides-and-quickstarts,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
getting-started/intro-quickstart/index.html,"   Documentation  Getting Started with MLflow  MLflow Tracking Quickstart       MLflow Tracking Quickstart  Welcome to MLflow! The purpose of this quickstart is to provide a quick guide to the most essential core APIs of MLflow Tracking. Specifically, those that enable the logging, registering, and loading of a model for inference.  Note For a more in-depth and tutorial-based approach (if that is your style), please see the Getting Started with MLflow tutorial. We recommend that you start here first, though, as this quickstart uses the most common and frequently-used APIs for MLflow Tracking and serves as a good foundation for the other tutorials in the documentation.   What you will learn  In just a few minutes of following along with this quickstart, you will learn:  How to log parameters, metrics, and a model The basics of the MLflow fluent API How to register a model during logging How to navigate to a model in the MLflow UI How to load a logged model for inference  If you would like to see this quickstart in a purely notebook format, we have a downloadable and viewable notebook-only version of this quickstart: View the Notebook    Step 1 - Get MLflow  MLflow is available on PyPI. If you don’t already have it installed on your system, you can install it with:  pip install mlflow      Step 2 - Start a Tracking Server   Using a Managed MLflow Tracking Server  For details on options for using a managed MLflow Tracking Server, including how to create a free Databricks Community Edition account with managed MLflow, see the guide for tracking server options.   (Optional) Run a local Tracking Server  We’re going to start a local MLflow Tracking Server, which we will connect to for logging our data for this quickstart. From a terminal, run:  mlflow server --host 127.0.0.1 --port 8080     Note You can choose any port that you would like, provided that it’s not already in use.    Set the Tracking Server URI (if not using a Databricks Managed MLflow Tracking Server)  If you’re using a managed MLflow Tracking Server that is not provided by Databricks, or if you’re running a local tracking server, ensure that you set the tracking server’s uri using:  import mlflow  mlflow.set_tracking_uri(uri=""http://<host>:<port>"")    If this is not set within your notebook or runtime environment, the runs will be logged to your local file system.    Step 3 - Train a model and prepare metadata for logging  In this section, we’re going to log a model with MLflow. A quick overview of the steps are:  Load and prepare the Iris dataset for modeling. Train a Logistic Regression model and evaluate its performance. Prepare the model hyperparameters and calculate metrics for logging.   import mlflow from mlflow.models import infer_signature  import pandas as pd from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score   # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True)  # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 )  # Define the model hyperparameters params = {     ""solver"": ""lbfgs"",     ""max_iter"": 1000,     ""multi_class"": ""auto"",     ""random_state"": 8888, }  # Train the model lr = LogisticRegression(**params) lr.fit(X_train, y_train)  # Predict on the test set y_pred = lr.predict(X_test)  # Calculate metrics accuracy = accuracy_score(y_test, y_pred)      Step 4 - Log the model and its metadata to MLflow  In this next step, we’re going to use the model that we trained, the hyperparameters that we specified for the model’s fit, and the loss metrics that were calculated by evaluating the model’s performance on the test data to log to MLflow. The steps that we will take are:  Initiate an MLflow run context to start a new run that we will log the model and metadata to. Log model parameters and performance metrics. Tag the run for easy retrieval. Register the model in the MLflow Model Registry while logging (saving) the model.   Note While it can be valid to wrap the entire code within the start_run block, this is not recommended. If there as in issue with the training of the model or any other portion of code that is unrelated to MLflow-related actions, an empty or partially-logged run will be created, which will necessitate manual cleanup of the invalid run. It is best to keep the training execution outside of the run context block to ensure that the loggable content (parameters, metrics, artifacts, and the model) are fully materialized prior to logging.   # Set our tracking server uri for logging mlflow.set_tracking_uri(uri=""http://127.0.0.1:8080"")  # Create a new MLflow Experiment mlflow.set_experiment(""MLflow Quickstart"")  # Start an MLflow run with mlflow.start_run():     # Log the hyperparameters     mlflow.log_params(params)      # Log the loss metric     mlflow.log_metric(""accuracy"", accuracy)      # Set a tag that we can use to remind ourselves what this run was for     mlflow.set_tag(""Training Info"", ""Basic LR model for iris data"")      # Infer the model signature     signature = infer_signature(X_train, lr.predict(X_train))      # Log the model     model_info = mlflow.sklearn.log_model(         sk_model=lr,         artifact_path=""iris_model"",         signature=signature,         input_example=X_train,         registered_model_name=""tracking-quickstart"",     )      Step 5 - Load the model as a Python Function (pyfunc) and use it for inference  After logging the model, we can perform inference by:  Loading the model using MLflow’s pyfunc flavor. Running Predict on new data using the loaded model.   Note The iris training data that we used was a numpy array structure. However, we can submit a Pandas DataFrame as well to the predict method, as shown below.   # Load the model back for predictions as a generic Python Function model loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)  predictions = loaded_model.predict(X_test)  iris_feature_names = datasets.load_iris().feature_names  result = pd.DataFrame(X_test, columns=iris_feature_names) result[""actual_class""] = y_test result[""predicted_class""] = predictions  result[:4]    The output of this code will look something like this:           sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) actual_class predicted_class    6.1 2.8 4.7 1.2 1 1  5.7 3.8 1.7 0.3 0 0  7.7 2.6 6.9 2.3 2 2  6.0 2.9 4.5 1.5 1 1      Step 6 - View the Run in the MLflow UI  In order to see the results of our run, we can navigate to the MLflow UI. Since we have already started the Tracking Server at http://localhost:8080, we can simply navigate to that URL in our browser. When opening the site, you will see a screen similar to the following:   The main MLflow Tracking page, showing Experiments that have been created   Clicking on the name of the Experiment that we created (“MLflow Quickstart”) will give us a list of runs associated with the Experiment. You should see a random name that has been generated for the run and nothing else show up in the Table list view to the right. Clicking on the name of the run will take you to the Run page, where the details of what we’ve logged will be shown. The elements have been highlighted below to show how and where this data is recorded within the UI.   The run view page for our run     Conclusion  Congratulations on working through the MLflow Tracking Quickstart! You should now have a basic understanding of how to use the MLflow Tracking API to log models. If you are interested in a more in-depth tutorial, please see the Getting Started with MLflow tutorial as a good next step in increasing your knowledge about MLflow!        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
getting-started/logging-first-model/index.html,"   Documentation  Getting Started with MLflow  Tutorial Overview       Tutorial Overview  In this entry point tutorial to MLflow, we’ll be covering the essential basics of core MLflow functionality associated with tracking training event data. We’ll start by learning how to start a local MLflow Tracking server, how to access and view the MLflow UI, and move on to our first interactions with the Tracking server through the use of the MLflow Client. The tutorial content builds upon itself, culminating in successfully logging your first MLflow model. The topics in this tutorial cover:  Starting an MLflow Tracking Server (Optionally) and connecting to a Tracking Server Exploring the MlflowClient API (briefly) Understanding the Default Experiment Searching for Experiments with the MLflow client API Understanding the uses of tags and how to leverage them for model organization Creating an Experiment that will contain our run (and our model) Learning how to log metrics, parameters, and a model artifact to a run Viewing our Experiment and our first run within the MLflow UI  To get started with the tutorial, click NEXT below or navigate to the section that you’re interested in:   Starting the MLflow Tracking Server Using the MLflow Client API Creating Experiments Searching Experiments Create a dataset about apples Logging our first runs with MLflow Logging Your First MLflow Model Notebook   If you would instead like to download a notebook-based version of this guide and follow along locally, you can download the notebook from the link below.  Download the Notebook      Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
tracking/autolog.html,"   Documentation  MLflow Tracking  MLflow Tracking APIs  Automatic Logging with MLflow Tracking       Automatic Logging with MLflow Tracking  Auto logging is a powerful feature that allows you to log metrics, parameters, and models without the need for explicit log statements. All you need to do is to call mlflow.autolog() before your training code. import mlflow  mlflow.autolog()  with mlflow.start_run():     # your training code goes here     ...   This will enable MLflow to automatically log various information about your run, including:  Metrics - MLflow pre-selects a set of metrics to log, based on what model and library you use Parameters - hyper params specified for the training, plus default values provided by the library if not explicitly set Model Signature - logs Model signature instance, which describes input and output schema of the model Artifacts -  e.g. model checkpoints Dataset - dataset object used for training (if applicable), such as tensorflow.data.Dataset   How to Get started   Step 1 - Get MLflow  MLflow is available on PyPI. If you don’t already have it installed on your system, you can install it with:  pip install mlflow      Step 2 - Insert mlflow.autolog in Your Code  For example, following code snippet shows how to enable autologging for a scikit-learn model: import mlflow  from sklearn.model_selection import train_test_split from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor  mlflow.autolog()  db = load_diabetes() X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)  rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3) # MLflow triggers logging automatically upon model fitting rf.fit(X_train, y_train)     Step 3 - Execute Your Code   python YOUR_ML_CODE.py      Step 4 - View Your Results in the MLflow UI  Once your training job finishes, you can run following command to launch the MLflow UI:  mlflow ui --port 8080    Then, navigate to http://localhost:8080 in your browser to view the results.    Customize Autologging Behavior  You can also control the behavior of autologging by passing arguments to mlflow.autolog() function. For example, you can disable logging of model checkpoints and assosiate tags with your run as follows: import mlflow  mlflow.autolog(     log_model_signatures=False,     extra_tags={""YOUR_TAG"": ""VALUE""}, )   See mlflow.autolog() for the full set of arguments you can use.  Enable / Disable Autologging for Specific Libraries  One common use case is to enable/disable autologging for a specific library. For example, if you train your model on PyTorch but use scikit-learn for data preprocessing, you may want to disable autologging for scikit-learn while keeping it enabled for PyTorch. You can achieve this by either (1) enable autologging only for PyTorch using PyTorch flavor (2) disable autologging for scikit-learn using its flavor with disable=True. import mlflow  # Option 1: Enable autologging only for PyTorch mlflow.pytorch.autolog()  # Option 2: Disable autologging for scikit-learn, but enable it for other libraries mlflow.sklearn.autolog(disable=True) mlflow.autolog()      Supported Libraries   Note The generic autolog function mlflow.autolog() enables autologging for each supported library you have installed as soon as you import it. Alternatively, you can use library-specific autolog calls such as mlflow.pytorch.autolog() to explicitly enable (or disable) autologging for a particular library.  The following libraries support autologging:   Fastai Gluon Keras LightGBM PyTorch Scikit-learn Spark Statsmodels XGBoost   For flavors that automatically save models as an artifact, additional files for dependency management are logged.  Fastai  Call the generic autolog function mlflow.fastai.autolog() before your training code to enable automatic logging of metrics and parameters. See an example usage with Fastai. Autologging captures the following information:          Framework Metrics Parameters Tags Artifacts  fastai user-specified metrics Logs optimizer data as parameters. For example, epochs, lr, opt_func, etc; Logs the parameters of the EarlyStoppingCallback and OneCycleScheduler callbacks – Model checkpoints are logged to a ‘models’ directory; MLflow Model (fastai Learner model) on training end; Model summary text is logged      Gluon  Call the generic autolog function mlflow.gluon.autolog() before your training code to enable automatic logging of metrics and parameters. See example usages with Gluon . Autologging captures the following information:          Framework Metrics Parameters Tags Artifacts  Gluon Training loss; validation loss; user-specified metrics Number of layers; optimizer name; learning rate; epsilon – MLflow Model (Gluon model); on training end      Keras  Call the generic autolog function or mlflow.tensorflow.autolog() before your training code to enable automatic logging of metrics and parameters. As an example, try running the Keras/Tensorflow example. Note that only versions of tensorflow>=2.3 are supported. The respective metrics associated with tf.estimator and EarlyStopping are automatically logged. As an example, try running the Keras/TensorFlow example. Autologging captures the following information:          Framework/module Metrics Parameters Tags Artifacts  tf.keras Training loss; validation loss; user-specified metrics fit() parameters; optimizer name; learning rate; epsilon – Model summary on training start; MLflow Model (Keras model); TensorBoard logs on training end  tf.keras.callbacks.EarlyStopping Metrics from the EarlyStopping callbacks. For example, stopped_epoch, restored_epoch, restore_best_weight, etc fit() parameters from EarlyStopping. For example, min_delta, patience, baseline, restore_best_weights, etc – –    If no active run exists when autolog() captures data, MLflow will automatically create a run to log information to. Also, MLflow will then automatically end the run once training ends via calls to tf.keras.fit(). If a run already exists when autolog() captures data, MLflow will log to that run but not automatically end that run after training. You will have to manually stop the run if you wish to start a new run context for logging to a new run.   LightGBM  Call the generic autolog function mlflow.lightgbm.autolog() before your training code to enable automatic logging of metrics and parameters. Autologging captures the following information:          Framework Metrics Parameters Tags Artifacts  LightGBM user-specified metrics lightgbm.train parameters – MLflow Model (LightGBM model) with model signature on training end; feature importance; input example    If early stopping is activated, metrics at the best iteration will be logged as an extra step/iteration.   PyTorch  Call the generic autolog function mlflow.pytorch.autolog() before your PyTorch Lightning training code to enable automatic logging of metrics, parameters, and models. See example usages here. Note that currently, PyTorch autologging supports only models trained using PyTorch Lightning. Autologging is triggered on calls to pytorch_lightning.trainer.Trainer.fit and captures the following information:          Framework/module Metrics Parameters Tags Artifacts  pytorch_lightning.trainer.Trainer Training loss; validation loss; average_test_accuracy; user-defined-metrics. fit() parameters; optimizer name; learning rate; epsilon. – Model summary on training start, MLflow Model (PyTorch model) on training end;  pytorch_lightning.callbacks.earlystopping Training loss; validation loss; average_test_accuracy; user-defined-metrics. Metrics from the EarlyStopping callbacks. For example, stopped_epoch, restored_epoch, restore_best_weight, etc. fit() parameters; optimizer name; learning rate; epsilon Parameters from the EarlyStopping callbacks. For example, min_delta, patience, baseline,``restore_best_weights``, etc – Model summary on training start; MLflow Model (PyTorch model) on training end; Best PyTorch model checkpoint, if training stops due to early stopping callback.    If no active run exists when autolog() captures data, MLflow will automatically create a run to log information, ending the run once the call to pytorch_lightning.trainer.Trainer.fit() completes. If a run already exists when autolog() captures data, MLflow will log to that run but not automatically end that run after training.  Note  Parameters not explicitly passed by users (parameters that use default values) while using pytorch_lightning.trainer.Trainer.fit() are not currently automatically logged In case of a multi-optimizer scenario (such as usage of autoencoder), only the parameters for the first optimizer are logged     Scikit-learn  Call mlflow.sklearn.autolog() before your training code to enable automatic logging of sklearn metrics, params, and models. See example usage here. Autologging for estimators (e.g. LinearRegression) and meta estimators (e.g. Pipeline) creates a single run and logs:         Metrics Parameters Tags Artifacts  Training score obtained by estimator.score Parameters obtained by estimator.get_params  Class name Fully qualified class name   Fitted estimator    Autologging for parameter search estimators (e.g. GridSearchCV) creates a single parent run and nested child runs - Parent run   - Child run 1   - Child run 2   - ...   containing the following data:          Run type Metrics Parameters Tags Artifacts  Parent Training score  Parameter search estimator’s parameters Best parameter combination    Class name Fully qualified class name    Fitted parameter search estimator Fitted best estimator Search results csv file    Child CV test score for each parameter combination Each parameter combination  Class name Fully qualified class name   –      Spark  Initialize a SparkSession with the mlflow-spark JAR attached (e.g. SparkSession.builder.config(""spark.jars.packages"", ""org.mlflow.mlflow-spark"")) and then call the generic autolog function mlflow.spark.autolog() to enable automatic logging of Spark datasource information at read-time, without the need for explicit log statements. Note that autologging of Spark ML (MLlib) models is not yet supported. Autologging captures the following information:          Framework Metrics Parameters Tags Artifacts  Spark – – Single tag containing source path, version, format. The tag contains one line per datasource –     Note  Moreover, Spark datasource autologging occurs asynchronously - as such, it’s possible (though unlikely) to see race conditions when launching short-lived MLflow runs that result in datasource information not being logged.    Important With Pyspark 3.2.0 or above, Spark datasource autologging requires PYSPARK_PIN_THREAD environment variable to be set to false.    Statsmodels  Call the generic autolog function mlflow.statsmodels.autolog() before your training code to enable automatic logging of metrics and parameters. Autologging captures the following information:          Framework Metrics Parameters Tags Artifacts  Statsmodels user-specified metrics statsmodels.base.model.Model.fit parameters – MLflow Model (statsmodels.base.wrapper.ResultsWrapper) on training end     Note  Each model subclass that overrides fit expects and logs its own parameters.     XGBoost  Call the generic autolog function mlflow.xgboost.autolog() before your training code to enable automatic logging of metrics and parameters. Autologging captures the following information:          Framework Metrics Parameters Tags Artifacts  XGBoost user-specified metrics xgboost.train parameters – MLflow Model (XGBoost model) with model signature on training end; feature importance; input example    If early stopping is activated, metrics at the best iteration will be logged as an extra step/iteration.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
getting-started/quickstart-2/index.html,"   Documentation  Getting Started with MLflow  Quickstart: Compare runs, choose a model, and deploy it to a REST API       Quickstart: Compare runs, choose a model, and deploy it to a REST API  In this quickstart, you will:  Run a hyperparameter sweep on a training script Compare the results of the runs in the MLflow UI Choose the best run and register it as a model Deploy the model to a REST API Build a container image suitable for deployment to a cloud platform  As an ML Engineer or MLOps professional, you can use MLflow to compare, share, and deploy the best models produced by the team. In this quickstart, you will use the MLflow Tracking UI to compare the results of a hyperparameter sweep, choose the best run, and register it as a model. Then, you will deploy the model to a REST API. Finally, you will create a Docker container image suitable for deployment to a cloud platform.   Set up  For a comprehensive guide on getting an MLflow environment setup that will give you options on how to configure MLflow tracking capabilities, you can read the guide here.   Run a hyperparameter sweep  This example tries to optimize the RMSE metric of a Keras deep learning model on a wine quality dataset. It has two hyperparameters that it tries to optimize: learning_rate and momentum. We will use the Hyperopt library to run a hyperparameter sweep across different values of learning_rate and momentum and record the results in MLflow. Before running the hyperparameter sweep, let’s set the MLFLOW_TRACKING_URI environment variable to the URI of our MLflow tracking server: export MLFLOW_TRACKING_URI=http://localhost:5000    Note If you would like to explore the possibilities of other tracking server deployments, including a fully-managed free-of-charge solution with Databricks Community Edition, please see this page.  Import the following packages import keras import numpy as np import pandas as pd from hyperopt import STATUS_OK, Trials, fmin, hp, tpe from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split  import mlflow from mlflow.models import infer_signature   Now load the dataset and split it into training, validation, and test sets. # Load dataset data = pd.read_csv(     ""https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv"",     sep="";"", )  # Split the data into training, validation, and test sets train, test = train_test_split(data, test_size=0.25, random_state=42) train_x = train.drop([""quality""], axis=1).values train_y = train[[""quality""]].values.ravel() test_x = test.drop([""quality""], axis=1).values test_y = test[[""quality""]].values.ravel() train_x, valid_x, train_y, valid_y = train_test_split(     train_x, train_y, test_size=0.2, random_state=42 ) signature = infer_signature(train_x, train_y)   Then let’s define the model architecture and train the model. The train_model function uses MLflow to track the parameters, results, and model itself of each trial as a child run. def train_model(params, epochs, train_x, train_y, valid_x, valid_y, test_x, test_y):     # Define model architecture     model = keras.Sequential(         [             keras.Input([train_x.shape[1]]),             keras.layers.Normalization(mean=np.mean(train_x), variance=np.var(train_x)),             keras.layers.Dense(64, activation=""relu""),             keras.layers.Dense(1),         ]     )      # Compile model     model.compile(         optimizer=keras.optimizers.SGD(             learning_rate=params[""lr""], momentum=params[""momentum""]         ),         loss=""mean_squared_error"",         metrics=[keras.metrics.RootMeanSquaredError()],     )      # Train model with MLflow tracking     with mlflow.start_run(nested=True):         model.fit(             train_x,             train_y,             validation_data=(valid_x, valid_y),             epochs=epochs,             batch_size=64,         )         # Evaluate the model         eval_result = model.evaluate(valid_x, valid_y, batch_size=64)         eval_rmse = eval_result[1]          # Log parameters and results         mlflow.log_params(params)         mlflow.log_metric(""eval_rmse"", eval_rmse)          # Log model         mlflow.tensorflow.log_model(model, ""model"", signature=signature)          return {""loss"": eval_rmse, ""status"": STATUS_OK, ""model"": model}   The objective function takes in the hyperparameters and returns the results of the train_model function for that set of hyperparameters. def objective(params):     # MLflow will track the parameters and results for each run     result = train_model(         params,         epochs=3,         train_x=train_x,         train_y=train_y,         valid_x=valid_x,         valid_y=valid_y,         test_x=test_x,         test_y=test_y,     )     return result   Next, we will define the search space for Hyperopt. In this case, we want to try different values of learning-rate and momentum. Hyperopt begins its optimization process by selecting an initial set of hyperparameters, typically chosen at random or based on a specified domain space. This domain space defines the range and distribution of possible values for each hyperparameter. After evaluating the initial set, Hyperopt uses the results to update its probabilistic model, guiding the selection of subsequent hyperparameter sets in a more informed manner, aiming to converge towards the optimal solution. space = {     ""lr"": hp.loguniform(""lr"", np.log(1e-5), np.log(1e-1)),     ""momentum"": hp.uniform(""momentum"", 0.0, 1.0), }   Finally, we will run the hyperparameter sweep using Hyperopt, passing in the objective function and search space. Hyperopt will try different hyperparameter combinations and return the results of the best one. We will store the best parameters, model, and evaluation metrics in MLflow. mlflow.set_experiment(""/wine-quality"") with mlflow.start_run():     # Conduct the hyperparameter search using Hyperopt     trials = Trials()     best = fmin(         fn=objective,         space=space,         algo=tpe.suggest,         max_evals=8,         trials=trials,     )      # Fetch the details of the best run     best_run = sorted(trials.results, key=lambda x: x[""loss""])[0]      # Log the best parameters, loss, and model     mlflow.log_params(best)     mlflow.log_metric(""eval_rmse"", best_run[""loss""])     mlflow.tensorflow.log_model(best_run[""model""], ""model"", signature=signature)      # Print out the best parameters and corresponding loss     print(f""Best parameters: {best}"")     print(f""Best eval rmse: {best_run['loss']}"")     Compare the results  Open the MLflow UI in your browser at the MLFLOW_TRACKING_URI. You should see a nested list of runs. In the default Table view, choose the Columns button and add the Metrics | test_rmse column and the Parameters | lr and Parameters | momentum column. To sort by RMSE ascending, click the test_rmse column header. The best run typically has an RMSE on the test dataset of ~0.70. You can see the parameters of the best run in the Parameters column.  Choose Chart view. Choose the Parallel coordinates graph and configure it to show the lr and momentum coordinates and the test_rmse metric. Each line in this graph represents a run and associates each hyperparameter evaluation run’s parameters to the evaluated error metric for the run. The red graphs on this graph are runs that fared poorly. The lowest one is a baseline run with both lr and momentum set to 0.0. That baseline run has an RMSE of ~0.89. The other red lines show that high momentum can also lead to poor results with this problem and architecture. The graphs shading towards blue are runs that fared better. Hover your mouse over individual runs to see their details.   Register your best model  Choose the best run and register it as a model. In the Table view, choose the best run. In the Run Detail page, open the Artifacts section and select the Register Model button. In the Register Model dialog, enter a name for the model, such as wine-quality, and click Register. Now, your model is available for deployment. You can see it in the Models page of the MLflow UI. Open the page for the model you just registered. You can add a description for the model, add tags, and easily navigate back to the source run that generated this model. You can also transition the model to different stages. For example, you can transition the model to Staging to indicate that it is ready for testing. You can transition it to Production to indicate that it is ready for deployment. Transition the model to Staging by choosing the Stage dropdown:    Serve the model locally  MLflow allows you to easily serve models produced by any run or model version. You can serve the model you just registered by running: mlflow models serve -m ""models:/wine-quality/1"" --port 5002   (Note that specifying the port as above will be necessary if you are running the tracking server on the same machine at the default port of 5000.) You could also have used a runs:/<run_id> URI to serve a model, or any supported URI described in Artifact Store. Please note that for production, we do not recommend deploying your model in the same VM as the tracking server because of resource limitation, within this guide we just run everything from the same machine for simplicity. To test the model, you can send a request to the REST API using the curl command: curl -d '{""dataframe_split"": { ""columns"": [""fixed acidity"",""volatile acidity"",""citric acid"",""residual sugar"",""chlorides"",""free sulfur dioxide"",""total sulfur dioxide"",""density"",""pH"",""sulphates"",""alcohol""], ""data"": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]}}' \ -H 'Content-Type: application/json' -X POST localhost:5002/invocations   Inferencing is done with a JSON POST request to the invocations path on localhost at the specified port. The columns key specifies the names of the columns in the input data. The data value is a list of lists, where each inner list is a row of data. For brevity, the above only requests one prediction of wine quality (on a scale of 3-8). The response is a JSON object with a predictions key that contains a list of predictions, one for each row of data. In this case, the response is: {""predictions"": [{""0"": 5.310967445373535}]}   The schema for input and output is available in the MLflow UI in the Artifacts | Model description. The schema is available because the train.py script used the mlflow.infer_signature method and passed the result to the mlflow.log_model method. Passing the signature to the log_model method is highly recommended, as it provides clear error messages if the input request is malformed.   Build a container image for your model  Most routes toward deployment will use a container to package your model, its dependencies, and relevant portions of the runtime environment. You can use MLflow to build a Docker image for your model. mlflow models build-docker --model-uri ""models:/wine-quality/1"" --name ""qs_mlops""   This command builds a Docker image named qs_mlops that contains your model and its dependencies. The model-uri in this case specifies a version number (/1) rather than a lifecycle stage (/staging), but you can use whichever integrates best with your workflow. It will take several minutes to build the image. Once it completes, you can run the image to provide real-time inferencing locally, on-prem, on a bespoke Internet server, or cloud platform. You can run it locally with: docker run -p 5002:8080 qs_mlops   This Docker run command runs the image you just built and maps port 5002 on your local machine to port 8080 in the container. You can now send requests to the model using the same curl command as before: curl -d '{""dataframe_split"": {""columns"": [""fixed acidity"",""volatile acidity"",""citric acid"",""residual sugar"",""chlorides"",""free sulfur dioxide"",""total sulfur dioxide"",""density"",""pH"",""sulphates"",""alcohol""], ""data"": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]}}' -H 'Content-Type: application/json' -X POST localhost:5002/invocations     Deploying to a cloud platform  Virtually all cloud platforms allow you to deploy a Docker image. The process varies considerably, so you will have to consult your cloud provider’s documentation for details. In addition, some cloud providers have built-in support for MLflow. For instance:  Azure ML Databricks Amazon SageMaker Google Cloud  all support MLflow. Cloud platforms generally support multiple workflows for deployment: command-line, SDK-based, and Web-based. You can use MLflow in any of these workflows, although the details will vary between platforms and versions. Again, you will need to consult your cloud provider’s documentation for details.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/index.html,"   Documentation  LLMs       LLMs  LLMs, or Large Language Models, have rapidly become a cornerstone in the machine learning domain, offering immense capabilities ranging from natural language understanding to code generation and more. However, harnessing the full potential of LLMs often involves intricate processes, from interfacing with multiple providers to fine-tuning specific models to achieve desired outcomes. Such complexities can easily become a bottleneck for developers and data scientists aiming to integrate LLM capabilities into their applications. MLflow’s Support for LLMs aims to alleviate these challenges by introducing a suite of features and tools designed with the end-user in mind:  MLflow Deployments Server for LLMs    Serving as a unified interface, the MLflow Deployments Server (previously known as “MLflow AI Gateway”) simplifies interactions with multiple LLM providers. In addition to supporting the most popular SaaS LLM providers, the MLflow Deployments Server provides an integration to MLflow model serving, allowing you to serve your own LLM or a fine-tuned foundation model within your own serving infrastructure.  Note The MLflow Deployments Server is in active development and has been marked as Experimental. APIs may change as this new feature is refined and its functionality is expanded based on feedback.   Benefits of the MLflow Deployments Server   Unified Endpoint: No more juggling between multiple provider APIs. Simplified Integrations: One-time setup, no repeated complex integrations. Secure Credential Management:  Centralized storage prevents scattered API keys. No hardcoding or user-handled keys.   Consistent API Experience:  Uniform API across all providers. Easy-to-use REST endpoints and Client API.   Seamless Provider Swapping:  Swap providers without touching your code. Zero downtime provider, model, or route swapping.      Explore the Native Providers of the MLflow Deployments Server  The MLflow Deployments Server supports a large range of foundational models from popular SaaS model vendors, as well as providing a means of self-hosting your own open source model via an integration with MLflow model serving. Please refer to Supported Provider Models for the full list of supported providers and models. If you’re interested in learning about how to set up the MLflow Deployments Server for a specific provider, follow the links below for our up-to-date documentation on GitHub. Each link will take you to a README file that will explain how to set up a route for the provider. In the same directory as the README, you will find a runnable example of how to query the routes that the example creates, providing you with a quick reference for getting started with your favorite provider!                                                       Note The MLflow and Hugging Face TGI providers are for self-hosted LLM serving of either foundation open-source LLM models, fine-tuned open-source LLM models, or your own custom LLM. The example documentation for these providers will show you how to get started with these, using free-to-use open-source models from the Hugging Face Hub.     LLM Evaluation    Navigating the vast landscape of Large Language Models (LLMs) can be daunting. Determining the right model, prompt, or service that aligns with a project’s needs is no small feat. Traditional machine learning evaluation metrics often fall short when it comes to assessing the nuanced performance of generative models. Enter MLflow LLM Evaluation. This feature is designed to simplify the evaluation process, offering a streamlined approach to compare foundational models, providers, and prompts.  Benefits of MLflow’s LLM Evaluation   Simplified Evaluation: Navigate the LLM space with ease, ensuring the best fit for your project with standard metrics that can be used to compare generated text. Use-Case Specific Metrics: Leverage MLflow’s mlflow.evaluate() API for a high-level, frictionless evaluation experience. Customizable Metrics: Beyond the provided metrics, MLflow supports a plugin-style for custom scoring, enhancing the evaluation’s flexibility. Comparative Analysis: Effortlessly compare foundational models, providers, and prompts to make informed decisions. Deep Insights: Dive into the intricacies of generative models with a comprehensive suite of LLM-relevant metrics.  MLflow’s LLM Evaluation is designed to bridge the gap between traditional machine learning evaluation and the unique challenges posed by LLMs.    Prompt Engineering UI    Effective utilization of LLMs often hinges on crafting the right prompts. The development of a high-quality prompt is an iterative process of trial and error, where subsequent experimentation is not guaranteed to result in cumulative quality improvements. With the volume and speed of iteration through prompt experimentation, it can quickly become very overwhelming to remember or keep a history of the state of different prompts that were tried. Serving as a powerful tool for prompt engineering, the MLflow Prompt Engineering UI revolutionizes the way developers interact with and refine LLM prompts.  Benefits of the MLflow Prompt Engineering UI   Iterative Development: Streamlined process for trial and error without the overwhelming complexity. UI-Based Prototyping: Prototype, iterate, and refine prompts without diving deep into code. Accessible Engineering: Makes prompt engineering more user-friendly, speeding up experimentation. Optimized Configurations: Quickly hone in on the best model configurations for tasks like question answering or document summarization. Transparent Tracking:  Every model iteration and configuration is meticulously tracked. Ensures reproducibility and transparency in your development process.     Note The MLflow Prompt Engineering UI is in active development and has been marked as Experimental. Features and interfaces may evolve as feedback is gathered and the tool is refined.     Native MLflow Flavors for LLMs  Harnessing the power of LLMs becomes effortless with flavors designed specifically for working with LLM libraries and frameworks.  Native Support for Popular Packages: Standardized interfaces for tasks like saving, logging, and managing inference configurations. PyFunc Compatibility:  Load models as PyFuncs for broad compatibility across serving infrastructures. Strengthens the MLOps process for LLMs, ensuring smooth deployments.   Cohesive Ecosystem:  All essential tools and functionalities consolidated under MLflow. Focus on deriving value from LLMs without getting bogged down by interfacing and optimization intricacies.     Explore the Native LLM Flavors  Select the integration below to read the documentation on how to leverage MLflow’s native integration with these popular libraries:                             Learn about MLflow's native integration with the Transformers 🤗 library and see example notebooks that leverage                     MLflow and Transformers to build Open-Source LLM powered solutions.                                              Learn about MLflow's native integration with the OpenAI SDK and see example notebooks that leverage                     MLflow and OpenAI's advanced LLMs to build interesting and fun applications.                                              Learn about MLflow's native integration with the Sentence Transformers library and see example notebooks that leverage                     MLflow and Sentence Transformers to perform operations with encoded text such as semantic search, text similarity, and information retrieval.                                              Learn about MLflow's native integration with LangChain and see example notebooks that leverage                     MLflow and LangChain to build LLM-backed applications.                          LLM Tracking in MLflow    Empowering developers with advanced tracking capabilities, the MLflow LLM Tracking System stands out as the premier solution for managing and analyzing interactions with Large Language Models (LLMs).  Benefits of the MLflow LLM Tracking System   Robust Interaction Management: Comprehensive tracking of every LLM interaction for maximum insight. Tailor-Made for LLMs:  Unique features specifically designed for LLMs. From logging prompts to tracking dynamic data, MLflow has it covered.   Deep Model Insight:  Introduces ‘predictions’ as a core entity, alongside the existing artifacts, parameters, and metrics. Gain unparalleled understanding of text-generating model behavior and performance.   Clarity and Repeatability:  Ensures consistent and transparent tracking across all LLM interactions. Facilitates informed decision-making and optimization in LLM deployment and utilization.       Tutorials and Use Case Guides for LLMs in MLflow  Interested in learning how to leverage MLflow for your LLM projects? Look in the tutorials and guides below to learn more about interesting use cases that could help to make your journey into leveraging LLMs a bit easier! Note that there are additional tutorials within the “Explore the Native LLM Flavors” section above, so be sure to check those out as well!                            Evaluating LLMs                                       Learn how to evaluate LLMs with MLflow.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging, customizing, and deploying advanced LLMs in MLflow using custom PyFuncs.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                              Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#id2,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#llm-guides-and-tutorials,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#mlflow-native-flavors-for-genai,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/transformers/index.html,"   Documentation  LLMs  MLflow Transformers Flavor       MLflow Transformers Flavor   Attention The transformers flavor is in active development and is marked as Experimental. Public APIs may change and new features are subject to be added as additional functionality is brought to the flavor.   Introduction  Transformers by 🤗 Hugging Face represents a cornerstone in the realm of machine learning, offering state-of-the-art capabilities for a multitude of frameworks including PyTorch, TensorFlow, and JAX. This library has become the de facto standard for natural language processing (NLP) and audio transcription processing. It also provides a compelling and advanced set of options for computer vision and multimodal AI tasks. Transformers achieves all of this by providing pre-trained models and accessible high-level APIs that are not only powerful but also versatile and easy to implement. For instance, one of the cornerstones of the simplicity of the transformers library is the pipeline API, an encapsulation of the most common NLP tasks into a single API call. This API allows users to perform a variety of tasks based on the specified task without having to worry about the underlying model or the preprocessing steps.   Transformers Pipeline Architecture for the Whisper Model   The integration of the Transformers library with MLflow enhances the management of machine learning workflows, from experiment tracking to model deployment. This combination offers a robust and efficient pathway for incorporating advanced NLP and AI capabilities into your applications. Key Features of the Transformers Library:  Access to Pre-trained Models: A vast collection of pre-trained models for various tasks, minimizing training time and resources. Task Versatility: Support for multiple modalities including text, image, and speech processing tasks. Framework Interoperability: Compatibility with PyTorch, TensorFlow, JAX, ONNX, and TorchScript. Community Support: An active community for collaboration and support, accessible via forums and the Hugging Face Hub.  MLflow’s Transformers Flavor: MLflow supports the use of the Transformers package by providing:  Simplified Experiment Tracking: Efficient logging of parameters, metrics, and models during the fine-tuning process. Effortless Model Deployment: Streamlined deployment to various production environments. Library Integration: Integration with HuggingFace libraries like Accelerate, PEFT for model optimization. Prompt Management: Save prompt templates with transformers pipelines to optimize inference with less boilerplate.  Example Use Case: For an illustration of fine-tuning a model and logging the results with MLflow, refer to the fine-tuning tutorials. These tutorial demonstrate the process of fine-tuning a pretrained foundational model into the application-specific model such as a spam classifier, SQL generator. MLflow plays a pivotal role in tracking the fine-tuning process, including datasets, hyperparameters, performance metrics, and the final model artifacts. The image below shows the result of the tutorial within the MLflow UI.   Fine-tuning a Transformers Model with MLflow    Deployment Made Easy  Once a model is trained, it needs to be deployed for inference. MLflow’s integration with Transformers simplifies this by providing functions such as mlflow.transformers.load_model() and mlflow.pyfunc.load_model(), which allow for easy model serving. As part of the feature support for enhanced inference with transformers, MLflow provides mechanisms to enable the use of inference arguments that can reduce the computational overhead and lower the memory requirements for deployment.    Getting Started with the MLflow Transformers Flavor - Tutorials and Guides  Below, you will find a number of guides that focus on different use cases (tasks) using transformers  that leverage MLflow’s APIs for tracking and inference capabilities.    Introductory Quickstart to using Transformers with MLflow  If this is your first exposure to transformers or use transformers extensively but are new to MLflow, this is a great place to start.                          Quickstart: Text Generation with Transformers                                       Learn how to leverage the transformers integration with MLflow in this introductory quickstart.                       Transformers Fine-Tuning Tutorials with MLflow  Fine-tuning a model is a common task in machine learning workflows. These tutorials are designed to showcase how to fine-tune a model using the transformers library with harnessing MLflow’s APIs for tracking experiment configurations and results.                          Fine tuning a transformers Foundation Model                                       Learn how to fine-tune a transformers model using MLflow to keep track of the training process and to log a use-case-specific tuned pipeline.                                           Fine tuning LLMs efficiently using PEFT and MLflow                                       Learn how to fine-tune a large foundational models with significantly reduced memory usage using PEFT (QLoRA) and MLflow.                      Use Case Tutorials for Transformers with MLflow  Interested in learning about how to leverage transformers for tasks other than basic text generation? Want to learn more about the breadth of problems that you can solve with transformers and MLflow? These more advanced tutorials are designed to showcase different applications of the transformers model architecture and how to leverage MLflow to track and deploy these models.                          Audio Transcription with Transformers                                       Learn how to leverage the Whisper Model with MLflow to generate accurate audio transcriptions.                                           Translation with Transformers                                       Learn about the options for saving and loading transformers models in MLflow for customization of your workflows with a fun translation example!                                           Chat with Transformers                                       Learn the basics of stateful chat Conversational Pipelines with Transformers and MLflow.                                           Building and Serving an OpenAI-Compatible Chatbot                                       Learn how to build an OpenAI-compatible chatbot using a local Transformers                     model and MLflow, and serve it with minimal configuration.                                           Prompt templating with Transformers Pipelines                                       Learn how to set prompt templates on Transformers Pipelines to optimize your LLM's outputs, and simplify the end-user experience.                                           Custom PyFunc for Transformers                                       Learn how to define a custom PyFunc using transformers for advanced, state-of-the-art new models.                        Important Details to be aware of with the transformers flavor  When working with the transformers flavor in MLflow, there are several important considerations to keep in mind:  Experimental Status: The Transformers flavor in MLflow is marked as experimental, which means that APIs are subject to change, and new features may be added over time with potentially breaking changes. PyFunc Limitations: Not all output from a Transformers pipeline may be captured when using the python_function flavor. For example, if additional references or scores are required from the output, the native implementation should be used instead. Also not all the pipeline types are supported for pyfunc. Please refer to Loading a Transformers Model as a Python Function for the supported pipeline types and their input and output format. Supported Pipeline Types: Not all Transformers pipeline types are currently supported for use with the python_function flavor. In particular, new model architectures may not be supported until the transformers library has a designated pipeline type in its supported pipeline implementations. Input and Output Types: The input and output types for the python_function implementation may differ from those expected from the native pipeline. Users need to ensure compatibility with their data processing workflows. Model Configuration: When saving or logging models, the model_config can be used to set certain parameters. However, if both model_config and a ModelSignature with parameters are saved, the default parameters in ModelSignature will override those in model_config. Audio and Vision Models: Audio and text-based large language models are supported for use with pyfunc, while other types like computer vision and multi-modal models are only supported for native type loading. Prompt Templates: Prompt templating is currently supported for a few pipeline types. For a full list of supported pipelines, and more information about the feature, see this link.    Detailed Documentation  To learn more about the nuances of the transformers flavor in MLflow, delve into the comprehensive guide, which covers:  Pipelines vs. Component Logging: Explore the different approaches for saving model components or complete pipelines and understand the nuances of loading these models for various use cases. Transformers Model as a Python Function : Familiarize yourself with the various transformers pipeline types compatible with the pyfunc model flavor. Understand the standardization of input and output formats in the pyfunc model implementation for the flavor, ensuring seamless integration with JSON and Pandas DataFrames. Prompt Template: Learn how to save a prompt template with transformers pipelines to optimize inference with less boilerplate. Model Config and Model Signature Params for Inference: Learn how to leverage model_config and ModelSignature for flexible and customized model loading and inference. Automatic Metadata and ModelCard Logging: Discover the automatic logging features for model cards and other metadata, enhancing model documentation and transparency. Model Signature Inference : Learn about MLflow’s capability within the transformers flavor to automatically infer and attach model signatures, facilitating easier model deployment. Overriding Pytorch dtype : Gain insights into optimizing transformers models for inference, focusing on memory optimization and data type configurations. Input Data Types for Audio Pipelines: Understand the specific requirements for handling audio data in transformers pipelines, including the handling of different input types like str, bytes, and np.ndarray. Storage-Efficient Model Logging with save_pretrained Option: Learn how to leverage the new save_pretrained option to speed up model saving and loading for large foundational models without consuming excessive storage space. PEFT Models in MLflow Transformers flavor: PEFT (Parameter-Efficient Fine-Tuning) is natively supported in MLflow, enabling various optimization techniques like LoRA, QLoRA, and more for reducing fine-tuning cost significantly. Check out the guide and tutorials to learn more about how to leverage PEFT with MLflow.      Learn more about Transformers  Interested in learning more about how to leverage transformers for your machine learning workflows? 🤗 Hugging Face has a fantastic NLP course. Check it out and see how to leverage Transformers, Datasets, Tokenizers, and Accelerate.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/openai/index.html,"   Documentation  LLMs  MLflow OpenAI Flavor       MLflow OpenAI Flavor   Attention The openai flavor is under active development and is marked as Experimental. Public APIs are subject to change, and new features may be added as the flavor evolves.   Introduction  OpenAI’s GPT Models represent a significant leap in natural language processing (NLP) capabilities. The Generative Pre-trained Transformer (GPT) models are renowned for their ability to generate human-like text, comprehend complex queries, summarize extensive documents, and much more. OpenAI has been at the forefront of NLP technology, offering models that are versatile and widely applicable in various domains. Leveraging MLflow’s robust experiment tracking and model management framework, the integration with OpenAI’s GPT-based models enables practitioners to efficiently utilize these advanced NLP tools in their projects. From simple text generation to complex conversational AI applications, the MLflow-OpenAI integration brings a new level of ease and effectiveness to managing these powerful models. The integration includes:  Text Analysis and Generation: Utilizing models like GPT-3.5 and GPT-4 for diverse text-related tasks. Conversational AI: Exploring the capabilities of the Chat Completions API for interactive, context-aware applications. Embeddings Generation: Corpus and text embeddings generation capabilities for advanced document retrieval use cases.    What makes this Integration so Special?  The combination of MLflow’s experiment tracking and model management with OpenAI’s cutting-edge NLP models unlocks new potential for AI applications. This MLflow flavor for OpenAI simplifies the process of:  Developing an application that leverages the power of OpenAI’s models. By simplifying the process of keeping track of the highly iterative and creative process of prompt engineering, MLflow prompt engineering makes sure that you never lose track of a great idea. Auditing and Reviewing your most promising experiments. The MLflow tracking service means that you can easily share the results of your work and get peer review of your work. Customizing the interface to your application. Whether you want to allow creative control with exposing parameters such as temperature or to relax cost controls by exposing max_tokens, MLflow allows you to configure default values and restrict the ability to modify the parameters used for inference. Tagging and annotating particular runs with tags during the iterative prompt engineering phase to flag particularly promising ideas that you and others can revisit later for inspiration, further testing, or deployment.   The Elephant in the Room: Prompt Engineering  In other fields of applied ML, the process of iterating over hypotheses is time-consuming, tedious, and lends itself to developing habits of meticulously recording every step of the feature refinement and training process. With the advent of generative AI and the latent power of state-of-the-art LLMs such as those offered by OpenAI, the process of refining the performance of a solution is much shorter. In the span of an hour, you could easily craft and test a dozen prompts. While this speed and ease of use is remarkably empowering, it generally leads to the dreaded realization after a few hours of experimentation that you can’t remember which of the dozens of prompts that you created hours ago was the one that created the best results that you remember seeing. This is where MLflow comes in. With MLflow, you can easily track the prompts that you use, the results that you get, and the artifacts that you generate. The figure below shows a fun take on this problem that MLflow helps to solve.   Prompt Engineering for space flight with MLflow   By logging each of the prompts that are used throughout testing, not only can you easily reproduce the results that you get, but you can also share those results with others so that they can evaluate the subjective quality of the results. Without tracking in place, you’re forced to come up with a solution for recording the various parameters, prompts, test inputs, and results. You could save all of that time and effort by using MLflow with OpenAI, giving you more time to come up with fun prompts.    Features  With the MLflow OpenAI flavor, users can:  Save and log applications using OpenAI models within MLflow using mlflow.openai.save_model() and mlflow.openai.log_model(). Seamlessly track detailed experiments, including parameters, prompts, and artifacts associated with model runs. Deploy OpenAI models for various NLP applications with ease. Utilize mlflow.pyfunc.PythonModel for flexible Python function inference, enabling custom and innovative ML solutions.   What can you do with OpenAI and MLflow?  The integration of OpenAI’s advanced NLP models with MLflow’s robust model management capabilities opens up a vast array of potential real-world applications. Here are some powerful and impactful use cases:  Automated Customer Support: Develop sophisticated chatbots that understand and respond to customer inquiries in a human-like manner, significantly improving customer service efficiency and satisfaction. Content Generation and Curation: Automatically generate high-quality, contextually relevant content for articles, blogs, or social media posts. Curate content by summarizing and categorizing large volumes of text data, enhancing content management strategies. Language Translation Services: Create advanced translation tools that not only convert text from one language to another but also capture nuances, idioms, and cultural context, bridging communication gaps more effectively. Sentiment Analysis for Market Research: Analyze customer feedback, social media posts, or product reviews to gauge public sentiment about brands, products, or services, providing valuable insights for marketing and product development teams. Personalized Education and Training Tools: Develop AI-driven educational platforms that can adapt content and teaching styles to individual learning preferences, making education more engaging and effective. Legal and Compliance Document Analysis: Automate the review and analysis of legal documents, contracts, and compliance materials, increasing accuracy and reducing the time and resources required for legal workflows. Healthcare Assistance and Research: Assist in medical research by summarizing and analyzing medical literature, patient records, or clinical trial data, contributing to faster and more informed decision-making in healthcare. Financial Analysis and Forecasting: Leverage NLP models to analyze financial reports, market trends, and news articles, providing deeper insights and predictions for investment strategies and economic forecasting.  With MLflow’s integration, these applications not only benefit from the linguistic prowess of OpenAI’s models but also gain from streamlined tracking, version control, and deployment processes. This synergy empowers developers and businesses to build sophisticated, AI-driven solutions that address complex challenges and create new opportunities in various industries.   Deployment Made Easy  Deploying OpenAI models becomes a breeze with MLflow. Functions like mlflow.openai.load_model() and mlflow.pyfunc.load_model() facilitate easy model serving. Discover more about deploying models with MLflow, explore the deployments API, and learn about starting a local model serving endpoint to fully leverage the deployment capabilities of MLflow.    Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides  Below, you will find a number of guides that focus on different ways that you can leverage the power of the openai library, leveraging MLflow’s APIs for tracking and inference capabilities. The diagram below shows the basic scope of the level of complexity that the tutorials cover.   The range of content within the tutorials for the OpenAI flavor    Introductory Tutorial                           OpenAI Quickstart                                       Learn the very basics of using the OpenAI package with MLflow with some simple prompt engineering and a fun use case to get                     started with this powerful integration.                     Download the Introductory Notebook  Advanced Tutorials  In these tutorials, the topics cover applied interactions with OpenAI models, leveraging custom Python Models to enhance the functionality beyond what is possible with the basic prompt-based interaction from the introductory tutorial. If you’re new to this flavor, please start with the Introductory Tutorial above, as it has information about environment configurations that you’ll need to understand in order to get the notebooks in this section to work.                          OpenAI ChatCompletions                                       Learn how to leverage the ChatCompletions endpoint in the OpenAI flavor to create a useful text messaging screening tool within MLflow.                                           OpenAI Custom Python Model - Code Helper                                       Learn how to leverage Custom Python Models with a useful Code Helper application that leverages OpenAI Models and MLflow.                                           OpenAI Embeddings - Document Comparison                                       Explore the application of embeddings with document comparison using an OpenAI model with MLflow.                        Download the Advanced Tutorial Notebooks  To download the advanced OpenAI tutorial notebooks to run in your environment, click the respective links below: Download the ChatCompletions Notebook Download the Code Helper Notebook Download the Embeddings Notebook    Detailed Documentation  To learn more about the details of the MLflow flavor for OpenAI, delve into the comprehensive guide below. View the Comprehensive Guide         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/langchain/index.html,"   Documentation  LLMs  MLflow LangChain Flavor       MLflow LangChain Flavor   Attention The langchain flavor is under active development and is marked as Experimental. Public APIs are subject to change, and new features may be added as the flavor evolves.  Welcome to the developer guide for the integration of LangChain with MLflow. This guide serves as a comprehensive resource for understanding and leveraging the combined capabilities of LangChain and MLflow in developing advanced language model applications.  What is LangChain?  LangChain is a versatile framework designed for building applications powered by language models. It excels in creating context-aware applications that utilize language models for reasoning and generating responses, enabling the development of sophisticated NLP applications.   Supported Elements in MLflow LangChain Integration   LLMChain Agents RetrievalQA Retrievers   Warning The Langchain’s new chat interfaces such as ChatOpenAI, AzureChatOpenAI, are not supported in MLflow due to a limitation in deserialization. Please use the legacy counterparts for these models such as langchain.llms.OpenAI, langchain.llms.AzureOpenAI, or create a custom Pyfunc model.    Why use MLflow with LangChain?  Aside from the benefits of using MLflow for managing and deploying machine learning models, the integration of LangChain with MLflow provides a number of benefits that are associated with using LangChain within the broader MLflow ecosystem.  MLflow Evaluate: With the native capabilities within MLflow to evaluate language models, you can easily utilize automated evaluation algorithms on the results of your LangChain application’s inference results. This integration facilitates the efficient assessment of inference results from your LangChain application, ensuring robust performance analytics. Simplified Experimentation: LangChain’s flexibility in experimenting with various agents, tools, and retrievers becomes even more powerful when paired with MLflow. This combination allows for rapid experimentation and iteration. You can effortlessly compare runs, making it easier to refine models and accelerate the journey from development to production deployment. Robust Dependency Management: Deploy your LangChain application with confidence, leveraging MLflow’s ability to manage and record all external dependencies. This ensures consistency between development and deployment environments, reducing deployment risks and simplifying the process.    Capabilities of LangChain and MLflow   Efficient Development: Streamline the development of NLP applications with LangChain’s modular components and MLflow’s robust tracking features. Flexible Integration: Leverage the versatility of LangChain within the MLflow ecosystem for a range of NLP tasks, from simple text generation to complex data retrieval and analysis. Advanced Functionality: Utilize LangChain’s advanced features like context-aware reasoning and dynamic action selection in agents, all within MLflow’s scalable platform.    Overview of Chains, Agents, and Retrievers   Chains: Sequences of actions or steps hardcoded in code. Chains in LangChain combine various components like prompts, models, and output parsers to create a flow of processing steps.  The figure below shows an example of interfacing directly with a SaaS LLM via API calls with no context to the history of the conversation in the top portion. The bottom portion shows the same queries being submitted to a LangChain chain that incorporates a conversation history state such that the entire conversation’s history is included with each subsequent input. Preserving conversational context in this manner is key to creating a “chat bot”.     Agents: Dynamic constructs that use language models to choose a sequence of actions. Unlike chains, agents decide the order of actions based on inputs, tools available, and intermediate outcomes.      Retrievers: Components in RetrievalQA chains responsible for sourcing relevant documents or data. Retrievers are key in applications where LLMs need to reference specific external information for accurate responses.       Getting Started with the MLflow LangChain Flavor - Tutorials and Guides   Introductory Tutorial  In this introductory tutorial, you will learn the most fundamental components of LangChain and how to leverage the integration with MLflow to store, retrieve, and use a chain.                          LangChain Quickstart                                       Get started with MLflow and LangChain by exploring the simplest possible chain configuration of a prompt and model chained to create                     a single-purpose utility application.                     Download the Introductory Notebook  Advanced Tutorials  In these tutorials, you can learn about more complex usages of LangChain with MLflow. It is highly advised to read through the introductory tutorial prior to exploring these more advanced use cases.                          LangChain Agents                                       Learn how to build a LangChain agent that can query a web search engine and perform calculations based on complex questions using MLflow.                                           RAG tutorial with LangChain                                       Learn how to build a LangChain RAG with MLflow integration to answer highly specific questions about the legality of business ventures.                     Download the LangChain Agents Notebook Download the LangChain Retriever Notebook     Detailed Documentation  To learn more about the details of the MLflow LangChain flavor, read the detailed guide below. View the Comprehensive Guide    FAQ   I can’t save my chain, agent, or retriever with MLflow.   Serialization Challenges with Cloudpickle: Serialization with cloudpickle can encounter limitations depending on the complexity of the objects.  Some objects, especially those with intricate internal states or dependencies on external system resources, are not inherently pickleable. This limitation arises because serialization essentially requires converting an object to a byte stream, which can be complex for objects tightly coupled with system states or those having external I/O operations. Try upgrading PyDantic to 2.x version to resolve this issue.   Verifying Native Serialization Support: Ensure that the langchain object (chain, agent, or retriever) is serializable natively using langchain APIs if saving or logging with MLflow doesn’t work.  Due to their complex structures, not all langchain components are readily serializable. If native serialization is not supported and MLflow doesn’t support saving the model, you can file an issue in the LangChain repository or ask for guidance in the LangChain Discussions board.   Keeping Up with New Features in MLflow: MLflow might not immediately support the latest LangChain features immediately.  If a new feature is not supported in MLflow, consider filing a feature request on the MLflow GitHub issues page.      I’m getting an AttributeError when saving my model   Handling Dependency Installation in LangChain and MLflow: LangChain and MLflow do not automatically install all dependencies.  Other packages that might be required for specific agents, retrievers, or tools may need to be explicitly defined when saving or logging your model. If your model relies on these external component libraries (particularly for tools) that not included in the standard LangChain package, these dependencies will not be automatically logged as part of the model at all times (see below for guidance on how to include them).   Declaring Extra Dependencies: Use the extra_pip_requirements parameter when saving and logging.  When saving or logging your model that contains external dependencies that are not part of the core langchain installation, you will need these additional dependencies. The model flavor contains two options for declaring these dependencies: extra_pip_requirements and pip_requirements. While specifying pip_requirements is entirely valid, we recommend using extra_pip_requirements as it does not rely on defining all of the core dependent packages that are required to use the langchain model for inference (the other core dependencies will be inferred automatically).      I can’t load the model logged by mlflow langchain autologging   Model contains langchain retrievers: LangChain retrievers are not supported by MLflow autologging.  If your model contains a retriever, you will need to manually log the model using the mlflow.langchain.log_model API. As loading those models requires specifying loader_fn and persist_dir parameters, please check examples in retriever_chain   Can’t pickle certain objects: Try manually log the model.  For certain models that langchain does not support native saving/loading, we pickle the object and save it, while it requires cloudpickle version to be consistent when saving/loading the model, as well as PyDantic version to be 2.x. Try manually log the model with mlflow.langchain.log_model API.            Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/sentence-transformers/index.html,"   Documentation  LLMs  MLflow Sentence-Transformers Flavor       MLflow Sentence-Transformers Flavor   Attention The sentence-transformers flavor is under active development and is marked as Experimental. Public APIs are subject to change, and new features may be added as the flavor evolves.   Introduction  Sentence-Transformers is a groundbreaking Python library that specializes in producing high-quality, semantically rich embeddings for sentences and paragraphs. Developed as an extension of the well-known Transformers library by 🤗 Hugging Face, Sentence-Transformers is tailored for tasks requiring a deep understanding of sentence-level context. This library is essential for NLP applications such as semantic search, text clustering, and similarity assessment. Leveraging pre-trained models like BERT, RoBERTa, and DistilBERT, which are fine-tuned for sentence embeddings, Sentence-Transformers simplifies the process of generating meaningful vector representations of text. The library stands out for its simplicity, efficiency, and the quality of embeddings it produces. The library features a number of powerful high-level utility functions for performing common follow-on tasks with sentence embeddings. These include:  Semantic Textual Similarity: Assessing the semantic similarity between two sentences. Semantic Search: Searching for the most semantically similar sentences in a corpus for a given query. Clustering: Grouping similar sentences together. Information Retrieval: Finding the most relevant sentences for a given query via document retrieval and ranking. Paraphrase Mining: Finding text entries that have similar (or identical) meaning in a large corpus of text.    What makes this Library so Special?  Let’s take a look at a very basic representation of how the Sentence-Transformers library works and what you can do with it!   Sentence-Transformers Model Architecture Overview   Integrating Sentence-Transformers with MLflow, a platform dedicated to streamlining the entire machine learning lifecycle, enhances the experiment tracking and deployment capabilities for these specialized NLP models. MLflow’s support for Sentence-Transformers enables practitioners to effectively manage experiments, track different model versions, and deploy models for various NLP tasks with ease. Sentence-Transformers offers:  High-Quality Sentence Embeddings: Efficient generation of sentence embeddings that capture the contextual and semantic nuances of language. Pre-Trained Model Availability: Access to a diverse range of pre-trained models fine-tuned for sentence embedding tasks, streamlining the process of embedding generation. Ease of Use: Simplified API, making it accessible for both NLP experts and newcomers. Custom Training and Fine-Tuning: Flexibility to fine-tune models on specific datasets or train new models from scratch for tailored NLP solutions.  With MLflow’s Sentence-Transformers flavor, users benefit from:  Streamlined Experiment Tracking: Easily log parameters, metrics, and sentence embedding models during the training and fine-tuning process. Hassle-Free Deployment: Deploy sentence embedding models for various applications with straightforward API calls. Broad Model Compatibility: Support for a range of sentence embedding models from the Sentence-Transformers library, ensuring access to the latest in embedding technology.  Whether you’re working on semantic text similarity, clustering, or information retrieval, MLflow’s integration with Sentence-Transformers provides a robust and efficient pathway for incorporating advanced sentence-level understanding into your applications.   Features  With MLflow’s Sentence-Transformers flavor, users can:  Save and log Sentence-Transformer models within MLflow with the respective APIs: mlflow.sentence_transformers.save_model() and mlflow.sentence_transformers.log_model(). Track detailed experiments, including parameters, metrics, and artifacts associated with fine tuning runs. Deploy sentence embedding models for practical applications. Utilize the mlflow.pyfunc.PythonModel flavor for generic Python function inference, enabling complex and powerful custom ML solutions.   What can you do with Sentence Transformers and MLflow?  One of the more powerful applications that can be built with these tools is a semantic search engine. By using readily available open source tooling, you can build a semantic search engine that can find the most semantically similar sentences in a corpus for a given query. This is a significant improvement over traditional keyword-based search engines, which are limited in their ability to understand the context of a query. An example high-level architecture for such an application stack is shown below:   A basic architecture for a semantic search engine built with Sentence Transformers and MLflow     Deployment Made Easy  Once a model is trained, it needs to be deployed for inference. MLflow’s integration with Sentence Transformers simplifies this by providing functions such as mlflow.sentence_transformers.load_model() and mlflow.pyfunc.load_model(), which allow for easy model serving. You can read more about deploying models with MLflow, find further information on using the deployments API, and starting a local model serving endpoint to get a deeper understanding of the deployment options that MLflow has available.    Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides  Below, you will find a number of guides that focus on different ways that you can leverage the power of the sentence-transformers library, leveraging MLflow’s APIs for tracking and inference capabilities.  Introductory Tutorial                           Sentence Transformers Quickstart                                       Learn the very basics of using the Sentence Transformers package with MLflow to generate sentence embeddings from a logged model in                     both native and generic Python function formats.                     Download the Introductory Notebook  Advanced Tutorials                           Semantic Similarity Tutorial                                       Learn how to leverage sentence embeddings to determine similarity scores between two sentences.                                           Semantic Search Tutorial                                       Learn how to use sentence embeddings to find the most similar embedding within a corpus of text.                                           Paraphrase Mining Tutorial                                       Explore the power of paraphrase mining to identify semantically similar sentences in a corpus of text.                        Download the Advanced Tutorial Notebooks  To download the advanced sentence transformers tutorial notebooks to run in your environment, click the respective links below: Download the Semantic Similarity Notebook Download the Semantic Search Notebook Download the Paraphrase Mining Notebook    Detailed Documentation  To learn more about the details of the MLflow flavor for sentence transformers, delve into the comprehensive guide below. View the Comprehensive Guide    Learning More About Sentence Transformers  Sentence Transformers is a versatile framework for computing dense vector representations of sentences, paragraphs, and images. Based on transformer networks like BERT, RoBERTa, and XLM-RoBERTa, it offers state-of-the-art performance across various tasks. The framework is designed for easy use and customization, making it suitable for a wide range of applications in natural language processing and beyond. For those interested in delving deeper into Sentence Transformers, the following resources are invaluable:  Official Documentation and Source code   Official Documentation: For a comprehensive guide to getting started, advanced usage, and API references, visit the Sentence Transformers Documentation. GitHub Repository: The Sentence Transformers GitHub repository is the primary source for the latest code, examples, and updates. Here, you can also report issues, contribute to the project, or explore how the community is using and extending the framework.    Official Guides and Tutorials for Sentence Transformers   Training Custom Models: The framework supports fine-tuning of custom embedding models to achieve the best performance on specific tasks. Publications and Research: To understand the scientific foundations of Sentence Transformers, the publications section offers a collection of research papers that have been integrated into the framework. Application Examples: Explore a variety of application examples demonstrating the practical use of Sentence Transformers in different scenarios.    Library Resources   PyPI Package: The PyPI page for Sentence Transformers provides information on installation, version history, and package dependencies. Conda Forge Package: For users preferring Conda as their package manager, the Conda Forge page for Sentence Transformers is the go-to resource for installation and package details. Pretrained Models: Sentence Transformers offers an extensive range of pretrained models optimized for various languages and tasks. These models can be easily integrated into your projects.  Sentence Transformers is continually evolving, with regular updates and additions to its capabilities. Whether you’re a researcher, developer, or enthusiast in the field of natural language processing, these resources will help you make the most of this powerful tool.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#general-genai-guides,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/prompt-engineering/index.html,"   Documentation  LLMs  Prompt Engineering UI (Experimental)       Prompt Engineering UI (Experimental)  Starting in MLflow 2.7, the MLflow Tracking UI provides a best-in-class experience for prompt engineering. With no code required, you can try out multiple LLMs from the MLflow Deployments Server, parameter configurations, and prompts to build a variety of models for question answering, document summarization, and beyond. Using the embedded Evaluation UI, you can also evaluate multiple models on a set of inputs and compare the responses to select the best one. Every model created with the prompt engineering UI is stored in the MLflow Model format and can be deployed for batch or real time inference. All configurations (prompt templates, choice of LLM, parameters, etc.) are tracked as MLflow Runs.  Quickstart  The following guide will get you started with MLflow’s UI for prompt engineering.  Step 1: Create an MLflow Deployments Server Completions or Chat Endpoint  To use the prompt engineering UI, you need to create one or more MLflow Deployments Server completions or chat Endpoints. Follow the MLflow Deployments Server Quickstart guide to easily create an endpoint in less than five minutes. If you already have access to an MLflow Deployments Server endpoint of type llm/v1/completions or llm/v1/chat, you can skip this step. mlflow deployments start-server --config-path config.yaml --port 7000     Step 2: Connect the MLflow Deployments Server to your MLflow Tracking Server  The prompt engineering UI also requires a connection between the MLflow Deployments Server and the MLflow Tracking Server. To connect the MLflow Deployments Server with the MLflow Tracking Server, simply set the MLFLOW_DEPLOYMENTS_TARGET environment variable in the environment where the server is running and restart the server. For example, if the MLflow Deployments Server is running at http://localhost:7000, you can start an MLflow Tracking Server in a shell on your local machine and connect it to the MLflow Deployments Server using the mlflow server command as follows: export MLFLOW_DEPLOYMENTS_TARGET=""http://127.0.0.1:7000"" mlflow server --port 5000     Step 3: Create or find an MLflow Experiment  Next, open an existing MLflow Experiment in the MLflow UI, or create a new experiment.      Step 4: Create a run with prompt engineering  Once you have opened the Experiment, click the New Run button and select using Prompt Engineering. This will open the prompt engineering playground where you can try out different LLMs, parameters, and prompts.     Step 5: Select your endpoint and evaluate the example prompt  Next, click the Select endpoint dropdown and select the MLflow Deployments Server completions endpoint you created in Step 1. Then, click the Evaluate button to test out an example prompt engineering use case for generating product advertisements. MLflow will embed the specified stock_type input variable value - ""books"" - into the specified prompt  template and send it to the LLM associated with the MLflow Deployments Server endpoint with the configured temperature (currently 0.01) and max_tokens (currently 1000). The LLM response will appear in the Output section.      Step 6: Try a prompt of your choosing  Replace the prompt template from the previous step with a prompt template of your choosing. Prompts can define multiple variables. For example, you can use the following prompt template to instruct the LLM to answer questions about the MLflow documentation: Read the following article from the MLflow documentation that appears between triple backticks. Then, answer the question about the documentation that appears between triple quotes. Include relevant links and code examples in your answer.  ```{{article}}```  """""" {{question}} """"""   Then, fill in the input variables. For example, in the MLflow documentation use case, the article input variable can be set to the contents of https://mlflow.org/docs/latest/tracking.html#logging-data-to-runs and the question input variable can be set to ""How do I create a new MLflow Run using the Python API?"". Finally, click the Evaluate button to see the new output. You can also try choosing a larger value of temperature to observe how the LLM’s output changes.      Step 7: Capture your choice of LLM, prompt template, and parameters as an MLflow Run  Once you’re satisfied with your chosen prompt template and parameters, click the Create Run button to store this information, along with your choice of LLM, as an MLflow Run. This will create a new Run with the prompt template, parameters, and choice of LLM stored as Run params. It will also automatically create an MLflow Model with this information that can be used for batch or real-time inference.  To view this information, click the Run name to open the Run page:       You can also see the parameters and compare them with other configurations by opening the Table view tab:       After your Run is created, MLflow will open the Evaluation tab where you can see your latest playground input & output and try out additional inputs:          Step 8: Try new inputs  To test the behavior of your chosen LLM, prompt template, and parameters on a new inputs:  Click the Add Row button and fill in a value(s) your prompt template’s input variable(s). For example, in the MLflow documentation use case, you can try asking a question unrelated to MLflow to see how the LLM responds. This is important to ensure that the application is robust to irrelevant inputs.      Then, click the Evaluate button to see the output.       Finally, click the Save button to store the new inputs and output.          Step 9: Adjust your prompt template and create a new Run  As you try additional inputs, you might discover scenarios where your choice of LLM, prompt template, and parameters doesn’t perform as well as you would like. For example, in the MLflow documentation use case, the LLM still attempts to answer irrelevant questions about MLflow Projects even if the answer does not appear in the specified article.  To improve performance, create a new Run by selecting the Duplicate run option from the context menu. For example, in the MLflow documentation use case, adding the following text to the prompt template helps improve robustness to irrelevant questions: If the question does not relate to the article, respond exactly with the phrase ""I do not know how to answer that question."" Do not include any additional text in your response.       Then, from the prompt engineering playground, adjust the prompt template (and / or choice of LLM and parameters), evaluate an input, and click the Create Run button to create a new Run.          Step 10: Evaluate the new prompt template on previous inputs  Now that you’ve made an adjustment to your prompt template, it’s important to make sure that the new template performs well on the previous inputs and compare the outputs with older configurations.  From the Evaluation tab, click the Evaluate all button next to the new Run to evaluate all of the previous inputs.     Click the Save button to store the results.        Step 11: Load evaluation data programmatically  All of the inputs and outputs produced by the MLflow prompt engineering UI and Evaluation UI are stored as artifacts in MLflow Runs. They can be accessed programmatically using the mlflow.load_table() API as follows:  import mlflow  mlflow.set_experiment(""/Path/to/your/prompt/engineering/experiment"")  # Load input and output data across all Runs (configurations) as a Pandas DataFrame inputs_outputs_pdf = mlflow.load_table(     # All inputs and outputs created from the MLflow UI are stored in an artifact called     # ""eval_results_table.json""     artifact_file=""eval_results_table.json"",     # Include the run ID as a column in the table to distinguish inputs and outputs     # produced by different runs     extra_columns=[""run_id""], ) # Optionally convert the Pandas DataFrame to Spark where it can be stored as a Delta # table or joined with existing Delta tables inputs_outputs_sdf = spark.createDataFrame(inputs_outputs_pdf)      Step 12: Generate predictions programmatically  Once you have found a configuration of LLM, prompt template, and parameters that performs well, you can generate predictions using the corresponding MLflow Model in a Python environment of your choosing, or you can deploy it for real-time serving.  To load the MLflow Model in a notebook for batch inference, click on the Run’s name to open the Run Page and select the model directory in the Artifact Viewer. Then, copy the first few lines of code from the Predict on a Pandas DataFrame section and run them in a Python environment of your choosing, for example:    import mlflow  logged_model = ""runs:/8451075c46964f82b85fe16c3d2b7ea0/model""  # Load model as a PyFuncModel. loaded_model = mlflow.pyfunc.load_model(logged_model)    Then, to generate predictions, call the predict() method and pass in a dictionary of input variables. For example: article_text = """""" An MLflow Project is a format for packaging data science code in a reusable and reproducible way. The MLflow Projects component includes an API and command-line tools for running projects, which also integrate with the Tracking component to automatically record the parameters and git commit of your source code for reproducibility.  This article describes the format of an MLflow Project and how to run an MLflow project remotely using the MLflow CLI, which makes it easy to vertically scale your data science code. """""" question = ""What is an MLflow project?""  loaded_model.predict({""article"": article_text, ""question"": question})   For more information about deployment for real-time serving with MLflow, see the instructions below.     Step 13: Perform metric-based evaluation of your model’s outputs  If you’d like to assess your model’s performance on specific metrics, MLflow provides the mlflow.evaluate() API. Let’s evaluate our model on some pre-defined metrics for text summarization:  import mlflow import pandas as pd  logged_model = ""runs:/840a5c43f3fb46f2a2059b761557c1d0/model""  article_text = """""" An MLflow Project is a format for packaging data science code in a reusable and reproducible way. The MLflow Projects component includes an API and command-line tools for running projects, which also integrate with the Tracking component to automatically record the parameters and git commit of your source code for reproducibility.  This article describes the format of an MLflow Project and how to run an MLflow project remotely using the MLflow CLI, which makes it easy to vertically scale your data science code. """""" question = ""What is an MLflow project?""  data = pd.DataFrame(     {         ""article"": [article_text],         ""question"": [question],         ""ground_truth"": [             article_text         ],  # used for certain evaluation metrics, such as ROUGE score     } )  with mlflow.start_run():     results = mlflow.evaluate(         model=logged_model,         data=data,         targets=""ground_truth"",         model_type=""text-summarization"",     )  eval_table = results.tables[""eval_results_table""] print(f""See evaluation table below: \n{eval_table}"")    The evaluation results can also be viewed in the MLflow Evaluation UI:      The mlflow.evaluate() API also supports custom metrics, static dataset evaluation, and much more. For a more in-depth guide, see MLflow LLM Evaluate.    Deployment for real-time serving  Once you have found a configuration of LLM, prompt template, and parameters that performs well, you can deploy the corresponding MLflow Model for real-time serving as follows:  Register your model with the MLflow Model Registry. The following example registers an MLflow Model created from the Quickstart as Version 1 of the Registered Model named ""mlflow_docs_qa_model"". mlflow.register_model(     model_uri=""runs:/8451075c46964f82b85fe16c3d2b7ea0/model"",     name=""mlflow_docs_qa_model"", )    Define the following environment variables in the environment where you will run your MLflow Model Server, such as a shell on your local machine:  MLFLOW_DEPLOYMENTS_TARGET: The URL of the MLflow Deployments Server   Use the mlflow models serve command to start the MLflow Model Server. For example, running the following command from a shell on your local machine will serve the model on port 8000: mlflow models serve --model-uri models:/mlflow_docs_qa_model/1 --port 8000    Once the server has been started, it can be queried via REST API call. For example:  input=' {     ""dataframe_records"": [         {             ""article"": ""An MLflow Project is a format for packaging data science code..."",             ""question"": ""What is an MLflow Project?""         }     ] }'  echo $input | curl \   -s \   -X POST \   https://localhost:8000/invocations   -H 'Content-Type: application/json' \   -d @-   where article and question are replaced with the input variable(s) from your prompt template.           Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/deployments/index.html,"   Documentation  LLMs  MLflow Deployments Server (Experimental)       MLflow Deployments Server (Experimental)   Important The feature previously known as MLflow AI Gateway in experimental status has been moved to utilize the MLflow deployments API. This major update involves changes to API endpoints and standardization for Large Language Models, both custom and SaaS-based. Users currently utilizing MLflow AI Gateway should refer to the new documentation for migration guidelines and familiarize themselves with the updated API structure. See MLflow AI Gateway Migration Guide for migration.  The MLflow Deployments Server is a powerful tool designed to streamline the usage and management of various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a high-level interface that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM related requests. A major advantage of using the MLflow Deployments Server is its centralized management of API keys. By storing these keys in one secure location, organizations can significantly enhance their security posture by minimizing the exposure of sensitive API keys throughout the system. It also helps to prevent exposing these keys within code or requiring end-users to manage keys safely. The deployments server is designed to be flexible and adaptable, capable of easily defining and managing endpoints by updating the configuration file. This enables the easy incorporation of new LLM providers or provider LLM types into the system without necessitating changes to applications that interface with the deployments server. This level of adaptability makes the MLflow Deployments Server Service an invaluable tool in environments that require agility and quick response to changes. This simplification and centralization of language model interactions, coupled with the added layer of security for API key management, make the MLflow Deployments Server an ideal choice for organizations that use LLMs on a regular basis.    Tutorials and Guides  If you’re interested in diving right in to a step by step guide that will get you up and running with the MLflow Deployments Server as fast as possible, the guides below will be your best first stop. View the Deployments Server Getting Started Guide  Quickstart  The following guide will assist you in getting up and running, using a 3-endpoint configuration to OpenAI services for chat, completions, and embeddings.  Step 1: Install the MLflow Deployments Server  First, you need to install the MLflow Deployments Server on your machine. You can do this using pip from PyPI or from the MLflow repository.  Installing from PyPI  pip install 'mlflow[genai]'      Step 2: Set the OpenAI API Key(s) for each provider  The deployments server needs to communicate with the OpenAI API. To do this, it requires an API key. You can create an API key from the OpenAI dashboard. For this example, we’re only connecting with OpenAI. If there are additional providers within the configuration, these keys will need to be set as well. Once you have the key, you can set it as an environment variable in your terminal: export OPENAI_API_KEY=your_api_key_here   This sets a temporary session-based environment variable. For production use cases, it is advisable to store this key in the .bashrc or .zshrc files so that the key doesn’t have to be re-entered upon system restart.   Step 3: Create a Deployments Server Configuration File  Next, you need to create a deployments server configuration file. This is a YAML file where you specify the endpoints that the MLflow Deployments Server should expose. Let’s create a file with three endpoints using OpenAI as a provider: completions, chat, and embeddings. For details about the configuration file’s parameters (including parameters for other providers besides OpenAI), see the Deployments Server Configuration Details section below. endpoints:   - name: completions     endpoint_type: llm/v1/completions     model:       provider: openai       name: gpt-3.5-turbo       config:         openai_api_key: $OPENAI_API_KEY     limit:       renewal_period: minute       calls: 10    - name: chat     endpoint_type: llm/v1/chat     model:       provider: openai       name: gpt-3.5-turbo       config:         openai_api_key: $OPENAI_API_KEY    - name: embeddings     endpoint_type: llm/v1/embeddings     model:       provider: openai       name: text-embedding-ada-002       config:         openai_api_key: $OPENAI_API_KEY   Save this file to a location on the system that is going to be running the MLflow Deployments Server.   Step 4: Start the Deployments Server  You’re now ready to start the deployments server! Use the MLflow Deployments Server start-server command and specify the path to your configuration file: mlflow deployments start-server --config-path config.yaml --port {port} --host {host} --workers {worker count}   The configuration file can also be set using the MLFLOW_DEPLOYMENTS_CONFIG environment variable: export MLFLOW_DEPLOYMENTS_CONFIG=/path/to/config.yaml   If you do not specify the host, a localhost address will be used. If you do not specify the port, port 5000 will be used. The worker count for gunicorn defaults to 2 workers.   Step 5: Access the Interactive API Documentation  The MLflow Deployments Server provides an interactive API documentation endpoint that you can use to explore and test the exposed endpoints. Navigate to http://{host}:{port}/ (or http://{host}:{port}/docs) in your browser to access it. The docs endpoint allow for direct interaction with the endpoints and permits submitting actual requests to the provider services by click on the “try it now” option within the endpoint definition entry.   Step 6: Send Requests Using the Client API  See the Client API section for further information.   Step 7: Send Requests to Endpoints via REST API  You can now send requests to the exposed endpoints. See the REST examples for guidance on request formatting.   Step 8: Compare Provider Models  Here’s an example of adding a new model from a provider to determine which model instance is better for a given use case. Firstly, update the MLflow Deployments Server config YAML file with the additional endpoint definition to test: endpoints:   - name: completions     endpoint_type: llm/v1/completions     model:       provider: openai       name: gpt-3.5-turbo       config:         openai_api_key: $OPENAI_API_KEY   - name: completions-gpt4     endpoint_type: llm/v1/completions     model:       provider: openai       name: gpt-4       config:         openai_api_key: $OPENAI_API_KEY   This updated configuration adds a new completions endpoint completions-gpt4 while still preserving the original completions endpoint that was configured with the gpt-3.5-turbo  model. Once the configuration file is updated, simply save your changes. The deployments server will automatically create the new endpoint with zero downtime. If you no longer need an endpoint, you can delete it from the configuration YAML and save your changes. The deployments server will automatically remove the endpoint.   Step 9: Use Deployments Server endpoints for model development  Now that you have created several deployments server endpoints, you can create MLflow Models that query these endpoints to build application-specific logic using techniques like prompt engineering. For more information, see Deployments Server and MLflow Models.    Concepts  There are several concepts that are referred to within the MLflow Deployments Server APIs, the configuration definitions, examples, and documentation. Becoming familiar with these terms will help to simplify both configuring new endpoints and using the MLflow Deployments Server APIs.  Providers  The MLflow Deployments Server is designed to support a variety of model providers. A provider represents the source of the machine learning models, such as OpenAI, Anthropic, and so on. Each provider has its specific characteristics and configurations that are encapsulated within the model part of an endpoint in the MLflow Deployments Server.  Supported Provider Models  The table below presents a non-exhaustive list of models and a corresponding endpoint type within the MLflow Deployments Server. With the rapid development of LLMs, there is no guarantee that this list will be up to date at all times. However, the associations listed below can be used as a helpful guide when configuring a given endpoint for any newly released model types as they become available with a given provider. N/A means that either the provider or the MLflow Deployments Server implementation currently doesn’t support the endpoint type.         Provider Endpoints   llm/v1/completions llm/v1/chat llm/v1/embeddings    OpenAI §  gpt-3.5-turbo-instruct davinci-002    gpt-3.5-turbo gpt-4    text-embedding-ada-002    MosaicML  mpt-7b-instruct mpt-30b-instruct llama2-70b-chat†    llama2-70b-chat†    instructor-large instructor-xl    Anthropic  claude-1 claude-1.3-100k claude-2   N/A N/A  Cohere  command command-light    command command-light    embed-english-v2.0 embed-multilingual-v2.0    Azure OpenAI  text-davinci-003 gpt-35-turbo    gpt-35-turbo gpt-4    text-embedding-ada-002    PaLM  text-bison-001    chat-bison-001    embedding-gecko-001    MLflow  MLflow served models*    MLflow served models*    MLflow served models**    HuggingFace TGI N/A  HF TGI Models   N/A  AI21 Labs  j2-ultra j2-mid j2-light   N/A N/A  Amazon Bedrock  Amazon Titan Third-party providers   N/A N/A  Mistral  mistral-tiny mistral-small   N/A  mistral-embed      § For full compatibility references for OpenAI, see the OpenAI Model Compatibility Matrix. † Llama 2 is licensed under the LLAMA 2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. Within each model block in the configuration file, the provider field is used to specify the name of the provider for that model. This is a string value that needs to correspond to a provider the MLflow Deployments Server supports.  Note * MLflow Model Serving will only work for chat or completions if the output return is in an endpoint-compatible format. The response must conform to either an output of {""predictions"": str} or {""predictions"": {""candidates"": str}}. Any complex return type from a model that does not conform to these structures will raise an exception at query time. ** Embeddings support is only available for models whose response signatures conform to the structured format of {""predictions"": List[float]} or {""predictions"": List[List[float]]}. Any other return type will raise an exception at query time. FeatureExtractionPipeline in transformers and models using the sentence_transformers flavor will return the correct data structures for the embeddings endpoint.  Here’s an example of a provider configuration within an endpoint: endpoints:   - name: chat     endpoint_type: llm/v1/chat     model:       provider: openai       name: gpt-4       config:         openai_api_key: $OPENAI_API_KEY     limit:       renewal_period: minute       calls: 10   In the above configuration, openai is the provider for the model. As of now, the MLflow Deployments Server supports the following providers:  mosaicml: This is used for models offered by MosaicML. openai: This is used for models offered by OpenAI and the Azure integrations for Azure OpenAI and Azure OpenAI with AAD. anthropic: This is used for models offered by Anthropic. cohere: This is used for models offered by Cohere. palm: This is used for models offered by PaLM. huggingface text generation inference: This is used for models deployed using Huggingface Text Generation Inference. ai21labs: This is used for models offered by AI21 Labs. bedrock: This is used for models offered by Amazon Bedrock. mistral: This is used for models offered by Mistral.  More providers are being added continually. Check the latest version of the MLflow Deployments Server Docs for the most up-to-date list of supported providers. Remember, the provider you specify must be one that the MLflow Deployments Server supports. If the provider is not supported, the deployments server will return an error when trying to route requests to that provider.    Endpoints  Endpoints are central to how the MLflow Deployments Server functions. Each endpoint acts as a proxy endpoint for the user, forwarding requests to the underlying Models and Providers specified in the configuration file. an endpoint in the MLflow Deployments Server consists of the following fields:  name: This is the unique identifier for the endpoint. This will be part of the URL when making API calls via the MLflow Deployments Server. type: The type of the endpoint corresponds to the type of language model interaction you desire. For instance, llm/v1/completions for text completion operations, llm/v1/embeddings for text embeddings, and llm/v1/chat for chat operations. model: Defines the model to which this endpoint will forward requests. The model contains the following details:   provider: Specifies the name of the provider for this model. For example, openai for OpenAI’s GPT-3.5 models. name: The name of the model to use. For example, gpt-3.5-turbo for OpenAI’s GPT-3.5-Turbo model. config: Contains any additional configuration details required for the model. This includes specifying the API base URL and the API key.    limit: Specify the rate limit setting this endpoint will follow. The limit field contains the following fields:   renewal_period: The time unit of the rate limit, one of [second|minute|hour|day|month|year]. calls: The number of calls this endpoint will accept within the specified time unit.     Here’s an example of an endpoint configuration: endpoints:   - name: completions     endpoint_type: llm/v1/chat     model:       provider: openai       name: gpt-3.5-turbo       config:         openai_api_key: $OPENAI_API_KEY     limit:       renewal_period: minute       calls: 10   In the example above, a request sent to the completions endpoint would be forwarded to the gpt-3.5-turbo model provided by openai. The endpoints in the configuration file can be updated at any time, and the MLflow Deployments Server will automatically update its available endpoints without requiring a restart. This feature provides you with the flexibility to add, remove, or modify endpoints as your needs change. It enables ‘hot-swapping’ of endpoints, providing a seamless experience for any applications or services that interact with the MLflow Deployments Server. When defining endpoints in the configuration file, ensure that each name is unique to prevent conflicts. Duplicate endpoint names will raise an MlflowException.   Models  The model section within an endpoint specifies which model to use for generating responses. This configuration block needs to contain a name field which is used to specify the exact model instance to be used. Additionally, a provider needs to be specified, one that you have an authenticated access api key for. Different endpoint types are often associated with specific models. For instance, the llm/v1/chat and llm/v1/completions endpoints are generally associated with conversational models, while llm/v1/embeddings endpoints would typically be associated with embedding or transformer models. The model you choose should be appropriate for the type of endpoint specified. Here’s an example of a model name configuration within an endpoint: endpoints:   - name: embeddings     endpoint_type: llm/v1/embeddings     model:       provider: openai       name: text-embedding-ada-002       config:         openai_api_key: $OPENAI_API_KEY   In the above configuration, text-embedding-ada-002 is the model used for the embeddings endpoint. When specifying a model, it is critical that the provider supports the model you are requesting. For instance, openai as a provider supports models like text-embedding-ada-002, but other providers may not. If the model is not supported by the provider, the MLflow Deployments Server will return an HTTP 4xx error when trying to route requests to that model.  Important Always check the latest documentation of the specified provider to ensure that the model you want to use is supported for the type of endpoint you’re configuring.  Remember, the model you choose directly affects the results of the responses you’ll get from the API calls. Therefore, choose a model that fits your use-case requirements. For instance, for generating conversational responses, you would typically choose a chat model. Conversely, for generating embeddings of text, you would choose an embedding model.    Configuring the Deployments Server  The MLflow Deployments Server relies on a user-provided configuration file, written in YAML, that defines the endpoints and providers available to the server. The configuration file dictates how the deployments server interacts with various language model providers and determines the end-points that users can access.  Deployments Server Configuration  The configuration file includes a series of sections, each representing a unique endpoint. Each endpoint section has a name, a type, and a model specification, which includes the model provider, name, and configuration details. The configuration section typically contains the base URL for the API and an environment variable for the API key. Here is an example of a single-endpoint configuration: endpoints:   - name: chat     endpoint_type: llm/v1/chat     model:       provider: openai       name: gpt-3.5-turbo       config:         openai_api_key: $OPENAI_API_KEY     limit:       renewal_period: minute       calls: 10   In this example, we define an endpoint named chat that corresponds to the llm/v1/chat type, which will use the gpt-3.5-turbo model from OpenAI to return query responses from the OpenAI service, and accept up to 10 requests per minute. The MLflow Deployments Server configuration is very easy to update. Simply edit the configuration file and save your changes, and the MLflow Deployments Server will automatically update the endpoints with zero disruption or down time. This allows you to try out new providers or model types while keeping your applications steady and reliable. In order to define an API key for a given provider, there are three primary options:  Directly include it in the YAML configuration file. Use an environment variable to store the API key and reference it in the YAML configuration file. Define your API key in a file and reference the location of that key-bearing file within the YAML configuration file.  If you choose to include the API key directly, replace $OPENAI_API_KEY in the YAML file with your actual API key.  Warning The MLflow Deployments Server provides direct access to billed external LLM services. It is strongly recommended to restrict access to this server. See the section on security for guidance.  If you prefer to use an environment variable (recommended), you can define it in your shell environment. For example: export OPENAI_API_KEY=""your_openai_api_key""   Note: Replace “your_openai_api_key” with your actual OpenAI API key.  Deployments Server Configuration Details  The MLflow Deployments Server relies on a user-provided configuration file. It defines how the deployments server interacts with various language model providers and dictates the endpoints that users can access. The configuration file is written in YAML and includes a series of sections, each representing a unique endpoint. Each endpoint section has a name, a type, and a model specification, which includes the provider, model name, and provider-specific configuration details. Here are the details of each configuration parameter:  General Configuration Parameters   endpoints: This is a list of endpoint configurations. Each endpoint represents a unique endpoint that maps to a particular language model service.  Each endpoint has the following configuration parameters:  name: This is the name of the endpoint. It needs to be a unique name without spaces or any non-alphanumeric characters other than hyphen and underscore. endpoint_type: This specifies the type of service offered by this endpoint. This determines the interface for inputs to an endpoint and the returned outputs. Current supported endpoint types are:  “llm/v1/completions” “llm/v1/chat” “llm/v1/embeddings”   model: This defines the provider-specific details of the language model. It contains the following fields:  provider: This indicates the provider of the AI model. It accepts the following values:  “openai” “mosaicml” “anthropic” “cohere” “palm” “azure” / “azuread” “mlflow-model-serving” “huggingface-text-generation-inference” “ai21labs” “bedrock” “mistral”   name: This is an optional field to specify the name of the model. config: This contains provider-specific configuration details.      Provider-Specific Configuration Parameters   OpenAI          Configuration Parameter Required Default Description    openai_api_key Yes  This is the API key for the OpenAI service.  openai_api_type No  This is an optional field to specify the type of OpenAI API to use.  openai_api_base No https://api.openai.com/v1 This is the base URL for the OpenAI API.  openai_api_version No  This is an optional field to specify the OpenAI API version.  openai_organization No  This is an optional field to specify the organization in OpenAI.      MosaicML          Configuration Parameter Required Default Description    mosaicml_api_key Yes N/A This is the API key for the MosaicML service.      Cohere          Configuration Parameter Required Default Description    cohere_api_key Yes N/A This is the API key for the Cohere service.      HuggingFace Text Generation Inference          Configuration Parameter Required Default Description    hf_server_url Yes N/A This is the url of the Huggingface TGI Server.      PaLM          Configuration Parameter Required Default Description    palm_api_key Yes N/A This is the API key for the PaLM service.      AI21 Labs          Configuration Parameter Required Default Description    ai21labs_api_key Yes N/A This is the API key for the AI21 Labs service.      Anthropic          Configuration Parameter Required Default Description    anthropic_api_key Yes N/A This is the API key for the Anthropic service.      Amazon Bedrock  Top-level model configuration for Amazon Bedrock endpoints must be one of the following two supported authentication modes: key-based or role-based.         Configuration Parameter Required Default Description    aws_config No  An object with either the key-based or role-based schema below.    To use key-based authentication, define an Amazon Bedrock endpoint with the required fields below. .. note: If using a configured endpoint purely for development or testing, utilizing an IAM User role or a temporary short-lived standard IAM role are recommended; while for production deployments, a standard long-expiry IAM role is recommended to ensure that the endpoint is capable of handling authentication for a long period. If the authentication expires and a new set of keys need to be supplied, the endpoint must be recreated in order to persist the new keys.           Configuration Parameter Required Default Description    aws_region No AWS_REGION/AWS_DEFAULT_REGION The AWS Region to use for bedrock access.  aws_secret_access_key Yes  AWS secret access key for the IAM user/role authorized to use bedrock  aws_access_key_id Yes  AWS access key ID for the IAM user/role authorized to use Bedrock  aws_session_token No None Optional session token, if required    Alternatively, for role-based authentication, an Amazon Bedrock endpoint can be defined and initialized with an a IAM Role  ARN that is authorized to access Bedrock.  The MLflow Deployments Server will attempt to assume this role with using the standard credential provider chain and will renew the role credentials if they have expired.         Configuration Parameter Required Default Description    aws_region No AWS_REGION/AWS_DEFAULT_REGION The AWS Region to use for bedrock access.  aws_role_arn Yes  An AWS role authorized to use Bedrock.  The standard credential provider chain must be able to find credentials authorized to assume this role.  session_length_seconds No 900 The length of session to request.      MLflow Model Serving          Configuration Parameter Required Default Description    model_server_url Yes N/A This is the url of the MLflow Model Server.    Note that with MLflow model serving, the name parameter for the model definition is not used for validation and is only present for reference purposes. This alias can be useful for understanding a particular version or endpoint definition that was used that can be referenced back to a deployed model. You may choose any name that you wish, provided that it is JSON serializable.   Azure OpenAI  Azure provides two different mechanisms for integrating with OpenAI, each corresponding to a different type of security validation. One relies on an access token for validation, referred to as azure, while the other uses Azure Active Directory (Azure AD) integration for authentication, termed as azuread. To match your user’s interaction and security access requirements, adjust the openai_api_type parameter to represent the preferred security validation model. This will ensure seamless interaction and reliable security for your Azure-OpenAI integration.         Configuration Parameter Required Default Description    openai_api_key Yes  This is the API key for the Azure OpenAI service.  openai_api_type Yes  This field must be either azure or azuread depending on the security access protocol.  openai_api_base Yes  This is the base URL for the Azure OpenAI API service provided by Azure.  openai_api_version Yes  The version of the Azure OpenAI service to utilize, specified by a date.  openai_deployment_name Yes  This is the name of the deployment resource for the Azure OpenAI service.  openai_organization No  This is an optional field to specify the organization in OpenAI.      Mistral          Configuration Parameter Required Default Description    mistral_api_key       | Yes      | N/A                      | This is the API key for the Mistral service.    An example configuration for Azure OpenAI is: endpoints:   - name: completions     endpoint_type: llm/v1/completions     model:       provider: openai       name: gpt-35-turbo       config:         openai_api_type: ""azuread""         openai_api_key: $AZURE_AAD_TOKEN         openai_deployment_name: ""{your_deployment_name}""         openai_api_base: ""https://{your_resource_name}-azureopenai.openai.azure.com/""         openai_api_version: ""2023-05-15""     limit:       renewal_period: minute       calls: 10    Note Azure OpenAI has distinct features as compared with the direct OpenAI service. For an overview, please see the comparison documentation.  For specifying an API key, there are three options:  (Preferred) Use an environment variable to store the API key and reference it in the YAML configuration file. This is denoted by a $ symbol before the name of the environment variable. (Preferred) Define the API key in a file and reference the location of that key-bearing file within the YAML configuration file. Directly include it in the YAML configuration file.   Important The use of environment variables or file-based keys is recommended for better security practices. If the API key is directly included in the configuration file, it should be ensured that the file is securely stored and appropriately access controlled. Please ensure that the configuration file is stored in a secure location as it contains sensitive API keys.        Querying the Deployments Server  Once the MLflow Deployments Server has been configured and started, it is ready to receive traffic from users.  Standard Query Parameters  The MLflow Deployments Server defines standard parameters for chat, completions, and embeddings that can be used when querying any endpoint regardless of its provider. Each parameter has a standard range and default value. When querying an endpoint with a particular provider, the MLflow Deployments Server automatically scales parameter values according to the provider’s value ranges for that parameter.  Completions  The standard parameters for completions endpoints with type llm/v1/completions are:          Query Parameter Type Required Default Description    prompt string Yes N/A The prompt for which to generate completions.  n integer No 1 The number of completions to generate for the specified prompt, between 1 and 5.  temperature float No 0.0 The sampling temperature to use, between 0 and 1. Higher values will make the output more random, and lower values will make the output more deterministic.  max_tokens integer No None The maximum completion length, between 1 and infinity (unlimited).  stop array[string] No None Sequences where the model should stop generating tokens and return the completion.      Chat  The standard parameters for chat endpoints with type llm/v1/chat are:          Query Parameter Type Required Default Description    messages array[message] Yes N/A A list of messages in a conversation from which to a new message (chat completion). For information about the message structure, see Messages.  n integer No 1 The number of chat completions to generate for the specified prompt, between 1 and 5.  temperature float No 0.0 The sampling temperature to use, between 0 and 1. Higher values will make the output more random, and lower values will make the output more deterministic.  max_tokens integer No None The maximum completion length, between 1 and infinity (unlimited).  stop array[string] No None Sequences where the model should stop generating tokens and return the chat completion.     Messages  Each chat message is a string dictionary containing the following fields:         Field Name Required Default Description    role Yes N/A The role of the conversation participant who sent the message. Must be one of: ""system"", ""user"", or ""assistant"".  content Yes N/A The message content.       Embeddings  The standard parameters for completions endpoints with type llm/v1/embeddings are:          Query Parameter Type Required Default Description    input string or array[string] Yes N/A A string or list of strings for which to generate embeddings.       Additional Query Parameters  In addition to the Standard Query Parameters, you can pass any additional parameters supported by the endpoint’s provider as part of your query. For example:  logit_bias (supported by OpenAI, Cohere) top_k (supported by MosaicML, Anthropic, PaLM, Cohere) frequency_penalty (supported by OpenAI, Cohere, AI21 Labs) presence_penalty (supported by OpenAI, Cohere, AI21 Labs) stream (supported by OpenAI, Cohere)  Below is an example of submitting a query request to an MLflow Deployments Server endpoint using additional parameters: from mlflow.deployments import get_deploy_client  client = get_deploy_client(""http://my.deployments:8888"")  data = {     ""prompt"": (         ""What would happen if an asteroid the size of ""         ""a basketball encountered the Earth traveling at 0.5c? ""         ""Please provide your answer in .rst format for the purposes of documentation.""     ),     ""temperature"": 0.5,     ""max_tokens"": 1000,     ""n"": 1,     ""frequency_penalty"": 0.2,     ""presence_penalty"": 0.2, }  client.predict(endpoint=""completions-gpt4"", inputs=data)   The results of the query are: {     ""id"": ""chatcmpl-8Pr33fsCAtD2L4oZHlyfOkiYHLapc"",     ""object"": ""text_completion"",     ""created"": 1701172809,     ""model"": ""gpt-4-0613"",     ""choices"": [         {             ""index"": 0,             ""text"": ""If an asteroid the size of a basketball ..."",         }     ],     ""usage"": {         ""prompt_tokens"": 43,         ""completion_tokens"": 592,         ""total_tokens"": 635,     }, }    Streaming  Some providers support streaming responses. Streaming responses are useful when you want to receive responses as they are generated, rather than waiting for the entire response to be generated before receiving it. Streaming responses are supported by the following providers:        Provider Endpoints   llm/v1/completions llm/v1/chat    OpenAI ✓ ✓  Cohere ✓ ✓    To enable streaming responses, set the stream parameter to true in your request. For example: curl -X POST http://my.deployments:8888/endpoints/chat/invocations \    -H ""Content-Type: application/json"" \    -d '{""messages"": [{""role"": ""user"", ""content"": ""hello""}], ""stream"": true}'   The results of the query follow the OpenAI schema.  Chat  data: {""choices"": [{""delta"": {""content"": null, ""role"": ""assistant""}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161926, ""id"": ""chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3"", ""model"": ""gpt-35-turbo"", ""object"": ""chat.completion.chunk""}  data: {""choices"": [{""delta"": {""content"": ""Hello"", ""role"": null}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161926, ""id"": ""chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3"", ""model"": ""gpt-35-turbo"", ""object"": ""chat.completion.chunk""}  data: {""choices"": [{""delta"": {""content"": "" there"", ""role"": null}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161926, ""id"": ""chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3"", ""model"": ""gpt-35-turbo"", ""object"": ""chat.completion.chunk""}  data: {""choices"": [{""delta"": {""content"": null, ""role"": null}, ""finish_reason"": ""stop"", ""index"": 0}], ""created"": 1701161926, ""id"": ""chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3"", ""model"": ""gpt-35-turbo"", ""object"": ""chat.completion.chunk""}     Completions  data: {""choices"": [{""delta"": {""role"": null, ""content"": null}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161629, ""id"": ""chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2"", ""model"": ""gpt-35-turbo"", ""object"": ""text_completion_chunk""}  data: {""choices"": [{""delta"": {""role"": null, ""content"": ""If""}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161629, ""id"": ""chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2"", ""model"": ""gpt-35-turbo"", ""object"": ""text_completion_chunk""}  data: {""choices"": [{""delta"": {""role"": null, ""content"": "" an""}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161629, ""id"": ""chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2"", ""model"": ""gpt-35-turbo"", ""object"": ""text_completion_chunk""}  data: {""choices"": [{""delta"": {""role"": null, ""content"": "" asteroid""}, ""finish_reason"": null, ""index"": 0}], ""created"": 1701161629, ""id"": ""chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2"", ""model"": ""gpt-35-turbo"", ""object"": ""text_completion_chunk""}  data: {""choices"": [{""delta"": {""role"": null, ""content"": null}, ""finish_reason"": ""length"", ""index"": 0}], ""created"": 1701161629, ""id"": ""chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2"", ""model"": ""gpt-35-turbo"", ""object"": ""text_completion_chunk""}       FastAPI Documentation (“/docs”)  FastAPI, the framework used for building the MLflow Deployments Server, provides an automatic interactive API documentation interface, which is accessible at the “/docs” endpoint (e.g., http://my.deployments:9000/docs). This interactive interface is very handy for exploring and testing the available API endpoints. As a convenience, accessing the root URL (e.g., http://my.deployments:9000) redirects to this “/docs” endpoint.   MLflow Python Client APIs  MlflowDeploymentClient is the user-facing client API that is used to interact with the MLflow Deployments Server. It abstracts the HTTP requests to the Deployments Server via a simple, easy-to-use Python API.  Client API  To use the MlflowDeploymentClient API, see the below examples for the available API methods:  Create an MlflowDeploymentClient  from mlflow.deployments import get_deploy_client  client = get_deploy_client(""http://my.deployments:8888"")     List all endpoints:  The list_endpoints() method returns a list of all endpoints. endpoint = client.list_endpoints() for endpoint in endpoints:     print(endpoint)     Query an endpoint:  The predict() method submits a query to a configured provider endpoint. The data structure you send in the query depends on the endpoint. response = client.predict(     endpoint=""chat"",     inputs={""messages"": [{""role"": ""user"", ""content"": ""Tell me a joke about rabbits""}]}, ) print(response)        LangChain Integration  LangChain supports an integration for MLflow Deployments. This integration enable users to use prompt engineering, retrieval augmented generation, and other techniques with LLMs in the deployments server.  Example  import mlflow from langchain import LLMChain, PromptTemplate from langchain.llms import Mlflow  llm = Mlflow(target_uri=""http://127.0.0.1:5000"", endpoint=""completions"") llm_chain = LLMChain(     llm=llm,     prompt=PromptTemplate(         input_variables=[""adjective""],         template=""Tell me a {adjective} joke"",     ), ) result = llm_chain.run(adjective=""funny"") print(result)  with mlflow.start_run():     model_info = mlflow.langchain.log_model(llm_chain, ""model"")  model = mlflow.pyfunc.load_model(model_info.model_uri) print(model.predict([{""adjective"": ""funny""}]))      MLflow Models  Interfacing with MLflow Models can be done in two ways. With the use of a custom PyFunc Model, a query can be issued directly to a deployments server endpoint and used in a broader context within a model. Data may be augmented, manipulated, or used in a mixture of experts paradigm. The other means of utilizing the MLflow Deployments Server along with MLflow Models is to define a served MLflow model directly as an endpoint within a deployments server.  Using the Deployments Server to Query a served MLflow Model  For a full walkthrough and example of using the MLflow serving integration to query a model directly through the MLflow Deployments Server, please see the full example. Within the guide, you will see the entire end-to-end process of serving multiple models from different servers and configuring an MLflow Deployments Server instance to provide a single unified point to handle queries from.   Using an MLflow Model to Query the Deployments Server  You can also build and deploy MLflow Models that call the MLflow Deployments Server. The example below demonstrates how to use a deployments server from within a custom pyfunc model.  Note The custom Model shown in the example below is utilizing environment variables for the deployments server’s uri. These values can also be set manually within the definition or can be applied via mlflow.deployments.get_deployments_target() after the uri has been set. For the example below, the value for MLFLOW_DEPLOYMENTS_TARGET is http://127.0.0.1:5000/. For an actual deployment use case, this value would be set to the configured and production deployment server.  import os import pandas as pd import mlflow   def predict(data):     from mlflow.deployments import get_deploy_client      client = get_deploy_client(os.environ[""MLFLOW_DEPLOYMENTS_TARGET""])      payload = data.to_dict(orient=""records"")     return [         client.predict(endpoint=""completions"", inputs=query)[""choices""][0][""text""]         for query in payload     ]   input_example = pd.DataFrame.from_dict(     {""prompt"": [""Where is the moon?"", ""What is a comet made of?""]} ) signature = mlflow.models.infer_signature(     input_example, [""Above our heads."", ""It's mostly ice and rocks.""] )  with mlflow.start_run():     model_info = mlflow.pyfunc.log_model(         python_model=predict,         registered_model_name=""anthropic_completions"",         artifact_path=""anthropic_completions"",         input_example=input_example,         signature=signature,     )  df = pd.DataFrame.from_dict(     {         ""prompt"": [""Tell me about Jupiter"", ""Tell me about Saturn""],         ""temperature"": 0.6,         ""max_records"": 500,     } )  loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)  print(loaded_model.predict(df))   This custom MLflow model can be used in the same way as any other MLflow model. It can be used within a spark_udf, used with mlflow.evaluate(), or deploy like any other model.    REST API  The REST API allows you to send HTTP requests directly to the MLflow Deployments Server. This is useful if you’re not using Python or if you prefer to interact with a deployments server using HTTP directly. Here are some examples for how you might use curl to interact with the MLflow Deployments Server:  Get information about a particular endpoint: GET /api/2.0/endpoints/{name} This route returns a serialized representation of the endpoint data structure. This provides information about the name and type, as well as the model details for the requested endpoint. curl -X GET http://my.deployments:8888/api/2.0/endpoints/embeddings    List all endpoints: GET /api/2.0/endpoints/ This route returns a list of all endpoints. curl -X GET http://my.deployments:8888/api/2.0/endpoints/    Query an endpoint: POST /endpoints/{name}/invocations This route allows you to submit a query to a configured provider endpoint. The data structure you send in the query depends on the endpoint. Here are examples for the “completions”, “chat”, and “embeddings” endpoints:  Completions curl -X POST http://my.deployments:8888/endpoints/completions/invocations \   -H ""Content-Type: application/json"" \   -d '{""prompt"": ""Describe the probability distribution of the decay chain of U-235""}'    Chat curl -X POST http://my.deployments:8888/endpoints/chat/invocations \   -H ""Content-Type: application/json"" \   -d '{""messages"": [{""role"": ""user"", ""content"": ""Can you write a limerick about orange flavored popsicles?""}]}'    Embeddings curl -X POST http://my.deployments:8888/endpoints/embeddings/invocations \   -H ""Content-Type: application/json"" \   -d '{""input"": [""I would like to return my shipment of beanie babies, please"", ""Can I please speak to a human now?""]}'       Note: Remember to replace my.deployments:8888 with the URL of your actual MLflow Deployments Server.     MLflow Deployments Server API Documentation  API documentation   Deployments Server Security Considerations  Remember to ensure secure access to the system that the MLflow Deployments Server is running in to protect access to these keys. An effective way to secure your deployments server is by placing it behind a reverse proxy. This will allow the reverse proxy to handle incoming requests and forward them to the MLflow Deployments Server. The reverse proxy effectively shields your application from direct exposure to Internet traffic. A popular choice for a reverse proxy is Nginx. In addition to handling the traffic to your application, Nginx can also serve static files and load balance the traffic if you have multiple instances of your application running. Furthermore, to ensure the integrity and confidentiality of data between the client and the server, it’s highly recommended to enable HTTPS on your reverse proxy. In addition to the reverse proxy, it’s also recommended to add an authentication layer before the requests reach the MLflow Deployments Server. This could be HTTP Basic Authentication, OAuth, or any other method that suits your needs. For example, here’s a simple configuration for Nginx with Basic Authentication: http {     server {         listen 80;          location / {             auth_basic ""Restricted Content"";             auth_basic_user_file /etc/nginx/.htpasswd;              proxy_pass http://localhost:5000;  # Replace with the MLflow Deployments Server port         }     } }   In this example, /etc/nginx/.htpasswd is a file that contains the username and password for authentication. These measures, together with a proper network setup, can significantly improve the security of your system and ensure that only authorized users have access to submit requests to your LLM services.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/llm-evaluate/index.html,"   Documentation  LLMs  MLflow LLM Evaluate       MLflow LLM Evaluate  With the emerging of ChatGPT, LLMs have shown its power of text generation in various fields, such as question answering, translating and text summarization. Evaluating LLMs’ performance is slightly different from traditional ML models, as very often there is no single ground truth to compare against. MLflow provides an API mlflow.evaluate() to help evaluate your LLMs. MLflow’s LLM evaluation functionality consists of 3 main components:  A model to evaluate: it can be an MLflow pyfunc model, a URI pointing to one registered MLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline. Metrics: the metrics to compute, LLM evaluate will use LLM metrics. Evaluation data: the data your model is evaluated at, it can be a pandas Dataframe, a python list, a numpy array or an mlflow.data.dataset.Dataset() instance.   Full Notebook Guides and Examples  If you’re interested in thorough use-case oriented guides that showcase the simplicity and power of MLflow’s evaluate functionality for LLMs, please navigate to the notebook collection below: View the Notebook Guides  Quickstart  Below is a simple example that gives an quick overview of how MLflow LLM evaluation works. The example builds a simple question-answering model by wrapping “openai/gpt-4” with custom prompt. You can paste it to your IPython or local editor and execute it, and install missing dependencies as prompted. Running the code requires OpenAI API key, if you don’t have an OpenAI key, you can set it up by following the OpenAI guide. export OPENAI_API_KEY='your-api-key-here'   import mlflow import openai import os import pandas as pd from getpass import getpass  eval_data = pd.DataFrame(     {         ""inputs"": [             ""What is MLflow?"",             ""What is Spark?"",         ],         ""ground_truth"": [             ""MLflow is an open-source platform for managing the end-to-end machine learning (ML) ""             ""lifecycle. It was developed by Databricks, a company that specializes in big data and ""             ""machine learning solutions. MLflow is designed to address the challenges that data ""             ""scientists and machine learning engineers face when developing, training, and deploying ""             ""machine learning models."",             ""Apache Spark is an open-source, distributed computing system designed for big data ""             ""processing and analytics. It was developed in response to limitations of the Hadoop ""             ""MapReduce computing model, offering improvements in speed and ease of use. Spark ""             ""provides libraries for various tasks such as data ingestion, processing, and analysis ""             ""through its components like Spark SQL for structured data, Spark Streaming for ""             ""real-time data processing, and MLlib for machine learning tasks"",         ],     } )  with mlflow.start_run() as run:     system_prompt = ""Answer the following question in two sentences""     # Wrap ""gpt-4"" as an MLflow model.     logged_model_info = mlflow.openai.log_model(         model=""gpt-4"",         task=openai.chat.completions,         artifact_path=""model"",         messages=[             {""role"": ""system"", ""content"": system_prompt},             {""role"": ""user"", ""content"": ""{question}""},         ],     )      # Use predefined question-answering metrics to evaluate our model.     results = mlflow.evaluate(         logged_model_info.model_uri,         eval_data,         targets=""ground_truth"",         model_type=""question-answering"",     )     print(f""See aggregated evaluation results below: \n{results.metrics}"")      # Evaluation result for each data record is available in `results.tables`.     eval_table = results.tables[""eval_results_table""]     print(f""See evaluation table below: \n{eval_table}"")     LLM Evaluation Metrics  There are two types of LLM evaluation metrics in MLflow:  Metrics relying on SaaS model (e.g., OpenAI) for scoring, e.g., mlflow.metrics.genai.answer_relevance(). These metrics are created via mlflow.metrics.genai.make_genai_metric() method. For each data record, these metrics under the hood sends one prompt consisting of the following information to the SaaS model, and extract the score from model response:  Metrics definition. Metrics grading criteria. Reference examples. Input data/context. Model output. [optional] Ground truth.  More details of how these fields are set can be found in the section “Create your Custom LLM-evaluation Metrics”.  Function-based per-row metrics. These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe), based on certain functions, like Rouge (mlflow.metrics.rougeL()) or Flesch Kincaid (mlflow.metrics.flesch_kincaid_grade_level()). These metrics are similar to traditional metrics.   Select Metrics to Evaluate  There are two ways to select metrics to evaluate your model:  Use default metrics for pre-defined model types. Use a custom list of metrics.    Use Default Metrics for Pre-defined Model Types  MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, “question-answering”. Depending on the LLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluations. To use defaults metrics for pre-selected tasks, specify the model_type argument in mlflow.evaluate(), as shown by the example below: results = mlflow.evaluate(     model,     eval_data,     targets=""ground_truth"",     model_type=""question-answering"", )   The supported LLM model types and associated metrics are listed below:  question-answering: model_type=""question-answering"":   exact-match toxicity 1 ari_grade_level 2 flesch_kincaid_grade_level 2    text-summarization: model_type=""text-summarization"":   ROUGE 3 toxicity 1 ari_grade_level 2 flesch_kincaid_grade_level 2    text models: model_type=""text"":   toxicity 1 ari_grade_level 2 flesch_kincaid_grade_level 2     1 Requires package evaluate, torch, and transformers 2 Requires package textstat 3 Requires package evaluate, nltk, and rouge-score   Use a Custom List of Metrics  Using the pre-defined metrics associated with a given model type is not the only way to generate scoring metrics for LLM evaluation in MLflow. You can specify a custom list of metrics in the extra_metrics argument in mlflow.evaluate:  To add additional metrics to the default metrics list of pre-defined model type, keep the model_type and add your metrics to extra_metrics: results = mlflow.evaluate(     model,     eval_data,     targets=""ground_truth"",     model_type=""question-answering"",     extra_metrics=[mlflow.metrics.latency()], )   The above code will evaluate your model using all metrics for “question-answering” model plus mlflow.metrics.latency().  To disable default metric calculation and only calculate your selected metrics, remove the model_type argument and define the desired metrics.  results = mlflow.evaluate(     model,     eval_data,     targets=""ground_truth"",     extra_metrics=[mlflow.metrics.toxicity(), mlflow.metrics.latency()], )      The full reference for supported evaluation metrics can be found here.   Metrics with LLM as the Judge  MLflow offers a few pre-canned metrics which uses LLM as the judge. Despite the difference under the hood, the usage is the same - put these metrics in the extra_metrics argument in mlflow.evaluate(). Here is the list of pre-canned metrics:  mlflow.metrics.genai.answer_similarity(): Use this metric when you want to evaluate how similar the model generated output is compared to the information in the ground_truth. High scores mean that your model outputs contain similar information as the ground_truth, while low scores mean that outputs may disagree with the ground_truth. mlflow.metrics.genai.answer_correctness(): Use this metric when you want to evaluate how factually correct the model generated output is based on the information in the ground_truth. High scores mean that your model outputs contain similar information as the ground_truth and that this information is correct, while low scores mean that outputs may disagree with the ground_truth or that the information in the output is incorrect. Note that this builds onto answer_similarity. mlflow.metrics.genai.answer_relevance(): Use this metric when you want to evaluate how relevant the model generated output is to the input (context is ignored). High scores mean that your model outputs are about the same subject as the input, while low scores mean that outputs may be non-topical. mlflow.metrics.genai.relevance(): Use this metric when you want to evaluate how relevant the model generated output is with respect to both the input and the context. High scores mean that the model has understood the context and correct extracted relevant information from the context, while low score mean that output has completely ignored the question and the context and could be hallucinating. mlflow.metrics.genai.faithfulness(): Use this metric when you want to evaluate how faithful the model generated output is based on the context provided. High scores mean that the outputs contain information that is in line with the context, while low scores mean that outputs may disagree with the context (input is ignored).    Selecting the LLM-as-judge Model  By default, llm-as-judge metrics use openai:/gpt-4 as the judge. You can change the default judge model by passing an override to the model argument within the metric definition, as shown below. In addition to OpenAI models, you can also use any endpoint via MLflow Deployments. Use mlflow.deployments.set_deployments_target() to set the target deployment client. To use an endpoint hosted by a local MLflow Deployments Server, you can use the following code. from mlflow.deployments import set_deployments_target  set_deployments_target(""http://localhost:5000"") my_answer_similarity = mlflow.metrics.genai.answer_similarity(     model=""endpoints:/my-endpoint"" )   To use an endpoint hosted on Databricks, you can use the following code. from mlflow.deployments import set_deployments_target  set_deployments_target(""databricks"") llama2_answer_similarity = mlflow.metrics.genai.answer_similarity(     model=""endpoints:/databricks-llama-2-70b-chat"" )   For more information about how various models perform as judges, please refer to this blog.    Creating Custom LLM-evaluation Metrics   Create LLM-as-judge Evaluation Metrics (Category 1)  You can also create your own Saas LLM evaluation metrics with MLflow API mlflow.metrics.genai.make_genai_metric(), which needs the following information:  name: the name of your custom metric. definition: describe what’s the metric doing. grading_prompt: describe the scoring critieria. examples: a few input/output examples with score, they are used as a reference for LLM judge. model: the identifier of LLM judge, in the format of “openai:/gpt-4” or “endpoints:/databricks-llama-2-70b-chat”. parameters: the extra parameters to send to LLM judge, e.g., temperature for ""openai:/gpt-3.5-turbo-16k"". aggregations: The list of options to aggregate the per-row scores using numpy functions. greater_is_better: indicates if a higher score means your model is better.  Under the hood, definition, grading_prompt, examples together with evaluation data and model output will be composed into a long prompt and sent to LLM. If you are familiar with the concept of prompt engineering, SaaS LLM evaluation metric is basically trying to compose a “right” prompt containing instructions, data and model output so that LLM, e.g., GPT4 can output the information we want. Now let’s create a custom GenAI metrics called “professionalism”, which measures how professional our model output is. Let’s first create a few examples with scores, these will be the reference samples LLM judge uses. To create such examples, we will use mlflow.metrics.genai.EvaluationExample() class, which has 4 fields:  input: input text. output: output text. score: the score for output in the context of input. justification: why do we give the score for the data.  professionalism_example_score_2 = mlflow.metrics.genai.EvaluationExample(     input=""What is MLflow?"",     output=(         ""MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps ""         ""you track experiments, package your code and models, and collaborate with your team, making the whole ML ""         ""workflow smoother. It's like your Swiss Army knife for machine learning!""     ),     score=2,     justification=(         ""The response is written in a casual tone. It uses contractions, filler words such as 'like', and ""         ""exclamation points, which make it sound less professional. ""     ), ) professionalism_example_score_4 = mlflow.metrics.genai.EvaluationExample(     input=""What is MLflow?"",     output=(         ""MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was ""         ""developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is ""         ""designed to address the challenges that data scientists and machine learning engineers face when ""         ""developing, training, and deploying machine learning models."",     ),     score=4,     justification=(""The response is written in a formal language and a neutral tone. ""), )   Now let’s define the professionalism metric, you will see how each field is set up. professionalism = mlflow.metrics.genai.make_genai_metric(     name=""professionalism"",     definition=(         ""Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is ""         ""tailored to the context and audience. It often involves avoiding overly casual language, slang, or ""         ""colloquialisms, and instead using clear, concise, and respectful language.""     ),     grading_prompt=(         ""Professionalism: If the answer is written using a professional tone, below are the details for different scores: ""         ""- Score 0: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for ""         ""professional contexts.""         ""- Score 1: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in ""         ""some informal professional settings.""         ""- Score 2: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.""         ""- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. ""         ""- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal ""         ""business or academic settings. ""     ),     examples=[professionalism_example_score_2, professionalism_example_score_4],     model=""openai:/gpt-3.5-turbo-16k"",     parameters={""temperature"": 0.0},     aggregations=[""mean"", ""variance""],     greater_is_better=True, )     Create heuristic-based LLM Evaluation Metrics (Category 2)  This is very similar to creating custom traditional metrics, with the exception of returning a mlflow.metrics.MetricValue() instance. Basically you need to:  Implement a eval_fn to define your scoring logic, it must take in 2 args predictions and targets. eval_fn must return a mlflow.metrics.MetricValue() instance. Pass eval_fn and other arguments to mlflow.metrics.make_metric API to create the metric.  The following code creates a dummy per-row metric called ""over_10_chars"": if the model output is greater than 10, the score is “yes” otherwise “no”. def eval_fn(predictions, targets):     scores = []     for i in range(len(predictions)):         if len(predictions[i]) > 10:             scores.append(""yes"")         else:             scores.append(""no"")     return MetricValue(         scores=scores,         aggregate_results=standard_aggregations(scores),     )   # Create an EvaluationMetric object. passing_code_metric = make_metric(     eval_fn=eval_fn, greater_is_better=False, name=""over_10_chars"" )   To create a custom metric that is dependent on other metrics, include those other metrics’ names as an argument after predictions and targets. This can be the name of a builtin metric or another custom metric. Ensure that you do not accidentally have any circular dependencies in your metrics, or the evaluation will fail. The following code creates a dummy per-row metric called ""toxic_or_over_10_chars"": if the model output is greater than 10 or the toxicity score is greater than 0.5, the score is “yes” otherwise “no”. def eval_fn(predictions, targets, toxicity, over_10_chars):     scores = []     for i in range(len(predictions)):         if toxicity.scores[i] > 0.5 or over_10_chars.scores[i]:             scores.append(""yes"")         else:             scores.append(""no"")     return MetricValue(scores=scores)   # Create an EvaluationMetric object. toxic_and_over_10_chars_metric = make_metric(     eval_fn=eval_fn, greater_is_better=False, name=""toxic_or_over_10_chars"" )      Prepare Your LLM for Evaluating  In order to evaluate your LLM with mlflow.evaluate(), your LLM has to be one of the following type:  A mlflow.pyfunc.PyFuncModel() instance or a URI pointing to a logged mlflow.pyfunc.PyFuncModel model. In general we call that MLflow model. The A python function that takes in string inputs and outputs a single string. Your callable must match the signature of mlflow.pyfunc.PyFuncModel.predict() (without params argument), briefly it should:  Has data as the only argument, which can be a pandas.Dataframe, numpy.ndarray, python list, dictionary or scipy matrix. Returns one of pandas.DataFrame, pandas.Series, numpy.ndarray or list.   An MLflow Deployments endpoint URI pointing to a local MLflow Deployments Server, Databricks Foundation Models API, and External Models in Databricks Model Serving. Set model=None, and put model outputs in data. Only applicable when the data is a Pandas dataframe.   Evaluating with an MLflow Model  For detailed instruction on how to convert your model into a mlflow.pyfunc.PyFuncModel instance, please read this doc. But in short, to evaluate your model as an MLflow model, we recommend following the steps below:  Package your LLM as an MLflow model and log it to MLflow server by log_model. Each flavor (opeanai, pytorch, …) has its own log_model API, e.g., mlflow.openai.log_model(): with mlflow.start_run():     system_prompt = ""Answer the following question in two sentences""     # Wrap ""gpt-3.5-turbo"" as an MLflow model.     logged_model_info = mlflow.openai.log_model(         model=""gpt-3.5-turbo"",         task=openai.chat.completions,         artifact_path=""model"",         messages=[             {""role"": ""system"", ""content"": system_prompt},             {""role"": ""user"", ""content"": ""{question}""},         ],     )    Use the URI of logged model as the model instance in mlflow.evaluate(): results = mlflow.evaluate(     logged_model_info.model_uri,     eval_data,     targets=""ground_truth"",     model_type=""question-answering"", )       Evaluating with a Custom Function  As of MLflow 2.8.0, mlflow.evaluate() supports evaluating a python function without requiring logging the model to MLflow. This is useful when you don’t want to log the model and just want to evaluate it. The following example uses mlflow.evaluate() to evaluate a function. You also need to set up OpenAI authentication to run the code below. eval_data = pd.DataFrame(     {         ""inputs"": [             ""What is MLflow?"",             ""What is Spark?"",         ],         ""ground_truth"": [             ""MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models."",             ""Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks"",         ],     } )   def openai_qa(inputs):     answers = []     system_prompt = ""Please answer the following question in formal language.""     for index, row in inputs.iterrows():         completion = openai.chat.completions.create(             model=""gpt-3.5-turbo"",             messages=[                 {""role"": ""system"", ""content"": system_prompt},                 {""role"": ""user"", ""content"": ""{row}""},             ],         )         answers.append(completion.choices[0].message.content)      return answers   with mlflow.start_run() as run:     results = mlflow.evaluate(         openai_qa,         eval_data,         model_type=""question-answering"",     )     Evaluating with an MLflow Deployments Endpoint  For MLflow >= 2.11.0, mlflow.evaluate() supports evaluating a model endpoint by directly passing the MLflow Deployments endpoint URI to the model argument. This is particularly useful when you want to evaluate a deployed model hosted by a local MLflow Deployments Server,  Databricks Foundation Models API, and External Models in Databricks Model Serving, without implementing custom prediction logic to wrap it as an MLflow model or a python function. Please don’t forget to set the target deployment client by using mlflow.deployments.set_deployments_target() before calling mlflow.evaluate() with the endpoint URI, as shown in the example below. Otherwise, you will see an error message like MlflowException: No deployments target has been set....  Hint When you want to use an endpoint not hosted by an MLflow Deployments Server or Databricks, you can create a custom Python function following the Evaluating with a Custom Function guide and use it as the model argument.   Supported Input Data Formats  The input data can be either of the following format when using an URI of the MLflow Deployment Endpoint as the model:        Data Format Example Additional Notes    A pandas DataFrame with a string column. pd.DataFrame(     {         ""inputs"": [             ""What is MLflow?"",             ""What is Spark?"",         ]     } )    For this input format, MLflow will construct the appropriate request payload to the model endpoint type. For example, if your model is a chat endpoint (llm/v1/chat), MLflow will wrap your input string with the chat messages format like {""messages"": [{""role"": ""user"", ""content"": ""What is MLflow?""}]}. If you want to customize the request payload e.g. including system prompt, please use the next format.  A pandas DataFrame with a dictionary column. pd.DataFrame(     {         ""inputs"": [             {                 ""messages"": [                     {""role"": ""system"", ""content"": ""Please answer.""},                     {""role"": ""user"", ""content"": ""What is MLflow?""},                 ],                 ""max_tokens"": 100,             },             # ... more dictionary records         ]     } )    In this format, the dictionary should have the correct request format for your model endpoint. Please refer to the MLflow Deployments documentation for more information about the request format for different model endpoint types.  A list of input strings. [     ""What is MLflow?"",     ""What is Spark?"", ]    The mlflow.evaluate() also accepts a list input.  A list of request payload (dictionary). [     {         ""messages"": [             {""role"": ""system"", ""content"": ""Please answer.""},             {""role"": ""user"", ""content"": ""What is MLflow?""},         ],         ""max_tokens"": 100,     },     # ... more dictionary records ]    Similarly to Pandas DataFrame input, the dictionary should have the correct request format for your model endpoint.      Passing Inference Parameters  You can pass additional inference parameters such as max_tokens, temperature, n, to the model endpoint by setting the inference_params argument in mlflow.evaluate(). The inference_params argument is a dictionary that contains the parameters to be passed to the model endpoint. The specified parameters are used for all the input record in the evaluation dataset.  Note When your input is a dictionary format that represents request payload, it can also include the parameters like max_tokens. If there are overlapping parameters in both the inference_params and the input data, the values in the inference_params will take precedence.    Examples  Chat Endpoint hosted by a local MLflow Deployments Server import mlflow from mlflow.deployments import set_deployments_target import pandas as pd  # Point the client to the local MLflow Deployments Server set_deployments_target(""http://localhost:5000"")  eval_data = pd.DataFrame(     {         # Input data must be a string column and named ""inputs"".         ""inputs"": [             ""What is MLflow?"",             ""What is Spark?"",         ],         # Additional ground truth data for evaluating the answer         ""ground_truth"": [             ""MLflow is an open-source platform ...."",             ""Apache Spark is an open-source, ..."",         ],     } )   with mlflow.start_run() as run:     results = mlflow.evaluate(         model=""endpoiints:/my-chat-endpoint"",         data=eval_data,         targets=""ground_truth"",         inference_params={""max_tokens"": 100, ""temperature"": 0.0},         model_type=""question-answering"",     )   Completion Endpoint hosted on Databricks Foundation Models API import mlflow from mlflow.deployments import set_deployments_target import pandas as pd  # Point the client to Databricks Foundation Models API set_deployments_target(""databricks"")  eval_data = pd.DataFrame(     {         # Input data must be a string column and named ""inputs"".         ""inputs"": [             ""Write 3 reasons why you should use MLflow?"",             ""Can you explain the difference between classification and regression?"",         ],     } )   with mlflow.start_run() as run:     results = mlflow.evaluate(         model=""endpoints:/databricks-mpt-7b-instruct"",         data=eval_data,         inference_params={""max_tokens"": 100, ""temperature"": 0.0},         model_type=""text"",     )   Evaluating External Models in Databricks Model Serving can be done in the same way, you just need to specify the different URI that points to the serving endpoint like ""endpoints:/your-chat-endpoint"".    Evaluating with a Static Dataset  For MLflow >= 2.8.0, mlflow.evaluate() supports evaluating a static dataset without specifying a model. This is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and want to evaluate the static dataset without re-running the model. If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the top-level predictions parameter in mlflow.evaluate(): import mlflow import pandas as pd  eval_data = pd.DataFrame(     {         ""inputs"": [             ""What is MLflow?"",             ""What is Spark?"",         ],         ""ground_truth"": [             ""MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. ""             ""It was developed by Databricks, a company that specializes in big data and machine learning solutions. ""             ""MLflow is designed to address the challenges that data scientists and machine learning engineers ""             ""face when developing, training, and deploying machine learning models."",             ""Apache Spark is an open-source, distributed computing system designed for big data processing and ""             ""analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, ""             ""offering improvements in speed and ease of use. Spark provides libraries for various tasks such as ""             ""data ingestion, processing, and analysis through its components like Spark SQL for structured data, ""             ""Spark Streaming for real-time data processing, and MLlib for machine learning tasks"",         ],         ""predictions"": [             ""MLflow is an open-source platform that provides handy tools to manage Machine Learning workflow ""             ""lifecycle in a simple way"",             ""Spark is a popular open-source distributed computing system designed for big data processing and analytics."",         ],     } )  with mlflow.start_run() as run:     results = mlflow.evaluate(         data=eval_data,         targets=""ground_truth"",         predictions=""predictions"",         extra_metrics=[mlflow.metrics.genai.answer_similarity()],         evaluators=""default"",     )     print(f""See aggregated evaluation results below: \n{results.metrics}"")      eval_table = results.tables[""eval_results_table""]     print(f""See evaluation table below: \n{eval_table}"")      Viewing Evaluation Results   View Evaluation Results via Code  mlflow.evaluate() returns the evaluation results as an mlflow.models.EvaluationResult() instance. To see the score on selected metrics, you can check:  metrics: stores the aggregated results, like average/variance across the evaluation dataset. Let’s take a second pass on the code example above and focus on printing out the aggregated results. with mlflow.start_run() as run:     results = mlflow.evaluate(         data=eval_data,         targets=""ground_truth"",         predictions=""predictions"",         extra_metrics=[mlflow.metrics.genai.answer_similarity()],         evaluators=""default"",     )     print(f""See aggregated evaluation results below: \n{results.metrics}"")    tables[""eval_results_table""]: stores the per-row evaluation results. with mlflow.start_run() as run:     results = mlflow.evaluate(         data=eval_data,         targets=""ground_truth"",         predictions=""predictions"",         extra_metrics=[mlflow.metrics.genai.answer_similarity()],         evaluators=""default"",     )     print(         f""See per-data evaluation results below: \n{results.tables['eval_results_table']}""     )       View Evaluation Results via the MLflow UI  Your evaluation result is automatically logged into MLflow server, so you can view your evaluation results directly from the MLflow UI. To view the evaluation results on MLflow UI, please follow the steps below:  Go to the experiment view of your MLflow experiment. Select the “Evaluation” tab. Select the runs you want to check evaluation results. Select the metrics from the dropdown menu on the right side.  Please see the screenshot below for clarity:            Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/custom-pyfunc-for-llms/index.html,"   Documentation  LLMs  Deploying Advanced LLMs with Custom PyFuncs in MLflow       Deploying Advanced LLMs with Custom PyFuncs in MLflow  Advanced Large Language Models (LLMs) such as the MPT-7B instruct transformer are intricate and have requirements that don’t align with traditional MLflow flavors. This demands a deeper understanding and the need for custom solutions. What’s in this tutorial? This guide is designed to provide insights into the deployment of advanced LLMs with MLflow, with a focus on using custom PyFuncs to address challenges:  The World of LLMs: An introduction to LLMs, particularly models like the MPT-7B instruct transformer. We’ll delve into their intricacies, importance, and the challenges associated with their deployment. Why Custom PyFuncs for LLM Deployment?: We’ll explore the reasons behind the need for custom PyFuncs in the context of LLMs. How do they provide a bridge, ensuring that LLMs can be seamlessly deployed while adhering to MLflow’s standards?   Managing Complex Behaviors: How custom PyFuncs can help in handling intricate model behaviors and dependencies that aren’t catered to by MLflow’s default flavors. Interface Data Manipulation: Delve into how custom PyFuncs allow the manipulation of interface data to generate prompts, thereby simplifying end-user interactions in a RESTful environment.    Crafting Custom PyFuncs for LLM Deployment: A step-by-step walkthrough on how to define, manage, and deploy an LLM using a custom PyFunc. We’ll look at how to design a pyfunc to address LLM requirements and behaviors, and then how to deploy it using MLflow. Challenges with Traditional LLM Deployment: Recognize the issues and limitations when trying to deploy an advanced LLM using MLflow’s built-in capabilities. Understand why custom PyFuncs become essential in such scenarios.  By the conclusion of this guide, you’ll possess a deep understanding of how to deploy advanced LLMs in MLflow using custom PyFuncs. You’ll appreciate the role of custom PyFuncs in making complex deployments streamlined and efficient.  Explore the Tutorial  View the Custom Pyfunc for LLMs Tutorial         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/rag/index.html,"   Documentation  LLMs  Retrieval Augmented Generation (RAG)       Retrieval Augmented Generation (RAG)  Retrieval Augmented Generation (RAG) is a powerful and efficient approach to natural language processing that combines the strength of both pre-trained foundation models and retrieval mechanisms. It allows the generative model to access a dataset of documents through a retrieval mechanism, which enhances generated responses to be more contextually relevant and factually accurate. This improvement results in a cost-effective and accessible alternative to training custom models for specific use cases. The Retrieval mechanism works by embedding documents and questions in the same latent space, allowing a user to ask a question and get the most relevant document chunk as a response. This mechanism then passes the contextual chunk to the generative model, resulting in better quality responses with fewer hallucinations.  Benefits of RAG   Provides LLM access to external knowledge through documents, resulting in contextually accurate and factual responses. RAG is more cost-effective than fine-tuning, since it doesn’t require the labeled data and computational resources that come with model training.    Understanding the Power of RAG  In the realm of artificial intelligence, particularly within natural language processing, the ability to generate coherent and contextually relevant responses is paramount. Large language models (LLMs) have shown immense promise in this area, but they often operate based on their internal knowledge, which can sometimes lead to inconsistencies or inaccuracies in their outputs. This is where RAG comes into play. RAG is a groundbreaking framework designed to enhance the capabilities of LLMs. Instead of solely relying on the vast but static knowledge embedded during their training, RAG empowers these models to actively retrieve and reference information from external knowledge bases. This dynamic approach ensures that the generated responses are not only rooted in the most current and reliable facts but also transparent in their sourcing. In essence, RAG transforms LLMs from closed-book learners, relying on memorized information, to open-book thinkers, capable of actively seeking out and referencing external knowledge. The implications of RAG are profound. By grounding responses in verifiable external sources, it significantly reduces the chances of LLMs producing misleading or incorrect information. Furthermore, it offers a more cost-effective solution for businesses, as there’s less need for continuous retraining of the model. With RAG, LLMs can provide answers that are not only more accurate but also more trustworthy, paving the way for a new era of AI-driven insights and interactions.   Explore RAG Tutorials  View RAG Tutorials         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
llms/llm-tracking/index.html,"   Documentation  LLMs  MLflow’s LLM Tracking Capabilities       MLflow’s LLM Tracking Capabilities  MLflow’s LLM Tracking system is an enhancement to the existing MLflow Tracking system, offerring additional capabilities for monitoring, managing, and interpreting interactions with Large Language Models (LLMs). At its core, MLflow’s LLM suite builds upon the standard logging capabilities familiar to professionals working with traditional Machine Learning (ML) and Deep Learning (DL). However, it introduces distinct features tailored for the unique intricacies of LLMs. One such standout feature is the introduction of “prompts” – the queries or inputs directed towards an LLM – and the subsequent data the model generates in response. While MLflow’s offerings for other model types typically exclude built-in mechanisms for preserving inference results, LLMs necessitate this due to their dynamic and generative nature. Recognizing this, MLflow introduces the term ‘predictions’ alongside the existing tracking components of artifacts, parameters, tags, and metrics, ensuring comprehensive lineage and quality tracking for text-generating models.  Introduction to LLM Tracking  The world of Large Language Models is vast, and as these models become more intricate and sophisticated, the need for a robust tracking system becomes paramount. MLflow’s LLM Tracking is centered around the concept of runs. In essence, a run is a distinct execution or interaction with the LLM — whether it’s a single query, a batch of prompts, or an entire fine-tuning session. Each run meticulously records:  Parameters: Key-value pairs that detail the input parameters for the LLM. These could range from model-specific parameters like top_k and temperature to more generic ones. They provide context and configuration for each run. Parameters can be logged using both mlflow.log_param() for individual entries and mlflow.log_params() for bulk logging. Metrics: These are quantitative measures, often numeric, that give insights into the performance, accuracy, or any other measurable aspect of the LLM interaction. Metrics are dynamic and can be updated as the run progresses, offering a real-time or post-process insight into the model’s behavior. Logging of metrics is facilitated through mlflow.log_metric() and mlflow.log_metrics(). Predictions: To understand and evaluate LLM outputs, MLflow allows for the logging of predictions. This encompasses the prompts or inputs sent to the LLM and the outputs or responses received. For structured storage and easy retrieval, these predictions are stored as artifacts in CSV format, ensuring that each interaction is preserved in its entirety. This logging is achieved using the dedicated mlflow.log_table(). Artifacts: Beyond predictions, MLflow’s LLM Tracking can store a myriad of output files, ranging from visualization images (e.g., PNGs), serialized models (e.g., an openai model), to structured data files (e.g., a Parquet file). The mlflow.log_artifact() function is at the heart of this, allowing users to log and organize their artifacts with ease.  Furthermore, to provide structured organization and comparative analysis capabilities, runs can be grouped into experiments. These experiments act as containers, grouping related runs, and providing a higher level of organization. This organization ensures that related runs can be compared, analyzed, and managed as a cohesive unit.   Detailed Logging of LLM Interactions  MLflow’s LLM Tracking doesn’t just record data — it offers structured logging mechanisms tailored to the needs of LLM interactions:  Parameters: Logging parameters is straightforward. Whether you’re logging a single parameter using mlflow.log_param() or multiple parameters simultaneously with mlflow.log_params(), MLflow ensures that every detail is captured. Metrics: Quantitative insights are crucial. Whether it’s tracking the accuracy of a fine-tuned LLM or understanding its response time, metrics provide this insight. They can be logged individually via mlflow.log_metric() or in bulk using mlflow.log_metrics(). Predictions: Every interaction with an LLM yields a result — a prediction. Capturing this prediction, along with the inputs that led to it, is crucial. The mlflow.log_table() function is specifically designed for this, ensuring that both inputs and outputs are logged cohesively. Artifacts: Artifacts act as the tangible outputs of an LLM run. They can be images, models, or any other form of data. Logging them is seamless with mlflow.log_artifact(), which ensures that every piece of data, regardless of its format, is stored and linked to its respective run.    Structured Storage of LLM Tracking Data  Every piece of data, every parameter, metric, prediction, and artifact is not just logged — it’s structured and stored as part of an MLflow Experiment run. This organization ensures data integrity, easy retrieval, and a structured approach to analyzing and understanding LLM interactions in the grand scheme of machine learning workflows.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
model-evaluation/index.html,"   Documentation  Model Evaluation       Model Evaluation   Harnessing the Power of Automation  In the evolving landscape of machine learning, the evaluation phase of model development is just as important as ever. Ensuring the accuracy, reliability, and efficiency of models is paramount to ensure that the model that has been trained has been as thoroughly validated as it can be prior to promoting it beyond the development phase. However, manual evaluation can be tedious, error-prone, and time-consuming. MLflow addresses these challenges head-on, offering a suite of automated tools that streamline the evaluation process, saving time and enhancing accuracy, helping you to have confidence that the solution that you’ve spent so much time working on will meet the needs of the problem you’re trying to solve.   LLM Model Evaluation  The rise of Large Language Models (LLMs) like ChatGPT has transformed the landscape of text generation, finding applications in question answering, translation, and text summarization. However, evaluating LLMs introduces unique challenges, primarily because there’s often no single ground truth to compare against. MLflow’s evaluation tools are tailored for LLMs, ensuring a streamlined and accurate evaluation process. Key Features:  Versatile Model Evaluation: MLflow supports evaluating various types of LLMs, whether it’s an MLflow pyfunc model, a URI pointing to a registered MLflow model, or any python callable representing your model. Comprehensive Metrics: MLflow offers a range of metrics for LLM evaluation. From metrics that rely on SaaS models like OpenAI for scoring (e.g., mlflow.metrics.genai.answer_relevance()) to function-based per-row metrics such as Rouge (mlflow.metrics.rougeL()) or Flesch Kincaid (mlflow.metrics.flesch_kincaid_grade_level()). Predefined Metric Collections: Depending on your LLM use case, MLflow provides predefined metric collections, such as “question-answering” or “text-summarization”, simplifying the evaluation process. Custom Metric Creation: Beyond the predefined metrics, MLflow allows users to create custom LLM evaluation metrics. Whether you’re looking to evaluate the professionalism of a response or any other custom criteria, MLflow provides the tools to define and implement these metrics. Evaluation with Static Datasets: As of MLflow 2.8.0, you can evaluate a static dataset without specifying a model. This is especially useful when you’ve saved model outputs in a dataset and want a swift evaluation without rerunning the model. Integrated Results View: MLflow’s mlflow.evaluate() returns comprehensive evaluation results, which can be viewed directly in the code or through the MLflow UI for a more visual representation.  Harnessing these features, MLflow’s LLM evaluation tools eliminate the complexities and ambiguities associated with evaluating large language models. By automating these critical evaluation tasks, MLflow ensures that users can confidently assess the performance of their LLMs, leading to more informed decisions in the deployment and application of these models.  Guides and Tutorials for LLM Model Evaluation  To learn more about how you can leverage MLflow’s evaluation features for your LLM-powered project work, see the tutorials below:                          RAG Evaluation                                       Learn how to evaluate a Retrieval Augmented Generation setup with MLflow Evaluate                                           Question-Answering Evaluation                                       See a working example of how to evaluate the quality of an LLM Question-Answering solution                                           RAG Question Generation Evaluation                                       See how to generate Questions for RAG generation and how to evaluate a RAG solution using MLflow                        Traditional ML Evaluation  Traditional machine learning techniques, from classification to regression, have been the bedrock of many industries. MLflow recognizes their significance and offers automated evaluation tools tailored for these classic techniques. Key Features:  Evaluating a Function: To get immediate results, you can evaluate a python function directly without logging the model. This is especially useful when you want a quick evaluation without the overhead of logging. Evaluating a Dataset: MLflow also supports evaluating a static dataset without specifying a model. This is invaluable when you’ve saved model outputs in a dataset and want a swift evaluation without having to rerun model inference. Evaluating a Model: With MLflow, you can set validation thresholds for your metrics. If a model doesn’t meet these thresholds compared to a baseline, MLflow will alert you. This automated validation ensures that only high-quality models progress to the next stages. Common Metrics and Visualizations: MLflow automatically logs common metrics like accuracy, precision, recall, and more. Additionally, visual graphs such as the confusion matrix, lift_curve_plot, and others are auto-logged, providing a comprehensive view of your model’s performance. SHAP Integration: MLflow is integrated with SHAP, allowing for the auto-logging of SHAP’s summarization importances validation visualizations when using the evaluate APIs.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#id3,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
deep-learning/index.html,"   Documentation  Deep Learning       Deep Learning  The realm of deep learning has witnessed an unprecedented surge, revolutionizing numerous sectors with its ability to process vast amounts of data and capture intricate patterns. From the real-time object detection in autonomous vehicles to the generation of art through Generative Adversarial Networks, and from natural language processing applications in chatbots to predictive analytics in e-commerce, deep learning models are at the forefront of today’s AI-driven innovations. In the deep learning realm, libraries such as PyTorch, Keras, Tensorflow provide handy tools to build and train deep learning models. MLflow, on the other hand, targets the problem of experiment tracking in deep learning, including logging your experiment setup (learning rate, batch size, etc) along with training metrics (loss, accuracy, etc) and the model (architecture, weights, etc). MLflow provides native integrations with deep learning libraries, so you can plug MLflow into your existing deep learning workflow with minimal changes to your code, and view your experiments in the MLflow UI.  Why MLflow for Deep Learning?  MLflow offers a list of features that power your deep learning workflows:  Experiments Tracking: MLflow tracks your deep learning experiments, including parameters, metrics, and models. Your experiments will be stored in the MLflow server, so you can compare across different experiments and share them. Model Registry: You can register your trained deep learning models in the MLflow server, so you can easily retrieve them later for inference. Model Deployment: After training, you can serve the trained model with MLflow as a REST API endpoint, so you can easily integrate it with your application.   Experiments Tracking  Tracking is the cornerstone of the MLflow ecosystem, and especially vital for the iterative nature of deep learning:  Experiments and Runs: Organize your deep learning projects into experiments, with each experiment containing multiple runs. Each run captures essential data like metrics at various training steps, hyperparameters, and the code state. Artifacts: Store vital outputs such as deep learning models, visualizations, or even tensorboard logs. This artifact repository ensures traceability and easy access. Metrics at Steps: With deep learning’s iterative nature, MLflow allows logging metrics at various training steps, offering a granular view of the model’s progress. Dependencies and Environment: Capture the computational environment, including deep learning frameworks’ versions, ensuring reproducibility. Input Examples and Model Signatures: Define the expected format of the model’s inputs, crucial for complex data like images or sequences. UI Integration: The enhanced UI provides a visual overview of deep learning runs, facilitating comparison and insights into training progress. Search Functionality: Efficiently navigate through your deep learning experiments using robust search capabilities. APIs: Interact with the tracking system programmatically, integrating deep learning workflows seamlessly.    Chart ComparisonChart CustomizationRun ComparisonStatistical EvaluationRealtime Tracking  Easier DL Model Comparison with Charts Use charts to compare deep learning (DL) model training convergence easily. Quickly identify superior         configuration sets across training iterations.      Chart Customization for DL Models Easily customize charts for DL training run comparisons. Adjust visualizations to pinpoint optimal parameter         settings, displaying optimization metrics across iterations in a unified view.      Enhanced Parameter and Metric Comparison Analyze parameter relationships from a unified interface to refine tuning parameters, optimizing your DL models efficiently.      Statistical Evaluation of Categorical Parameters Leverage boxplot visualizations for categorical parameter evaluation. Quickly discern the most effective         settings for hyperparameter tuning.      Real-Time Training Tracking Automatically monitor DL training progress over epochs with the MLflow UI. Instantly track results to validate         your hypotheses, eliminating constant manual updates.        Model Registry  A centralized repository for your deep learning models:  Versioning: Handle multiple iterations and versions of deep learning models, facilitating comparison or reversion. Annotations: Attach notes, training datasets, or other relevant metadata to models. Lifecycle Stages: Clearly define the stage of each model version, ensuring clarity in deployment and further fine-tuning.    Model Deployment  Transition deep learning models from training to real-world applications:  Consistency: Ensure models, especially those with GPU dependencies, behave consistently across different deployment environments. Docker and GPU Support: Deploy in containerized environments, ensuring all dependencies, including GPU support, are encapsulated. Scalability: From deploying a single model to serving multiple distributed deep learning models, MLflow scales as per your requirements.     Native Library Support  MLflow has native integrations with common deep learning libraries, such as PyTorch, Keras and Tensorflow, so you can plug MLflow into your workflow easily to elevate your deep learning projects. For detailed guide on how to integrate MLflow with these libraries, refer to the following pages:                             Learn about MLflow's native integration with the Tensorflow library and see example notebooks that leverage                     MLflow and Tensorflow to build deep learning workflows.                                              Learn about MLflow's native integration with the PyTorch library and see example notebooks that leverage                     MLflow and PyTorch to build deep learning workflows.                                              Learn about MLflow's native integration with the Keras library and see example notebooks that leverage                     MLflow and Keras to build deep learning workflows.                                              Learn about MLflow's native integration with the Spacy library and see example code.                                              Learn about MLflow's native integration with the FastAI library and see example code.                              Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#id4,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
traditional-ml/index.html,"   Documentation  Traditional ML       Traditional ML  In the dynamic landscape of machine learning, traditional techniques remain foundational, playing pivotal roles across various industries and research institutions. From the precision of classification algorithms in healthcare diagnostics to the predictive prowess of regression models in finance, and from the forecasting capabilities of time-series analyses in supply chain management to the insights drawn from statistical modeling in social sciences, these core methodologies underscore many of the technological advancements we witness today. MLflow recognizes the enduring significance of traditional machine learning. Designed with precision and a deep understanding of the challenges and intricacies faced by data scientists and ML practitioners, MLflow offers a comprehensive suite of tools tailor-made for these classic techniques. This platform not only streamlines the model development and deployment processes but also ensures reproducibility, scalability, and traceability. As we delve further, we’ll explore the multifaceted functionalities MLflow offers, showcasing how it enhances the efficacy, reliability, and insights derived from traditional ML models. Whether you’re a seasoned expert looking to optimize workflows or a newcomer eager to make a mark, MLflow stands as an invaluable ally in your machine learning journey.  Native Library Support  There are a number of natively supported traditional ML libraries within MLflow. Throughout the documentation, you may see these referred to as “flavors”, as they are specific implementations of native support for saving, logging, loading, and generic python function representation for the models that are produced from these libraries. There are distinct benefits to using the native versions of these implementations, as many have auto-logging functionality built in, as well as specific custom handling with serialization and deserialization that can greatly simplify your MLOps experiences when using these libraries. The officially supported integrations for traditional ML libraries include:                                         Tutorials and Guides                           Hyperparameter Tuning with MLflow and Optuna                                       Explore the integration of MLflow Tracking with Optuna for hyperparameter tuning. Dive into the capabilities of MLflow,                     understand parent-child run relationships, and compare different tuning runs to optimize model performance.                                           Custom Pyfunc Models with MLflow                                       Dive deep into the world of MLflow's Custom Pyfunc. Starting with basic model definitions, embark on a journey that                     showcases the versatility and power of Pyfunc. From simple mathematical curves to complex machine learning integrations,                     discover how Pyfunc offers standardized, reproducible, and efficient workflows for a variety of use cases.                                           Multi-Model Endpoints with PyFunc                                       Dive deep into custom multi-model inference via MLflow's custom PyFunc models. Learn how to                     simplify low-latency inference by passing additional inference parameters to a simple custom PyFunc implementation.                     This tutorial can serve as a jumping off point for many multi-model endpoing (MME) use cases!                         MLflow Tracking  Tracking is central to the MLflow ecosystem, facilitating the systematic organization of experiments and runs:  Experiments and Runs: Each experiment encapsulates a specific aspect of your research, and each experiment can house multiple runs. Runs document critical data like metrics, parameters, and the code state. Artifacts: Store crucial output from runs, be it models, visualizations, datasets, or other metadata. This repository of artifacts ensures traceability and easy access. Metrics and Parameters: By allowing users to log parameters and metrics, MLflow makes it straightforward to compare different runs, facilitating model optimization. Dependencies and Environment: The platform automatically captures the computational environment, ensuring that experiments are reproducible across different setups. Input Examples and Model Signatures: These features allow developers to define the expected format of the model’s inputs, making validation and debugging more straightforward. UI Integration: The integrated UI provides a visual overview of all runs, enabling easy comparison and deeper insights. Search Functionality: Efficiently sift through your experiments using MLflow’s robust search functionality. APIs: Comprehensive APIs are available, allowing users to interact with the tracking system programmatically, integrating it into existing workflows.    MLflow Recipes  Recipes in MLflow are predefined templates tailored for specific tasks:  Reduced Boilerplate: These templates help eliminate repetitive setup or initialization code, speeding up development. Best Practices: MLflow’s recipes are crafted keeping best practices in mind, ensuring that users are aligned with industry standards right from the get-go. Customizability: While recipes provide a structured starting point, they’re designed to be flexible, accommodating tweaks and modifications as needed.    MLflow Evaluate  Ensuring model quality is paramount:  Auto-generated Metrics: MLflow automatically evaluates models, providing key metrics for regression (like RMSE, MAE) and classification (such as F1-score, AUC-ROC). Visualization: Understand your model better with automatically generated plots. For instance, MLflow can produce confusion matrices, precision-recall curves, and more for classification tasks. Extensibility: While MLflow provides a rich set of evaluation tools out of the box, it’s also designed to accommodate custom metrics and visualizations.    Model Registry  This feature acts as a catalog for models:  Versioning: As models evolve, keeping track of versions becomes crucial. The Model Registry handles versioning, ensuring that users can revert to older versions or compare different iterations. Annotations: Models in the registry can be annotated with descriptions, use-cases, or other relevant metadata. Lifecycle Stages: Track the stage of each model version, be it ‘staging’, ‘production’, or ‘archived’. This ensures clarity in deployment and maintenance processes.    Deployment  MLflow simplifies the transition from development to production:  Consistency: By meticulously recording dependencies and the computational environment, MLflow ensures that models behave consistently across different deployment setups. Docker Support: Facilitate deployment in containerized environments using Docker, encapsulating all dependencies and ensuring a uniform runtime environment. Scalability: MLflow is designed to accommodate both small-scale deployments and large, distributed setups, ensuring that it scales with your needs.         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#id5,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#traditional-ml-guides-and-tutorials,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
traditional-ml/hyperparameter-tuning-with-child-runs/index.html,"   Documentation  Traditional ML  Hyperparameter Tuning with MLflow and Optuna       Hyperparameter Tuning with MLflow and Optuna  In this guide, we venture into a frequent use case of MLflow Tracking: hyperparameter tuning. Our journey will begin with a detailed notebook that showcases hyperparameter tuning using Optuna, and how each of these tuning runs are logged seamlessly with MLflow. Following this, we’ll delve deeper, exploring alternative APIs and techniques that can be leveraged to further enhance our model tracking capabilities. Throughout this guide, our focal points will be:  Introducing the capabilities of MLflow for tracking hyperparameter tuning Understanding the distinction between parent and child runs in MLflow Delving into the components of MLflow and their relevance in our workflow Discovering and navigating experiments, parent runs, and child runs in the MLflow UI Conducting a comparative analysis of runs to understand the nuances of hyperparameter tuning   If you're keen on navigating directly to a specific topic within this guide, the links below will lead you right to the desired section:  Full Notebooks The Parent-Child relationship with runs Logging plots with MLflow   To get started with this guide, click NEXT below.       Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
traditional-ml/creating-custom-pyfunc/index.html,"   Documentation  Traditional ML  Building Custom Python Function Models with MLflow       Building Custom Python Function Models with MLflow  MLflow offers a wide range of pre-defined model flavors, but there are instances where you’d want to go beyond these and craft something tailored to your needs. That’s where custom PyFuncs come in handy. What’s in this tutorial? This guide aims to walk you through the intricacies of PyFuncs, explaining the why, the what, and the how:  Named Model Flavors: Before we dive into the custom territory, it’s essential to understand the existing named flavors in MLflow. These pre-defined flavors simplify model tracking and deployment, but they might not cover every use case. Custom PyFuncs Demystified: What exactly is a custom PyFunc? How is it different from the named flavors, and when would you want to use one? We’ll cover:   Pre/Post Processing: Integrate preprocessing or postprocessing steps as part of your model’s prediction pipeline. Unsupported Libraries: Maybe you’re using a niche or newly-released ML library that MLflow doesn’t support yet. No worries, custom PyFuncs have you covered. External References: Avert serialization issues and simplify model deployment by externalizing references.    Getting Started with Custom PyFuncs: We’ll begin with the simplest of examples, illuminating the core components and abstract methods essential for your custom PyFunc. Tackling Unsupported Libraries: Stepping up the complexity, we’ll demonstrate how to integrate a model from an unsupported library into MLflow using custom PyFuncs. Overriding Default Inference Methods: Sometimes, the default isn’t what you want. We’ll show you how to override a model’s inference method, for example, using predict_proba instead of predict.  By the end of this tutorial, you’ll have a clear understanding of how to leverage custom PyFuncs in MLflow to cater to specialized needs, ensuring flexibility without compromising on the ease of use.   Models, Flavors, and PyFuncs in MLflow Understanding Pyfunc Components Full Notebooks         Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
deployment/index.html,"   Documentation  Deployment       Deployment   Important This page describes the toolset for deploying your in-house MLflow Model. For information on the LLM Deployment Server (formerly known as AI Gateway), please refer to MLflow Deployment Server.  After training your machine learning model and ensuring its performance, the next step is deploying it to a production environment. This process can be complex, but MLflow simplifies it by offering an easy toolset for deploying your ML models to various targets, including local environments, cloud services, and Kubernetes clusters.    By using MLflow deployment toolset, you can enjoy the following benefits:  Effortless Deployment: MLflow provides a simple interface for deploying models to various targets, eliminating the need to write boilerplate code. Dependency and Environment Management: MLflow ensures that the deployment environment mirrors the training environment, capturing all dependencies. This guarantees that models run consistently, regardless of where they’re deployed. Packaging Models and Code: With MLflow, not just the model, but any supplementary code and configurations are packaged along with the deployment container. This ensures that the model can be executed seamlessly without any missing components. Avoid Vendor Lock-in: MLflow provides a standard format for packaging models and unified APIs for deployment. You can easily switch between deployment targets without having to rewrite your code.   Concepts   MLflow Model  MLflow Model is a standard format that packages a machine learning model with its metadata, such as dependencies and inference schema. You typically create a model as a result of training execution using the MLflow Tracking APIs, for instance, mlflow.pyfunc.log_model(). Alternatively, models can be registered and retrieved via the MLflow Model Registry. To use MLflow deployment, you must first create a model.   Container  Container plays a critical role for simplifying and standardizing the model deployment process. MLflow uses Docker containers to package models with their dependencies, enabling deployment to various destinations without environment compatibility issues. See Building a Docker Image for MLflow Model for more details on how to deploy your model as a container. If you’re new to Docker, you can start learning at “What is a Container”.   Deployment Target  Deployment target refers to the destination environment for your model. MLflow supports various targets, including local environments, cloud services (AWS, Azure), Kubernetes clusters, and others.    How it works  An MLflow Model already packages your model and its dependencies, hence MLflow can create either a virtual environment (for local deployment) or a Docker container image containing everything needed to run your model. Subsequently, MLflow launches an inference server with REST endpoints using frameworks like Flask, preparing it for deployment to various destinations to handle inference requests. Detailed information about the server and endpoints is available in Inference Server Specification. MLflow provides CLI commands and Python APIs to facilitate the deployment process. The required commands differ based on the deployment target, so please continue reading to the next section for more details about your specific target.   Supported Deployment Targets  MLflow offers support for a variety of deployment targets. For detailed information and tutorials on each, please follow the respective links below.                            Deploying a Model Locally                                      Deploying a model locally as an inference server is straightforward with MLflow, requiring just a single command mlflow models serve.                                              Amazon SageMaker is a fully managed service for scaling ML inference containers.                     MLflow simplifies the deployment process with easy-to-use commands, eliminating the need to write container definitions.                                              MLflow integrates seamlessly with Azure ML. You can deploy MLflow Model to the Azure ML managed online/batch endpoints,                     or to Azure Container Instances (ACI) / Azure Kubernetes Service (AKS).                                              Databricks Model Serving offers a fully managed service for serving MLflow models at scale,                     with added benefits of performance optimizations and monitoring capabilities.                                             MLflow Deployment integrates with Kubernetes-native ML serving frameworks                    such as Seldon Core and KServe (formerly KFServing).                                            Community Supported Targets                                       MLflow also supports more deployment targets such as Ray Serve, Redis AI, Torch Serve, Oracle Cloud Infrastructure (OCI), through community-supported plugins.                       API References   Command Line Interface  Deployment-related commands are primarily categorized under two modules:  mlflow models - typically used for local deployment. mlflow deployments - typically used for deploying to custom targets.  Note that these categories are not strictly separated and may overlap. Furthermore, certain targets require custom modules or plugins, for example, mlflow sagemaker is used for Amazon SageMaker deployments, and the azureml-mlflow library is required for Azure ML. Therefore, it is advisable to consult the specific documentation for your chosen target to identify the appropriate commands.   Python APIs  Almost all functionalities available in MLflow deployment can also be accessed via Python APIs. For more details, refer to the following API references:  mlflow.models mlflow.deployments mlflow.sagemaker     FAQ  If you encounter any dependency issues during model deployment, please refer to Model Dependencies FAQ for guidance on how to troubleshoot and validate fixes.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#id6,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
#deployment-guides-and-tutorials,"   Documentation  MLflow: A Tool for Managing the Machine Learning Lifecycle       MLflow: A Tool for Managing the Machine Learning Lifecycle  MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of MLflow and how they can be leveraged to solve real-world MLOps problems.  Getting Started with MLflow  If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is getting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to enhance your learning in area-specific guides and tutorials.  Getting Started Guides and Quickstarts                           MLflow Tracking Quickstart                                   A great place to start to learn the fundamentals of MLflow Tracking! Learn in 5 minutes how to log, register, and load a model for inference.                                           Intro to MLflow Tutorial                                       Learn how to get started with the basics of MLflow in a step-by-step instructional tutorial that shows the critical                     path to logging your first model                                           Autologging Quickstart                                       Short on time? This is a no-frills quickstart that shows how to leverage autologging during training and how to                     load a model for inference                                           Deployment Quickstart                                       Learn the basics of registering a model, setting up local serving for validation, and the process of                     containerization of a model for remote serving                        LLMs  Explore the comprehensive LLM-focused native support in MLflow. From MLflow Deployments for LLMs to the Prompt Engineering UI and native LLM-focused MLflow flavors like open-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the benefits of these powerful natural language deep learning models. You’ll learn how MLflow simplifies both using LLMs and developing solutions that leverage LLMs. Important tasks such as prompt development, evaluation of prompts, comparison of foundation models, fine-tuning and logging LLMs, and setting up production-grade interface servers are all covered by MLflow. Explore the guides and tutorials below to start your journey!  LLM Guides and Tutorials   MLflow Native flavors for GenAI                           🤗 Transformers in MLflow                                       Learn how to leverage the HuggingFace Transformers package with MLflow. Explore multiple tutorials and examples that leverage                     the power of state-of-the-art LLMs.                                           OpenAI in MLflow                                       Learn how to leverage the state-of-the-art LLMs offered by OpenAI directly as MLflow flavors to build and track useful language-based                     applications.                                           LangChain in MLflow                                       Learn how to build both simple and complex LLM-powered applications with LangChain, using MLflow to simplify deployment, dependency                     management, and service integration.                                           Sentence Transformers in MLflow                                       Learn how to leverage the advanced capabilities with semantic sentence embeddings within the Sentence Transformers package, using MLflow to simplify                     inference, create custom deployable applications, and more.                       General GenAI Guides                           Guide to the MLflow Prompt Engineering UI                                       Explore the features and functions of MLflow's Prompt Engineering UI for development, testing, evaluation, and                     deployment of validated prompts for your LLM use cases.                                           Guide for the MLflow Deployments for LLMs                                       Learn how to configure, setup, deploy, and use the MLflow Deployments for testing and production use cases of both                     SaaS and custom open-source LLMs.                                           Evaluating LLMs with MLflow Guide                                       Learn how to evaluate LLMs and LLM-powered solutions with MLflow Evaluate.                                           Using Custom PyFunc with LLMs                                       Explore the nuances of packaging and deploying advanced LLMs in MLflow using custom PyFuncs. This guide delves deep                     into managing intricate model behaviors, ensuring seamless and efficient LLM deployments.                                           Evaluation for RAG                                       Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API.                                           LLM Tracking with MLflow                                       Dive into the intricacies of MLflow's LLM Tracking system. From capturing prompts to monitoring generated outputs,                     discover how MLflow provides a holistic solution for managing LLM interactions.                         Model Evaluation  Dive into MLflow’s robust framework for evaluating the performance of your ML models. With support for traditional ML evaluation (classification and regression tasks), as well as support for evaluating large language models (LLMs), this suite of APIs offers a simple but powerful automated approach to evaluating the quality of the model development work that you’re doing. In particular, for LLM evaluation, the mlflow.evaluate() API allows you to validate not only models, but providers and prompts. By leveraging your own datasets and using the provided default evaluation criteria for tasks such as text summarization and question answering, you can get reliable metrics that allow you to focus on improving the quality of your solution, rather than spending time writing scoring code. Visual insights are also available through the MLflow UI, showcasing logged outputs, auto-generated plots, and model comparison artifacts.   Deep Learning  See how MLflow can help manage the full lifecycle of your Deep Learning projects. Whether you’re using frameworks like TensorFlow (tensorflow), Keras (keras), PyTorch (torch), Fastai (fastai), or spaCy (spacy), MLflow offers first-class support, ensuring seamless integration and deployment. Additionally, generic packaging frameworks that have native MLflow integration such as ONNX (onnx) grealy help to simplify the deployment of deep learning models to a wide variety of deployment providers and environments. Paired with MLflow’s streamlined APIs and comparative UI, you are equipped with everything needed to manage, track, and optimize your deep learning workflows.   Traditional ML  Leverage the power of MLflow for all your Traditional Machine Learning needs. Whether you’re working with supervised, unsupervised, statistical, or time series data, MLflow streamlines the process by providing an integrated environment that supports a large array of widely-used libraries like Scikit-learn (sklearn), SparkML (spark), XGBoost (xgboost), LightGBM (lightgbm), CatBoost (catboost), Statsmodels, Prophet, and Pmdarima. With MLflow, you not only get APIs tailored for these libraries but also a user-friendly UI to compare various runs, ensuring that your model tuning and evaluation phases are both efficient and insightful.  Traditional ML Guides and Tutorials                           Hyperparameter Tuning with Optuna and MLflow                                       This in-depth guide will show you how to leverage some core functionality in MLflow to keep your tuning iterations organized and                     searchable, all while covering a number of features within MLflow that cater to the needs of this common activity.                                           Custom PyFunc Tutorials with MLflow                                       Dive into the foundational aspects of MLflow's custom `pyfunc` to encapsulate, manage, and invoke models from any framework.                     This guide elucidates the versatility of `pyfunc`, highlighting how it bridges the gap between supported named flavors and bespoke model requirements.                        Deployment  In today’s ML-driven landscape, the ability to deploy models seamlessly and reliably is crucial. MLflow offers a robust suite tailored for this very purpose, ensuring that models transition from development to production without a hitch. Whether you’re aiming for real-time predictions, batch analyses, or interactive insights, MLflow’s deployment capabilities have got you covered. From managing dependencies and packaging models with their associated code to offering a large ecosystem of deployment avenues like local servers, cloud platforms, or Kubernetes clusters, MLflow ensures your models are not just artifacts but actionable, decision-making tools. Dive deep into the platform’s offerings, explore the tutorials, and harness the power of MLflow for efficient model serving.  Deployment Guides and Tutorials                           Deploying a Model to Kubernetes with MLflow                                       This guide showcases the seamless end-to-end process of training a linear regression model, packaging it in a reproducible format,                     and deploying to a Kubernetes cluster using MLflow. Explore how MLflow simplifies model deployment to production environments.                              Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
deployment/deploy-model-to-kubernetes/index.html,"   Documentation  Deployment  Deploy MLflow Model to Kubernetes       Deploy MLflow Model to Kubernetes   Using MLServer as the Inference Server  By default, MLflow deployment uses Flask, a widely used WSGI web application framework for Python, to serve the inference endpoint. However, Flask is mainly designed for a lightweight application and might not be suitable for production use cases at scale. To address this gap, MLflow integrates with MLServer as an alternative deployment option, which is used as a core Python inference server in Kubernetes-native frameworks like Seldon Core and KServe (formerly known as KFServing). Using MLServer, you can take advantage of the scalability and reliability of Kubernetes to serve your model at scale. See Serving Framework for the detailed comparison between Flask and MLServer, and why MLServer is a better choice for ML production use cases.   Building a Docker Image for MLflow Model  The essential step to deploy an MLflow model to Kubernetes is to build a Docker image that contains the MLflow model and the inference server. This can be done via build-docker CLI command or Python API.  CLIPythonmlflow models build-docker -m runs:/<run_id>/model -n <image_name> --enable-mlserver   If you want to use the bare-bones Flask server instead of MLServer, remove the --enable-mlserver flag. For other options, see the build-docker command documentation. import mlflow  mlflow.models.build_docker(     model_uri=f""runs:/{run_id}/model"",     name=""<image_name>"",     enable_mlserver=True, )   If you want to use the bare-bones Flask server instead of MLServer, remove enable_mlserver=True. For other options, see the mlflow.models.build_docker function documentation.   Important Since MLflow 2.10.1, the Docker image spec has been changed to reduce the image size and improve the performance. Most notably, Java is no longer installed in the image except for the Java model flavor such as spark. If you need to install Java for other flavors, e.g. custom Python model that uses SparkML, please specify the --install-java flag to enforce Java installation.    Deployment Steps  Please refer to the following partner documentations for deploying MLflow Models to Kubernetes using MLServer. You can also follow the tutorial below to learn the end-to-end process including environment setup, model training, and deployment.  Deploy MLflow models with KServe InferenceService Deploy MLflow models to Seldon Core    Tutorial  You can also learn how to train a model in MLflow and deploy to Kubernetes in the following tutorial:                            Develop ML model with MLflow and deploy to Kubernetes                                       This tutorial walks you through the end-to-end ML development process from training a machine learning mdoel,                     compare the performance, and deploy the model to Kubernetes using KServe.                            Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
introduction/index.html,"   Documentation  What is MLflow?       What is MLflow?  Stepping into the world of Machine Learning (ML) is an exciting journey, but it often comes with complexities that can hinder innovation and experimentation. MLflow is a solution to many of these issues in this dynamic landscape, offering tools and simplifying processes to streamline the ML lifecycle and foster collaboration among ML practitioners. Whether you’re an individual researcher, a member of a large team, or somewhere in between, MLflow provides a unified platform to navigate the intricate maze of model development, deployment, and management. MLflow aims to enable innovation in ML solution development by streamlining otherwise cumbersome logging, organization, and lineage concerns that are unique to model development. This focus allows you to ensure that your ML projects are robust, transparent, and ready for real-world challenges. Read on to discover the core components of MLflow and understand the unique advantages it brings to the complex workflows associated with model development and management.  Core Components of MLflow  MLflow, at its core, provides a suite of tools aimed at simplifying the ML workflow. It is tailored to assist ML practitioners throughout the various stages of ML development and deployment. Despite its expansive offerings, MLflow’s functionalities are rooted in several foundational components:  Tracking: MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time. Whether working in standalone scripts, notebooks, or other environments, Tracking facilitates the logging of results either to local files or a server, making it easier to compare multiple runs across different users. Model Registry: A systematic approach to model management, the Model Registry assists in handling different versions of models, discerning their current state, and ensuring smooth productionization. It offers a centralized model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, including model lineage, versioning, aliasing, tagging, and annotations. MLflow Deployments for LLMs: This server, equipped with a set of standardized APIs, streamlines access to both SaaS and OSS LLM models. It serves as a unified interface, bolstering security through authenticated access, and offers a common set of APIs for prominent LLMs. Evaluate: Designed for in-depth model analysis, this set of tools facilitates objective model comparison, be it traditional ML algorithms or cutting-edge LLMs. Prompt Engineering UI: A dedicated environment for prompt engineering, this UI-centric component provides a space for prompt experimentation, refinement, evaluation, testing, and deployment. Recipes: Serving as a guide for structuring ML projects, Recipes, while offering recommendations, are focused on ensuring functional end results optimized for real-world deployment scenarios. Projects: MLflow Projects standardize the packaging of ML code, workflows, and artifacts, akin to an executable. Each project, be it a directory with code or a Git repository, employs a descriptor or convention to define its dependencies and execution method.  By integrating these core components, MLflow offers an end-to-end platform, ensuring efficiency, consistency, and traceability throughout the ML lifecycle.   Why Use MLflow?  The machine learning (ML) process is intricate, comprising various stages, from data preprocessing to model deployment and monitoring. Ensuring productivity and efficiency throughout this lifecycle poses several challenges:  Experiment Management: It’s tough to keep track of the myriad experiments, especially when working with files or interactive notebooks. Determining which combination of data, code, and parameters led to a particular result can become a daunting task. Reproducibility: Ensuring consistent results across runs is not trivial. Beyond just tracking code versions and parameters, capturing the entire environment, including library dependencies, is critical. This becomes even more challenging when collaborating with other data scientists or when scaling the code to different platforms. Deployment Consistency: With the plethora of ML libraries available, there’s often no standardized way to package and deploy models. Custom solutions can lead to inconsistencies, and the crucial link between a model and the code and parameters that produced it might be lost. Model Management: As data science teams produce numerous models, managing, testing, and continuously deploying these models becomes a significant hurdle. Without a centralized platform, managing model lifecycles becomes unwieldy. Library Agnosticism: While individual ML libraries might offer solutions to some of the challenges, achieving the best results often involves experimenting across multiple libraries. A platform that offers compatibility with various libraries while ensuring models are usable as reproducible “black boxes” is essential.  MLflow addresses these challenges by offering a unified platform tailored for the entire ML lifecycle. Its benefits include:  Traceability: With tools like the Tracking Server, every experiment is logged, ensuring that teams can trace back and understand the evolution of models. Consistency: Be it accessing models through the MLflow Deployments for LLMs or structuring projects with MLflow Recipes, MLflow promotes a consistent approach, reducing both the learning curve and potential errors. Flexibility: MLflow’s library-agnostic design ensures compatibility with a wide range of machine learning libraries. It offers comprehensive support across different programming languages, backed by a robust REST API, CLI, and APIs for Python API, R API, and Java API.  By simplifying the complex landscape of ML workflows, MLflow empowers data scientists and developers to focus on building and refining models, ensuring a streamlined path from experimentation to production.   Who Uses MLflow?  Throughout the lifecycle of a particular project, there are components within MLflow that are designed to cater to different needs.    MLflow’s versatility enhances workflows across various roles, from data scientists to prompt engineers, extending its impact beyond just the confines of a Data Science team.  Data Scientists leverage MLflow for:  Experiment tracking and hypothesis testing persistence. Code structuring for better reproducibility. Model packaging and dependency management. Evaluating hyperparameter tuning selection boundaries. Comparing the results of model retraining over time. Reviewing and selecting optimal models for deployment.  MLOps Professionals utilize MLflow to:  Manage the lifecycles of trained models, both pre and post deployment. Deploy models securely to production environments. Audit and review candidate models prior to deployment. Manage deployment dependencies.  Data Science Managers interact with MLflow by:  Reviewing the outcomes of experimentation and modeling activities. Collaborating with teams to ensure that modeling objectives align with business goals.  Prompt Engineering Users use MLflow for:  Evaluating and experimenting with large language models. Crafting custom prompts and persisting their candidate creations. Deciding on the best base model suitable for their specific project requirements.     Use Cases of MLflow  MLflow is versatile, catering to diverse machine learning scenarios. Here are some typical use cases:  Experiment Tracking: A data science team leverages MLflow Tracking to log parameters and metrics for experiments within a particular domain. Using the MLflow UI, they can compare results and fine-tune their solution approach. The outcomes of these experiments are preserved as MLflow models. Model Selection and Deployment: MLOps engineers employ the MLflow UI to assess and pick the top-performing models. The chosen model is registered in the MLflow Registry, allowing for monitoring its real-world performance. Model Performance Monitoring: Post deployment, MLOps engineers utilize the MLflow Registry to gauge the model’s efficacy, juxtaposing it against other models in a live environment. Collaborative Projects: Data scientists embarking on new ventures organize their work as an MLflow Project. This structure facilitates easy sharing and parameter modifications, promoting collaboration.    Scalability in MLflow  MLflow is architected to seamlessly integrate with diverse data environments, from small datasets to Big Data applications. It’s built with the understanding that quality machine learning outcomes often hinge on robust data sources, and as such, scales adeptly to accommodate varying data needs. Here’s how MLflow addresses scalability across different dimensions:  Distributed Execution: MLflow runs can operate on distributed clusters. For instance, integration with Apache Spark allows for distributed processing. Furthermore, runs can be initiated on the distributed infrastructure of your preference, with results relayed to a centralized Tracking Server for analysis. Notably, MLflow offers an integrated API to initiate runs on Databricks. Parallel Runs: For use cases like hyperparameter tuning, MLflow can orchestrate multiple runs simultaneously, each with distinct parameters. Interoperability with Distributed Storage: MLflow Projects can interface with distributed storage solutions, including Azure ADLS, Azure Blob Storage, AWS S3, Cloudflare R2 and DBFS. Whether it’s automatically fetching files to a local environment or interfacing with a distributed storage URI directly, MLflow ensures that projects can handle extensive datasets – even scenarios like processing a 100 TB file. Centralized Model Management with Model Registry: Large-scale organizations can benefit from the MLflow Model Registry, a unified platform tailored for collaborative model lifecycle management. In environments where multiple data science teams might be concurrently developing numerous models, the Model Registry proves invaluable. It streamlines model discovery, tracks experiments, manages versions, and facilitates understanding a model’s intent across different teams.  By addressing these scalability dimensions, MLflow ensures that users can capitalize on its capabilities regardless of their data environment’s size or complexity.        Previous Next                © MLflow Project, a Series of LF Projects, LLC. All rights reserved.          "
